[96m[INFO] Origianl output [0m
['labels', 'boxes', 'scores']
[38;5;13m[V] Marking all ONNX tensors as outputs[0m
[38;5;13m[V] Loaded Module: onnx | Version: 1.15.0 | Path: ['/root/miniconda3/envs/rtdetr/lib/python3.10/site-packages/onnx'][0m
[96m[INFO] output to be compared [0m
['labels', 'boxes', 'scores']
[38;5;14m[I] trt-runner-N0-05/19/25-15:03:57     | Activating and starting inference[0m
[38;5;13m[V] Loaded Module: tensorrt | Version: 10.7.0 | Path: ['/root/miniconda3/envs/rtdetr/lib/python3.10/site-packages/tensorrt'][0m
[38;5;13m[V] The logger passed into createInferBuilder differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.[0m
[38;5;104m[X] CUDA lazy loading is enabled.[0m
[38;5;104m[X] Registered plugin creator - ::ROIAlign_TRT version 2[0m
[38;5;104m[X] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::BatchedNMS_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::BatchTilePlugin_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::Clip_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::CoordConvAC version 1[0m
[38;5;104m[X] Registered plugin creator - ::CropAndResizeDynamic version 1[0m
[38;5;104m[X] Registered plugin creator - ::CropAndResize version 1[0m
[38;5;104m[X] Registered plugin creator - ::DecodeBbox3DPlugin version 1[0m
[38;5;104m[X] Registered plugin creator - ::DetectionLayer_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::EfficientNMS_Explicit_TF_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::EfficientNMS_Implicit_TF_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::EfficientNMS_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::FlattenConcat_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::GenerateDetection_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::GridAnchor_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::GridAnchorRect_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::InstanceNormalization_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::InstanceNormalization_TRT version 2[0m
[38;5;104m[X] Registered plugin creator - ::InstanceNormalization_TRT version 3[0m
[38;5;104m[X] Registered plugin creator - ::LReLU_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::ModulatedDeformConv2d version 1[0m
[38;5;104m[X] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::MultilevelProposeROI_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::MultiscaleDeformableAttnPlugin_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::NMSDynamic_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::NMS_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::Normalize_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::PillarScatterPlugin version 1[0m
[38;5;104m[X] Registered plugin creator - ::PriorBox_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::ProposalDynamic version 1[0m
[38;5;104m[X] Registered plugin creator - ::ProposalLayer_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::Proposal version 1[0m
[38;5;104m[X] Registered plugin creator - ::PyramidROIAlign_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::Region_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::Reorg_TRT version 2[0m
[38;5;104m[X] Registered plugin creator - ::Reorg_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::ResizeNearest_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::ROIAlign_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::RPROI_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::ScatterElements version 1[0m
[38;5;104m[X] Registered plugin creator - ::ScatterElements version 2[0m
[38;5;104m[X] Registered plugin creator - ::ScatterND version 1[0m
[38;5;104m[X] Registered plugin creator - ::SpecialSlice_TRT version 1[0m
[38;5;104m[X] Registered plugin creator - ::Split version 1[0m
[38;5;104m[X] Registered plugin creator - ::VoxelGeneratorPlugin version 1[0m
[38;5;13m[V] ----------------------------------------------------------------[0m
[38;5;13m[V] Input filename:   default_mtq_int8_q_qint8baseline-output_modified.onnx[0m
[38;5;13m[V] ONNX IR version:  0.0.8[0m
[38;5;13m[V] Opset version:    17[0m
[38;5;13m[V] Producer name:    pytorch[0m
[38;5;13m[V] Producer version: 2.5.0[0m
[38;5;13m[V] Domain:           [0m
[38;5;13m[V] Model version:    0[0m
[38;5;13m[V] Doc string:       [0m
[38;5;13m[V] ----------------------------------------------------------------[0m
[38;5;104m[X] Adding network input: images with dtype: float32, dimensions: (1, 3, 640, 640)[0m
[38;5;104m[X] Registering tensor: images for ONNX tensor: images[0m
[38;5;104m[X] Adding network input: orig_target_sizes with dtype: int64, dimensions: (1, 2)[0m
[38;5;11m[W] ModelImporter.cpp:459: Make sure input orig_target_sizes has Int64 binding.[0m
[38;5;104m[X] Registering tensor: orig_target_sizes for ONNX tensor: orig_target_sizes[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.conv1.conv1_3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.short.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.anchors[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.decoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.proj.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_score_head.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_score_head.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_score_head.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.weight[0m
[38;5;104m[X] Importing initializer: model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.lateral_convs.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.fpn_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.0.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.downsample_convs.1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Importing initializer: model.encoder.pan_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] Importing initializer: onnx::Add_3614[0m
[38;5;104m[X] Importing initializer: onnx::Add_3616[0m
[38;5;104m[X] Importing initializer: onnx::Add_3618[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3619[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3620[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3621[0m
[38;5;104m[X] Importing initializer: onnx::Mul_3692[0m
[38;5;104m[X] Importing initializer: onnx::Add_3731[0m
[38;5;104m[X] Importing initializer: onnx::Add_3733[0m
[38;5;104m[X] Importing initializer: onnx::Add_3735[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3736[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3737[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3738[0m
[38;5;104m[X] Importing initializer: onnx::Mul_3755[0m
[38;5;104m[X] Importing initializer: onnx::Add_3803[0m
[38;5;104m[X] Importing initializer: onnx::Add_3805[0m
[38;5;104m[X] Importing initializer: onnx::Add_3807[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3808[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3809[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3810[0m
[38;5;104m[X] Importing initializer: onnx::Add_3875[0m
[38;5;104m[X] Importing initializer: onnx::Add_3877[0m
[38;5;104m[X] Importing initializer: onnx::Add_3879[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3880[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3881[0m
[38;5;104m[X] Importing initializer: onnx::MatMul_3882[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/Constant_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Split_2305[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Importing initializer: /postprocessor/Constant_output_0[0m
[38;5;104m[X] Importing initializer: onnx::Tile_3498[0m
[38;5;104m[X] Importing initializer: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] Importing initializer: _v_4326[0m
[38;5;104m[X] Importing initializer: _v_1997[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] Importing initializer: /model/encoder/Concat_1_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/Concat_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Importing initializer: _v_1846[0m
[38;5;104m[X] Importing initializer: _v_1848[0m
[38;5;104m[X] Importing initializer: _v_1850[0m
[38;5;104m[X] Importing initializer: _v_1749[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] Importing initializer: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] Importing initializer: _v_1663[0m
[38;5;104m[X] Importing initializer: _v_1665[0m
[38;5;104m[X] Importing initializer: _v_1669[0m
[38;5;104m[X] Importing initializer: _v_1675[0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: images[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [images -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight for ONNX node: tmp_weight[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 3, 640, 640)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_0 for ONNX node: tmp_weight_0[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_1.conv.weight -> (32, 3, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1 for ONNX node: tmp_weight_1[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 3, 3, 3)[INT8]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_2 for ONNX node: tmp_weight_2[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 3, 640, 640)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 3, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/conv/Conv for ONNX node: /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_1.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_1/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_1.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_1.norm.running_var -> (32)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_1/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_1/act/Relu for ONNX node: /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_1/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_3 for ONNX node: tmp_weight_3[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_4 for ONNX node: tmp_weight_4[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_2.conv.weight -> (32, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_5 for ONNX node: tmp_weight_5[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear_output_0 -> (32, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0 -> (32)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_output_0 -> (32)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_6 for ONNX node: tmp_weight_6[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear_output_0 -> (32, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/conv/Conv for ONNX node: /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_2.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_2/conv/Conv_output_0 -> (1, 32, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_2.norm.weight -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.bias -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_mean -> (32)[FLOAT]], [model.backbone.conv1.conv1_2.norm.running_var -> (32)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_2/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_2/norm/BatchNormalization_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_2/act/Relu for ONNX node: /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_2/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_2/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_2/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/conv1/conv1_2/act/Relu_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_7 for ONNX node: tmp_weight_7[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 32, 320, 320)[INT8]], [/model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_8 for ONNX node: tmp_weight_8[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.conv1.conv1_3.conv.weight -> (64, 32, 3, 3)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.conv1.conv1_3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_9 for ONNX node: tmp_weight_9[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 32, 3, 3)[INT8]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_10 for ONNX node: tmp_weight_10[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] inputs: [/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 32, 320, 320)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 32, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/conv/Conv for ONNX node: /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/conv/Conv [Conv] outputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.conv1.conv1_3.norm.running_var[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/conv1/conv1_3/conv/Conv_output_0 -> (1, 64, 320, 320)[FLOAT]], [model.backbone.conv1.conv1_3.norm.weight -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.bias -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_mean -> (64)[FLOAT]], [model.backbone.conv1.conv1_3.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization for ONNX node: /model/backbone/conv1/conv1_3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/conv1/conv1_3/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] inputs: [/model/backbone/conv1/conv1_3/norm/BatchNormalization_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/conv1/conv1_3/act/Relu for ONNX node: /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0 for ONNX tensor: /model/backbone/conv1/conv1_3/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/conv1/conv1_3/act/Relu [Relu] outputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/MaxPool [MaxPool][0m
[38;5;104m[X] Parsing node: /model/backbone/MaxPool [MaxPool][0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/MaxPool [MaxPool] inputs: [/model/backbone/conv1/conv1_3/act/Relu_output_0 -> (1, 64, 320, 320)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/MaxPool for ONNX node: /model/backbone/MaxPool[0m
[38;5;104m[X] Registering tensor: /model/backbone/MaxPool_output_0 for ONNX tensor: /model/backbone/MaxPool_output_0[0m
[38;5;104m[X] /model/backbone/MaxPool [MaxPool] outputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/MaxPool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/MaxPool_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_11 for ONNX node: tmp_weight_11[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_12 for ONNX node: tmp_weight_12[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_13 for ONNX node: tmp_weight_13[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_14 for ONNX node: tmp_weight_14[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2a.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_15 for ONNX node: tmp_weight_15[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_16 for ONNX node: tmp_weight_16[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_17 for ONNX node: tmp_weight_17[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_18 for ONNX node: tmp_weight_18[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.branch2b.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.0.short.conv.weight -> (64, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.0.short.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_19 for ONNX node: tmp_weight_19[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 1, 1)[INT8]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_20 for ONNX node: tmp_weight_20[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.0.short.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.0/short/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.0.short.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/Add for ONNX node: /model/backbone/res_layers.0/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.0/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_21 for ONNX node: tmp_weight_21[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_22 for ONNX node: tmp_weight_22[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2a.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_23 for ONNX node: tmp_weight_23[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_24 for ONNX node: tmp_weight_24[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2a.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2a/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_25 for ONNX node: tmp_weight_25[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_26 for ONNX node: tmp_weight_26[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.0.blocks.1.branch2b.conv.weight -> (64, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_27 for ONNX node: tmp_weight_27[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (64, 64, 3, 3)[INT8]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (64)[FLOAT]], [/model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_output_0 -> (64)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_28 for ONNX node: tmp_weight_28[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (64, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv_output_0 -> (1, 64, 160, 160)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.weight -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.bias -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_mean -> (64)[FLOAT]], [model.backbone.res_layers.0.blocks.1.branch2b.norm.running_var -> (64)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/Add for ONNX node: /model/backbone/res_layers.0/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.0/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.0/blocks.1/Add_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.0/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.0/blocks.1/act/Relu_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_29 for ONNX node: tmp_weight_29[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 160, 160)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_30 for ONNX node: tmp_weight_30[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2a.conv.weight -> (128, 64, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_31 for ONNX node: tmp_weight_31[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_32 for ONNX node: tmp_weight_32[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2a.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_33 for ONNX node: tmp_weight_33[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_34 for ONNX node: tmp_weight_34[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_35 for ONNX node: tmp_weight_35[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_36 for ONNX node: tmp_weight_36[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.branch2b.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 160, 160)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/pool/AveragePool_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_37 for ONNX node: tmp_weight_37[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 64, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_38 for ONNX node: tmp_weight_38[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.0.short.conv.conv.weight -> (128, 64, 1, 1)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_39 for ONNX node: tmp_weight_39[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 64, 1, 1)[INT8]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_40 for ONNX node: tmp_weight_40[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 64, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 64, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.0.short.conv.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/Add for ONNX node: /model/backbone/res_layers.1/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.0/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_41 for ONNX node: tmp_weight_41[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_42 for ONNX node: tmp_weight_42[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2a.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_43 for ONNX node: tmp_weight_43[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_44 for ONNX node: tmp_weight_44[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2a.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2a/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_45 for ONNX node: tmp_weight_45[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_46 for ONNX node: tmp_weight_46[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.1.blocks.1.branch2b.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_47 for ONNX node: tmp_weight_47[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_48 for ONNX node: tmp_weight_48[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.weight -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.bias -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_mean -> (128)[FLOAT]], [model.backbone.res_layers.1.blocks.1.branch2b.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/Add for ONNX node: /model/backbone/res_layers.1/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.1/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.1/blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.1/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.1/blocks.1/act/Relu_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_49 for ONNX node: tmp_weight_49[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_50 for ONNX node: tmp_weight_50[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2a.conv.weight -> (256, 128, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_51 for ONNX node: tmp_weight_51[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_52 for ONNX node: tmp_weight_52[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2a.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_53 for ONNX node: tmp_weight_53[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_54 for ONNX node: tmp_weight_54[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_55 for ONNX node: tmp_weight_55[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_56 for ONNX node: tmp_weight_56[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.branch2b.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/pool/AveragePool_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_57 for ONNX node: tmp_weight_57[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_58 for ONNX node: tmp_weight_58[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.0.short.conv.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_59 for ONNX node: tmp_weight_59[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_60 for ONNX node: tmp_weight_60[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.0.short.conv.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/Add for ONNX node: /model/backbone/res_layers.2/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.0/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_61 for ONNX node: tmp_weight_61[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_62 for ONNX node: tmp_weight_62[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2a.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_63 for ONNX node: tmp_weight_63[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_64 for ONNX node: tmp_weight_64[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2a.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2a/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_65 for ONNX node: tmp_weight_65[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_66 for ONNX node: tmp_weight_66[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.2.blocks.1.branch2b.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_67 for ONNX node: tmp_weight_67[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_68 for ONNX node: tmp_weight_68[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.weight -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.bias -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_mean -> (256)[FLOAT]], [model.backbone.res_layers.2.blocks.1.branch2b.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/Add for ONNX node: /model/backbone/res_layers.2/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.2/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.2/blocks.1/Add_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.2/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.2/blocks.1/act/Relu_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_69 for ONNX node: tmp_weight_69[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_70 for ONNX node: tmp_weight_70[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2a.conv.weight -> (512, 256, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_71 for ONNX node: tmp_weight_71[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_72 for ONNX node: tmp_weight_72[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2a.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_73 for ONNX node: tmp_weight_73[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_74 for ONNX node: tmp_weight_74[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_75 for ONNX node: tmp_weight_75[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_76 for ONNX node: tmp_weight_76[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.branch2b.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool for ONNX node: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool [AveragePool] outputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/pool/AveragePool_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_77 for ONNX node: tmp_weight_77[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_78 for ONNX node: tmp_weight_78[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.0.short.conv.conv.weight -> (512, 256, 1, 1)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_79 for ONNX node: tmp_weight_79[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 256, 1, 1)[INT8]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_80 for ONNX node: tmp_weight_80[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.0.short.conv.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/Add for ONNX node: /model/backbone/res_layers.3/blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.0/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.0/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_81 for ONNX node: tmp_weight_81[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_82 for ONNX node: tmp_weight_82[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2a.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_83 for ONNX node: tmp_weight_83[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_84 for ONNX node: tmp_weight_84[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2a.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2a/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_85 for ONNX node: tmp_weight_85[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_86 for ONNX node: tmp_weight_86[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.backbone.res_layers.3.blocks.1.branch2b.conv.weight -> (512, 512, 3, 3)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_87 for ONNX node: tmp_weight_87[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear_output_0 -> (512, 512, 3, 3)[INT8]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_88 for ONNX node: tmp_weight_88[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear_output_0 -> (512, 512, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv [Conv] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.weight[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.bias[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv_output_0 -> (1, 512, 20, 20)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.weight -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.bias -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_mean -> (512)[FLOAT]], [model.backbone.res_layers.3.blocks.1.branch2b.norm.running_var -> (512)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization for ONNX node: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization [BatchNormalization] outputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/Add [Add] inputs: [/model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/Add for ONNX node: /model/backbone/res_layers.3/blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/Add [Add] outputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/backbone/res_layers.3/blocks.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/Add_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] inputs: [/model/backbone/res_layers.3/blocks.1/Add_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/backbone/res_layers.3/blocks.1/act/Relu for ONNX node: /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0 for ONNX tensor: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/act/Relu [Relu] outputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.0.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_89 for ONNX node: tmp_weight_89[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_90 for ONNX node: tmp_weight_90[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/Conv [Conv] inputs: [/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/conv/Conv for ONNX node: /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/conv/Conv [Conv] outputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_91 for ONNX node: tmp_weight_91[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_92 for ONNX node: tmp_weight_92[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/Conv [Conv] inputs: [/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/conv/Conv for ONNX node: /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/conv/Conv [Conv] outputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/backbone/res_layers.3/blocks.1/act/Relu_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_93 for ONNX node: tmp_weight_93[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_94 for ONNX node: tmp_weight_94[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.input_proj.2.conv.weight -> (256, 512, 1, 1)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.input_proj.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_95 for ONNX node: tmp_weight_95[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 512, 1, 1)[INT8]], [/model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_96 for ONNX node: tmp_weight_96[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/Conv [Conv] inputs: [/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/conv/Conv for ONNX node: /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/Conv [Conv] outputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.encoder.input_proj.2.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/encoder/input_proj.2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/encoder/Reshape [Reshape] inputs: [/model/encoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/Reshape for ONNX node: /model/encoder/Reshape[0m
[38;5;104m[X] Registering tensor: /model/encoder/Reshape_output_0 for ONNX tensor: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/Reshape [Reshape] outputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/Transpose [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Transpose for ONNX node: /model/encoder/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/Transpose_output_0 for ONNX tensor: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/Transpose [Transpose] outputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/Constant_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add for ONNX node: /model/encoder/encoder.0/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/encoder/Reshape_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3619[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3619 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3619 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_97 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3614[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3614 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3614 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_98 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_99 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3620[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3620 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3620 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_100 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_101 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3616[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3616 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3616 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_102 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_103 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3621[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_1_output_0 -> (400, 1, 256)[FLOAT]], [onnx::MatMul_3621 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3621 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_104 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_105 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3618[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3618 -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/MatMul_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3618 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_106 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_107 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_108 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_1_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_109 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Add_2_output_0 -> (400, 1, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_110 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_2_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_2_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_111 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_112 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_1_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Mul_1_output_0 -> (8, 400, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_3_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_113 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Softmax [Softmax] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Softmax_output_0 -> (8, 400, 400)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_3_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/MatMul_4_output_0 -> (8, 400, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_5_output_0 -> (400, 8, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_3_output_0 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_114 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_3_output_0 -> (400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_115 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_116 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Gemm [Gemm] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Gemm_output_0 -> (400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_117 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/self_attn/Reshape_4_output_0 -> (400, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 for ONNX node: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] inputs: [/model/encoder/Transpose_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Transpose_6_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_1 for ONNX node: /model/encoder/encoder.0/layers.0/Add_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_1 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_1_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_120 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_121 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_122 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_123 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_124 for ONNX node: tmp_weight_124[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_125 for ONNX node: tmp_weight_125[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_126 for ONNX node: tmp_weight_126[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_127 for ONNX node: tmp_weight_127[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_128 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_129 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/MatMul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_130 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_131 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear1/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear1/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Div [Div][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_132 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_133 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Div for ONNX node: /model/encoder/encoder.0/layers.0/activation/Div[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Div_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Div [Div] outputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Erf [Erf][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Div_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] inputs: [/model/encoder/encoder.0/layers.0/activation/Div_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Erf for ONNX node: /model/encoder/encoder.0/layers.0/activation/Erf[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Erf_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Erf [Erf] outputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Erf_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] inputs: [/model/encoder/encoder.0/layers.0/activation/Erf_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_134 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_135 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Add for ONNX node: /model/encoder/encoder.0/layers.0/activation/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] inputs: [/model/encoder/encoder.0/layers.0/linear1/Add_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Add_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_136 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_137 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/activation/Mul_1 for ONNX node: /model/encoder/encoder.0/layers.0/activation/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/activation/Mul_1 [Mul] outputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/activation/Mul_1_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_138 for ONNX node: tmp_weight_138[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 400, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_139 for ONNX node: tmp_weight_139[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.encoder.0.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_140 for ONNX node: tmp_weight_140[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_141 for ONNX node: tmp_weight_141[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] inputs: [/model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Transpose for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Transpose [Transpose] outputs: [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] inputs: [/model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 400, 1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_142 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_143 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/MatMul for ONNX node: /model/encoder/encoder.0/layers.0/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/MatMul [MatMul] outputs: [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] inputs: [model.encoder.encoder.0.layers.0.linear2.bias -> (256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/MatMul_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_144 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_145 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/linear2/Add for ONNX node: /model/encoder/encoder.0/layers.0/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/linear2/Add [Add] outputs: [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] inputs: [/model/encoder/encoder.0/layers.0/norm1/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear2/Add_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/Add_2 for ONNX node: /model/encoder/encoder.0/layers.0/Add_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/Add_2_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/Add_2 [Add] outputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/Add_2_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] Searching for input: model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/encoder/encoder.0/layers.0/Add_2_output_0 -> (1, 400, 256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.weight -> (256)[FLOAT]], [model.encoder.encoder.0.layers.0.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.encoder.encoder.0.layers.0.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_148 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_149 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_150 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_151 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization for ONNX node: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/encoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Transpose_1 [Transpose] inputs: [/model/encoder/encoder.0/layers.0/norm2/LayerNormalization_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Transpose_1 for ONNX node: /model/encoder/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/Transpose_1_output_0 for ONNX tensor: /model/encoder/Transpose_1_output_0[0m
[38;5;104m[X] /model/encoder/Transpose_1 [Transpose] outputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/encoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/encoder/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_1_output_0[0m
[38;5;104m[X] /model/encoder/Reshape_1 [Reshape] inputs: [/model/encoder/Transpose_1_output_0 -> (1, 256, 400)[FLOAT]], [/model/encoder/Concat_1_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_152 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/Reshape_1 for ONNX node: /model/encoder/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/encoder/Reshape_1_output_0 for ONNX tensor: /model/encoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/encoder/Reshape_1 [Reshape] outputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Reshape_1_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_153 for ONNX node: tmp_weight_153[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_154 for ONNX node: tmp_weight_154[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.lateral_convs.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_155 for ONNX node: tmp_weight_155[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_156 for ONNX node: tmp_weight_156[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/conv/Conv for ONNX node: /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.0/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.lateral_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.0/act/Mul for ONNX node: /model/encoder/lateral_convs.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Resize [Resize][0m
[38;5;104m[X] Parsing node: /model/encoder/Resize [Resize][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] /model/encoder/Resize [Resize] inputs: [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Resize for ONNX node: /model/encoder/Resize[0m
[38;5;104m[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest[0m
[38;5;104m[X] Registering tensor: /model/encoder/Resize_output_0 for ONNX tensor: /model/encoder/Resize_output_0[0m
[38;5;104m[X] /model/encoder/Resize [Resize] outputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_2 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_2 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/Resize_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Concat_2 [Concat] inputs: [/model/encoder/Resize_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_2 for ONNX node: /model/encoder/Concat_2[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_2_output_0 for ONNX tensor: /model/encoder/Concat_2_output_0[0m
[38;5;104m[X] /model/encoder/Concat_2 [Concat] outputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_2_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_157 for ONNX node: tmp_weight_157[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_158 for ONNX node: tmp_weight_158[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_159 for ONNX node: tmp_weight_159[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_160 for ONNX node: tmp_weight_160[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_161 for ONNX node: tmp_weight_161[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_162 for ONNX node: tmp_weight_162[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_163 for ONNX node: tmp_weight_163[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_164 for ONNX node: tmp_weight_164[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_165 for ONNX node: tmp_weight_165[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_166 for ONNX node: tmp_weight_166[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_167 for ONNX node: tmp_weight_167[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_168 for ONNX node: tmp_weight_168[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_169 for ONNX node: tmp_weight_169[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_170 for ONNX node: tmp_weight_170[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_171 for ONNX node: tmp_weight_171[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_172 for ONNX node: tmp_weight_172[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_173 for ONNX node: tmp_weight_173[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_174 for ONNX node: tmp_weight_174[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/Add [Add] inputs: [/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/Add for ONNX node: /model/encoder/fpn_blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/Add [Add] outputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_175 for ONNX node: tmp_weight_175[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_176 for ONNX node: tmp_weight_176[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.0.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_177 for ONNX node: tmp_weight_177[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_178 for ONNX node: tmp_weight_178[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_179 for ONNX node: tmp_weight_179[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_180 for ONNX node: tmp_weight_180[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.lateral_convs.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.lateral_convs.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_181 for ONNX node: tmp_weight_181[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_182 for ONNX node: tmp_weight_182[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] inputs: [/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/conv/Conv for ONNX node: /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/conv/Conv [Conv] outputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.lateral_convs.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/lateral_convs.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.lateral_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.lateral_convs.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/lateral_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/act/Sigmoid for ONNX node: /model/encoder/lateral_convs.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/lateral_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Mul [Mul] inputs: [/model/encoder/lateral_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/lateral_convs.1/act/Mul for ONNX node: /model/encoder/lateral_convs.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/lateral_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/lateral_convs.1/act/Mul [Mul] outputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Resize_1 [Resize][0m
[38;5;104m[X] Parsing node: /model/encoder/Resize_1 [Resize][0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_9_output_0[0m
[38;5;104m[X] /model/encoder/Resize_1 [Resize] inputs: [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [optional input, not set], [/model/encoder/Constant_9_output_0 -> (4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Resize_1 for ONNX node: /model/encoder/Resize_1[0m
[38;5;104m[X] Running resize layer with: 
    Transformation mode: asymmetric
    Resize mode: nearest[0m
[38;5;104m[X] Registering tensor: /model/encoder/Resize_1_output_0 for ONNX tensor: /model/encoder/Resize_1_output_0[0m
[38;5;104m[X] /model/encoder/Resize_1 [Resize] outputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_3 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_3 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/Resize_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/Concat_3 [Concat] inputs: [/model/encoder/Resize_1_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_3 for ONNX node: /model/encoder/Concat_3[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_3_output_0 for ONNX tensor: /model/encoder/Concat_3_output_0[0m
[38;5;104m[X] /model/encoder/Concat_3 [Concat] outputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_3_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_183 for ONNX node: tmp_weight_183[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_184 for ONNX node: tmp_weight_184[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_185 for ONNX node: tmp_weight_185[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_186 for ONNX node: tmp_weight_186[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_187 for ONNX node: tmp_weight_187[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_188 for ONNX node: tmp_weight_188[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_189 for ONNX node: tmp_weight_189[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_190 for ONNX node: tmp_weight_190[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_191 for ONNX node: tmp_weight_191[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_192 for ONNX node: tmp_weight_192[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_193 for ONNX node: tmp_weight_193[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_194 for ONNX node: tmp_weight_194[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_195 for ONNX node: tmp_weight_195[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_196 for ONNX node: tmp_weight_196[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_197 for ONNX node: tmp_weight_197[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_198 for ONNX node: tmp_weight_198[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.fpn_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_199 for ONNX node: tmp_weight_199[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_200 for ONNX node: tmp_weight_200[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.fpn_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/Add [Add] inputs: [/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/Add for ONNX node: /model/encoder/fpn_blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/Add_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/Add [Add] outputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/Add_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_201 for ONNX node: tmp_weight_201[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 80, 80)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_202 for ONNX node: tmp_weight_202[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.fpn_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.fpn_blocks.1.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_203 for ONNX node: tmp_weight_203[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_204 for ONNX node: tmp_weight_204[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.fpn_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/fpn_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.fpn_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/fpn_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/fpn_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/fpn_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/fpn_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/fpn_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_205 for ONNX node: tmp_weight_205[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 80, 80)[INT8]], [/model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_206 for ONNX node: tmp_weight_206[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.0.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.downsample_convs.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_207 for ONNX node: tmp_weight_207[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_208 for ONNX node: tmp_weight_208[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/conv/Conv for ONNX node: /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.0.norm.running_var[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.0/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.downsample_convs.0.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.0/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.0/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.0/act/Mul for ONNX node: /model/encoder/downsample_convs.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.0/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.0/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_4 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_4 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/Concat_4 [Concat] inputs: [/model/encoder/downsample_convs.0/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/lateral_convs.1/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_4 for ONNX node: /model/encoder/Concat_4[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_4_output_0 for ONNX tensor: /model/encoder/Concat_4_output_0[0m
[38;5;104m[X] /model/encoder/Concat_4 [Concat] outputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_4_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_4_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_209 for ONNX node: tmp_weight_209[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 40, 40)[INT8]], [/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_210 for ONNX node: tmp_weight_210[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_211 for ONNX node: tmp_weight_211[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_212 for ONNX node: tmp_weight_212[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv1/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_213 for ONNX node: tmp_weight_213[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_214 for ONNX node: tmp_weight_214[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_215 for ONNX node: tmp_weight_215[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_216 for ONNX node: tmp_weight_216[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_217 for ONNX node: tmp_weight_217[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_218 for ONNX node: tmp_weight_218[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_219 for ONNX node: tmp_weight_219[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_220 for ONNX node: tmp_weight_220[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_221 for ONNX node: tmp_weight_221[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_222 for ONNX node: tmp_weight_222[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_223 for ONNX node: tmp_weight_223[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_224 for ONNX node: tmp_weight_224[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.0.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_225 for ONNX node: tmp_weight_225[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_226 for ONNX node: tmp_weight_226[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv2/conv/Conv_output_0 -> (1, 128, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.0.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Sigmoid_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/Add [Add] inputs: [/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv2/act/Mul_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/Add for ONNX node: /model/encoder/pan_blocks.0/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/Add_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/Add [Add] outputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/Add_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_227 for ONNX node: tmp_weight_227[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 40, 40)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_228 for ONNX node: tmp_weight_228[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.0.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.0.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_229 for ONNX node: tmp_weight_229[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_230 for ONNX node: tmp_weight_230[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.0.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.0/conv3/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.0.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.0/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.0/conv3/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/pan_blocks.0/conv3/act/Sigmoid_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.0/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.0/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.0/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.0/conv3/act/Mul_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_231 for ONNX node: tmp_weight_231[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 40, 40)[INT8]], [/model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_232 for ONNX node: tmp_weight_232[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.downsample_convs.1.conv.weight -> (256, 256, 3, 3)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.downsample_convs.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_233 for ONNX node: tmp_weight_233[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 3, 3)[INT8]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_234 for ONNX node: tmp_weight_234[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/conv/Conv for ONNX node: /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/conv/Conv [Conv] outputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.downsample_convs.1.norm.running_var[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/downsample_convs.1/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.downsample_convs.1.norm.weight -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.bias -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_mean -> (256)[FLOAT]], [model.encoder.downsample_convs.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/norm/BatchNormalization for ONNX node: /model/encoder/downsample_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/act/Sigmoid for ONNX node: /model/encoder/downsample_convs.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/downsample_convs.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Mul [Mul] inputs: [/model/encoder/downsample_convs.1/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/downsample_convs.1/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/downsample_convs.1/act/Mul for ONNX node: /model/encoder/downsample_convs.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/downsample_convs.1/act/Mul_output_0 for ONNX tensor: /model/encoder/downsample_convs.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/downsample_convs.1/act/Mul [Mul] outputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/Concat_5 [Concat][0m
[38;5;104m[X] Parsing node: /model/encoder/Concat_5 [Concat][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/lateral_convs.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/Concat_5 [Concat] inputs: [/model/encoder/downsample_convs.1/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/lateral_convs.0/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Concat_5 for ONNX node: /model/encoder/Concat_5[0m
[38;5;104m[X] Registering tensor: /model/encoder/Concat_5_output_0 for ONNX tensor: /model/encoder/Concat_5_output_0[0m
[38;5;104m[X] /model/encoder/Concat_5 [Concat] outputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/Concat_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/Concat_5_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_235 for ONNX node: tmp_weight_235[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 512, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_236 for ONNX node: tmp_weight_236[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv1.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_237 for ONNX node: tmp_weight_237[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_238 for ONNX node: tmp_weight_238[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv1.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv1.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv1/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_239 for ONNX node: tmp_weight_239[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_240 for ONNX node: tmp_weight_240[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.0.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_241 for ONNX node: tmp_weight_241[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_242 for ONNX node: tmp_weight_242[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.0.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.0.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_243 for ONNX node: tmp_weight_243[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_244 for ONNX node: tmp_weight_244[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.1.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_245 for ONNX node: tmp_weight_245[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_246 for ONNX node: tmp_weight_246[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.1.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.1.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_247 for ONNX node: tmp_weight_247[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_248 for ONNX node: tmp_weight_248[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.bottlenecks.2.conv.weight -> (128, 128, 3, 3)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_249 for ONNX node: tmp_weight_249[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 128, 3, 3)[INT8]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_250 for ONNX node: tmp_weight_250[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.bottlenecks.2.conv.bias[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 128, 3, 3)[FLOAT]], [model.encoder.pan_blocks.1.bottlenecks.2.conv.bias -> (128)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv2.conv.weight -> (128, 512, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_251 for ONNX node: tmp_weight_251[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear_output_0 -> (128, 512, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0 -> (128)[FLOAT]], [/model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (128)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_252 for ONNX node: tmp_weight_252[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 512, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear_output_0 -> (128, 512, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv2.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv2/conv/Conv_output_0 -> (1, 128, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.weight -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.bias -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_mean -> (128)[FLOAT]], [model.encoder.pan_blocks.1.conv2.norm.running_var -> (128)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv2/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Sigmoid_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv2/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv2/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv2/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/Add [Add] inputs: [/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv2/act/Mul_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/Add for ONNX node: /model/encoder/pan_blocks.1/Add[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/Add_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/Add_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/Add [Add] outputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/Add_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_253 for ONNX node: tmp_weight_253[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 128, 20, 20)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_254 for ONNX node: tmp_weight_254[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.encoder.pan_blocks.1.conv3.conv.weight -> (256, 128, 1, 1)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.encoder.pan_blocks.1.conv3.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_255 for ONNX node: tmp_weight_255[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 128, 1, 1)[INT8]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_256 for ONNX node: tmp_weight_256[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] inputs: [/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 128, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 128, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/conv/Conv for ONNX node: /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/conv/Conv [Conv] outputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.weight[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.bias[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.encoder.pan_blocks.1.conv3.norm.running_var[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] inputs: [/model/encoder/pan_blocks.1/conv3/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.weight -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.bias -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_mean -> (256)[FLOAT]], [model.encoder.pan_blocks.1.conv3.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization for ONNX node: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization [BatchNormalization] outputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Sigmoid [Sigmoid] outputs: [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/encoder/pan_blocks.1/conv3/act/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] inputs: [/model/encoder/pan_blocks.1/conv3/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/encoder/pan_blocks.1/conv3/act/Sigmoid_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/encoder/pan_blocks.1/conv3/act/Mul for ONNX node: /model/encoder/pan_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Registering tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0 for ONNX tensor: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv3/act/Mul [Mul] outputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.0.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.0.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_257 for ONNX node: tmp_weight_257[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_258 for ONNX node: tmp_weight_258[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 80, 80)[FLOAT]], [/model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/conv/Conv for ONNX node: /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/conv/Conv [Conv] outputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.0.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.0/conv/Conv_output_0 -> (1, 256, 80, 80)[FLOAT]], [model.decoder.input_proj.0.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.0.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.0/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.0/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.1.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.1.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_259 for ONNX node: tmp_weight_259[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_260 for ONNX node: tmp_weight_260[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/Conv [Conv] inputs: [/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 40, 40)[FLOAT]], [/model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/conv/Conv for ONNX node: /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/conv/Conv [Conv] outputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.1.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.1/conv/Conv_output_0 -> (1, 256, 40, 40)[FLOAT]], [model.decoder.input_proj.1.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.1.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.1/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.1/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/encoder/pan_blocks.1/conv3/act/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/encoder/pan_blocks.1/conv3/act/Mul_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_261 for ONNX node: tmp_weight_261[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> (1, 256, 20, 20)[INT8]], [/model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_262 for ONNX node: tmp_weight_262[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.input_proj.2.conv.weight -> (256, 256, 1, 1)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.input_proj.2.conv.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_263 for ONNX node: tmp_weight_263[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear_output_0 -> (256, 256, 1, 1)[INT8]], [/model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_264 for ONNX node: tmp_weight_264[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/conv/Conv [Conv][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/Conv [Conv] inputs: [/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear_output_0 -> (1, 256, 20, 20)[FLOAT]], [/model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear_output_0 -> (256, 256, 1, 1)[FLOAT]], [0m
[38;5;104m[X] Kernel weights are not set yet. Kernel weights must be set using setInput(1, kernel_tensor) API call.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/conv/Conv for ONNX node: /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/conv/Conv_output_0 for ONNX tensor: /model/decoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/conv/Conv [Conv] outputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/conv/Conv_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.bias[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.running_mean[0m
[38;5;104m[X] Searching for input: model.decoder.input_proj.2.norm.running_var[0m
[38;5;104m[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] inputs: [/model/decoder/input_proj.2/conv/Conv_output_0 -> (1, 256, 20, 20)[FLOAT]], [model.decoder.input_proj.2.norm.weight -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.bias -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_mean -> (256)[FLOAT]], [model.decoder.input_proj.2.norm.running_var -> (256)[FLOAT]], [0m
[38;5;104m[X] Found BatchNormalization node with conforming initializer types. Combining into a single scale node.[0m
[38;5;104m[X] Registering layer: /model/decoder/input_proj.2/norm/BatchNormalization for ONNX node: /model/decoder/input_proj.2/norm/BatchNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0 for ONNX tensor: /model/decoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] /model/decoder/input_proj.2/norm/BatchNormalization [BatchNormalization] outputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.0/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape [Reshape] inputs: [/model/decoder/input_proj.0/norm/BatchNormalization_output_0 -> (1, 256, 80, 80)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_265 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape for ONNX node: /model/decoder/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_output_0 for ONNX tensor: /model/decoder/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/Reshape [Reshape] outputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/Transpose [Transpose] inputs: [/model/decoder/Reshape_output_0 -> (1, 256, 6400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose for ONNX node: /model/decoder/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_output_0 for ONNX tensor: /model/decoder/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/Transpose [Transpose] outputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.1/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape_1 [Reshape] inputs: [/model/decoder/input_proj.1/norm/BatchNormalization_output_0 -> (1, 256, 40, 40)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_266 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape_1 for ONNX node: /model/decoder/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_1_output_0 for ONNX tensor: /model/decoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/Reshape_1 [Reshape] outputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_1 [Transpose] inputs: [/model/decoder/Reshape_1_output_0 -> (1, 256, 1600)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose_1 for ONNX node: /model/decoder/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_1_output_0 for ONNX tensor: /model/decoder/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_1 [Transpose] outputs: [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/input_proj.2/norm/BatchNormalization_output_0[0m
[38;5;104m[X] Searching for input: _v_4326[0m
[38;5;104m[X] /model/decoder/Reshape_2 [Reshape] inputs: [/model/decoder/input_proj.2/norm/BatchNormalization_output_0 -> (1, 256, 20, 20)[FLOAT]], [_v_4326 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_267 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Reshape_2 for ONNX node: /model/decoder/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/Reshape_2_output_0 for ONNX tensor: /model/decoder/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/Reshape_2 [Reshape] outputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_2 [Transpose] inputs: [/model/decoder/Reshape_2_output_0 -> (1, 256, 400)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Transpose_2 for ONNX node: /model/decoder/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/Transpose_2_output_0 for ONNX tensor: /model/decoder/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/Transpose_2 [Transpose] outputs: [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Concat_3 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/Concat_3 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/Concat_3 [Concat] inputs: [/model/decoder/Transpose_output_0 -> (1, 6400, 256)[FLOAT]], [/model/decoder/Transpose_1_output_0 -> (1, 1600, 256)[FLOAT]], [/model/decoder/Transpose_2_output_0 -> (1, 400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Concat_3 for ONNX node: /model/decoder/Concat_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/Concat_3_output_0 for ONNX tensor: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] /model/decoder/Concat_3 [Concat] outputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/Mul [Mul][0m
[38;5;104m[X] Searching for input: onnx::Mul_3692[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] /model/decoder/Mul [Mul] inputs: [onnx::Mul_3692 -> (1, 8400, 1)[FLOAT]], [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Mul_3692 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Mul for ONNX node: /model/decoder/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/Mul_output_0 for ONNX tensor: /model/decoder/Mul_output_0[0m
[38;5;104m[X] /model/decoder/Mul [Mul] outputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Mul_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_268 for ONNX node: tmp_weight_268[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_269 for ONNX node: tmp_weight_269[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_output.proj.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_270 for ONNX node: tmp_weight_270[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_271 for ONNX node: tmp_weight_271[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Transpose [Transpose] inputs: [/model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/Transpose for ONNX node: /model/decoder/enc_output/proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/Transpose_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Transpose [Transpose] outputs: [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/MatMul [MatMul] inputs: [/model/decoder/enc_output/proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_272 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_273 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/MatMul for ONNX node: /model/decoder/enc_output/proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/MatMul_output_0 for ONNX tensor: /model/decoder/enc_output/proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/MatMul [MatMul] outputs: [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Add [Add] inputs: [model.decoder.enc_output.proj.bias -> (256)[FLOAT]], [/model/decoder/enc_output/proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_274 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_275 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/proj/Add for ONNX node: /model/decoder/enc_output/proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/proj/Add_output_0 for ONNX tensor: /model/decoder/enc_output/proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/proj/Add [Add] outputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/Add_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] Searching for input: model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] inputs: [/model/decoder/enc_output/proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [model.decoder.enc_output.norm.weight -> (256)[FLOAT]], [model.decoder.enc_output.norm.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.norm.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.enc_output.norm.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_278 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_279 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_280 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_281 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_output/norm/LayerNormalization for ONNX node: /model/decoder/enc_output/norm/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0 for ONNX tensor: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/enc_output/norm/LayerNormalization [LayerNormalization] outputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_282 for ONNX node: tmp_weight_282[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_score_head/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_283 for ONNX node: tmp_weight_283[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_score_head.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_score_head.weight -> (80, 256)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_score_head.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_284 for ONNX node: tmp_weight_284[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_score_head/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_285 for ONNX node: tmp_weight_285[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Transpose [Transpose] inputs: [/model/decoder/enc_score_head/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/Transpose for ONNX node: /model/decoder/enc_score_head/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/Transpose_output_0 for ONNX tensor: /model/decoder/enc_score_head/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Transpose [Transpose] outputs: [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_score_head/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_286 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_287 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/MatMul for ONNX node: /model/decoder/enc_score_head/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/MatMul_output_0 for ONNX tensor: /model/decoder/enc_score_head/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/MatMul [MatMul] outputs: [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_score_head/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_score_head/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_score_head.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Add [Add] inputs: [model.decoder.enc_score_head.bias -> (80)[FLOAT]], [/model/decoder/enc_score_head/MatMul_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_score_head.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_288 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_289 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_score_head/Add for ONNX node: /model/decoder/enc_score_head/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_score_head/Add_output_0 for ONNX tensor: /model/decoder/enc_score_head/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_score_head/Add [Add] outputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_290 for ONNX node: tmp_weight_290[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_291 for ONNX node: tmp_weight_291[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/enc_score_head/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_292 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_293 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.0.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.0/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_294 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_295 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.0/Add for ONNX node: /model/decoder/enc_bbox_head/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.0/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.0/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/act/Relu for ONNX node: /model/decoder/enc_bbox_head/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/act/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_296 for ONNX node: tmp_weight_296[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_297 for ONNX node: tmp_weight_297[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_298 for ONNX node: tmp_weight_298[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_299 for ONNX node: tmp_weight_299[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_300 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_301 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.1/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_302 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_303 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.1/Add for ONNX node: /model/decoder/enc_bbox_head/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.1/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] inputs: [/model/decoder/enc_bbox_head/layers.1/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/act_1/Relu for ONNX node: /model/decoder/enc_bbox_head/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/act_1/Relu [Relu] outputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/enc_bbox_head/act_1/Relu_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_304 for ONNX node: tmp_weight_304[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_305 for ONNX node: tmp_weight_305[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.enc_bbox_head.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_306 for ONNX node: tmp_weight_306[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_307 for ONNX node: tmp_weight_307[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] inputs: [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Transpose for ONNX node: /model/decoder/enc_bbox_head/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Transpose [Transpose] outputs: [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] inputs: [/model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_308 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_309 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/MatMul for ONNX node: /model/decoder/enc_bbox_head/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/MatMul [MatMul] outputs: [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/enc_bbox_head/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] inputs: [model.decoder.enc_bbox_head.layers.2.bias -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/MatMul_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.enc_bbox_head.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_310 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_311 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/enc_bbox_head/layers.2/Add for ONNX node: /model/decoder/enc_bbox_head/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0 for ONNX tensor: /model/decoder/enc_bbox_head/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/enc_bbox_head/layers.2/Add [Add] outputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.anchors[0m
[38;5;104m[X] /model/decoder/Add [Add] inputs: [/model/decoder/enc_bbox_head/layers.2/Add_output_0 -> (1, 8400, 4)[FLOAT]], [model.decoder.anchors -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.anchors required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Add for ONNX node: /model/decoder/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/Add_output_0 for ONNX tensor: /model/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/Add [Add] outputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/ReduceMax [ReduceMax][0m
[38;5;104m[X] Parsing node: /model/decoder/ReduceMax [ReduceMax][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/Add_output_0[0m
[38;5;104m[X] /model/decoder/ReduceMax [ReduceMax] inputs: [/model/decoder/enc_score_head/Add_output_0 -> (1, 8400, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/ReduceMax for ONNX node: /model/decoder/ReduceMax[0m
[38;5;104m[X] Registering tensor: /model/decoder/ReduceMax_output_0 for ONNX tensor: /model/decoder/ReduceMax_output_0[0m
[38;5;104m[X] /model/decoder/ReduceMax [ReduceMax] outputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/TopK [TopK][0m
[38;5;104m[X] Parsing node: /model/decoder/TopK [TopK][0m
[38;5;104m[X] Searching for input: /model/decoder/ReduceMax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] /model/decoder/TopK [TopK] inputs: [/model/decoder/ReduceMax_output_0 -> (1, 8400)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Constant_18_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_convertToScalar required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/TopK for ONNX node: /model/decoder/TopK[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/TopK_output_0 for ONNX tensor: /model/decoder/TopK_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/TopK_output_1 for ONNX tensor: /model/decoder/TopK_output_1[0m
[38;5;104m[X] /model/decoder/TopK [TopK] outputs: [/model/decoder/TopK_output_0 -> (1, 300)[FLOAT]], [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/TopK_output_1[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/Unsqueeze [Unsqueeze] inputs: [/model/decoder/TopK_output_1 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Constant_7_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Unsqueeze for ONNX node: /model/decoder/Unsqueeze[0m
[38;5;104m[X] Registering tensor: /model/decoder/Unsqueeze_output_0 for ONNX tensor: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] /model/decoder/Unsqueeze [Unsqueeze] outputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Tile [Tile][0m
[38;5;104m[X] Parsing node: /model/decoder/Tile [Tile][0m
[38;5;104m[X] Searching for input: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] /model/decoder/Tile [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Tile for ONNX node: /model/decoder/Tile[0m
[38;5;104m[X] Registering tensor: /model/decoder/Tile_output_0 for ONNX tensor: /model/decoder/Tile_output_0[0m
[38;5;104m[X] /model/decoder/Tile [Tile] outputs: [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/GatherElements [GatherElements][0m
[38;5;104m[X] Parsing node: /model/decoder/GatherElements [GatherElements][0m
[38;5;104m[X] Searching for input: /model/decoder/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Tile_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements [GatherElements] inputs: [/model/decoder/Add_output_0 -> (1, 8400, 4)[FLOAT]], [/model/decoder/Tile_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_312 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/GatherElements for ONNX node: /model/decoder/GatherElements[0m
[38;5;104m[X] Registering tensor: /model/decoder/GatherElements_output_0 for ONNX tensor: /model/decoder/GatherElements_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements [GatherElements] outputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Tile_1 [Tile][0m
[38;5;104m[X] Parsing node: /model/decoder/Tile_1 [Tile][0m
[38;5;104m[X] Searching for input: /model/decoder/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/Tile_1 [Tile] inputs: [/model/decoder/Unsqueeze_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_7_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_313 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Tile_1 for ONNX node: /model/decoder/Tile_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/Tile_1_output_0 for ONNX tensor: /model/decoder/Tile_1_output_0[0m
[38;5;104m[X] /model/decoder/Tile_1 [Tile] outputs: [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/GatherElements_1 [GatherElements][0m
[38;5;104m[X] Parsing node: /model/decoder/GatherElements_1 [GatherElements][0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/norm/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Tile_1_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements_1 [GatherElements] inputs: [/model/decoder/enc_output/norm/LayerNormalization_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/Tile_1_output_0 -> (1, 300, 256)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_314 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/GatherElements_1 for ONNX node: /model/decoder/GatherElements_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/GatherElements_1_output_0 for ONNX tensor: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] /model/decoder/GatherElements_1 [GatherElements] outputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid [Sigmoid] inputs: [/model/decoder/GatherElements_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid for ONNX node: /model/decoder/decoder/Sigmoid[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_315 for ONNX node: tmp_weight_315[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_316 for ONNX node: tmp_weight_316[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.0.weight -> (512, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_317 for ONNX node: tmp_weight_317[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (512, 4)[INT8]], [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0 -> (512)[FLOAT]], [/model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (512)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_318 for ONNX node: tmp_weight_318[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (512, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_319 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_320 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_321 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_322 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_323 for ONNX node: tmp_weight_323[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_324 for ONNX node: tmp_weight_324[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.query_pos_head.layers.1.weight -> (256, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_325 for ONNX node: tmp_weight_325[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_326 for ONNX node: tmp_weight_326[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Transpose for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_327 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_328 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.query_pos_head.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_329 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_330 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add for ONNX node: /model/decoder/decoder/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add [Add] outputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3736[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3736 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3736 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_331 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_332 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3731[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] inputs: [onnx::Add_3731 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3731 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_333 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_334 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3737[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3737 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3737 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_335 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_336 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3733[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] inputs: [onnx::Add_3733 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3733 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_337 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_338 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3738[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3738 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3738 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_339 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_340 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3735[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] inputs: [onnx::Add_3735 -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3735 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_341 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_342 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_343 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_344 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_345 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_346 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_347 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_348 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.0/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_349 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.0.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.0/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_350 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_351 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Concat_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Concat_4_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_352 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.0/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/GatherElements_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_1 [Add] inputs: [/model/decoder/GatherElements_1_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_1 for ONNX node: /model/decoder/decoder/layers.0/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_1 [Add] outputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_355 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_356 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_357 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_358 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_2 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_2 for ONNX node: /model/decoder/decoder/layers.0/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_2 [Add] outputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/Concat_3_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_359 for ONNX node: tmp_weight_359[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 8400, 256)[INT8]], [/model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_360 for ONNX node: tmp_weight_360[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_361 for ONNX node: tmp_weight_361[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_362 for ONNX node: tmp_weight_362[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_363 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_364 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_365 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_366 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_367 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_368 for ONNX node: tmp_weight_368[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_369 for ONNX node: tmp_weight_369[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_370 for ONNX node: tmp_weight_370[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_371 for ONNX node: tmp_weight_371[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_372 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_373 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_374 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_375 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_376 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_377 for ONNX node: tmp_weight_377[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_378 for ONNX node: tmp_weight_378[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_379 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_380 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_381 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_382 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_383 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_384 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Mul_3755 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_385 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_386 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: _v_1997 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Constant_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_387 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_388 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_391 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_392 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_394 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_395 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_396 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_398 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_399 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_400 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_402 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_403 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_405 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_406 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_407 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_408 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_410 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_411 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_412 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_413 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_415 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_416 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_417 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_418 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_419 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: onnx::Unsqueeze_1255 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_420 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_421 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_423 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_424 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_426 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_427 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_429 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_430 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_431 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_433 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_434 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_435 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_437 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_438 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_440 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_441 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_442 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_443 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_445 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_446 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_447 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_448 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_450 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_451 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_452 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_453 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_454 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_455 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_457 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_458 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_460 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_461 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_463 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_464 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_465 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_467 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_468 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_469 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_471 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_472 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_474 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_475 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_476 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_477 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_479 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_480 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_481 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_482 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_484 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_485 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_486 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_487 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_488 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_490 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_491 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_493 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_494 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_496 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_497 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_498 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_500 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_501 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_502 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_504 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_505 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_507 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_508 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_509 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_510 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_512 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_513 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_514 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_515 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_517 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_518 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_519 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_520 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_521 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_523 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_524 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_526 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_527 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_529 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_530 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_531 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_533 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_534 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_535 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_537 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_538 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_540 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_541 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_542 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_543 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_545 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_546 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_547 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_548 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_550 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_551 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_552 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_553 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_554 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_6 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_6_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_555 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_556 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_557 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_558 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_559 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_560 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_561 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Split_562 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_563 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_564 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_565 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_566 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.0/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] inputs: [/model/decoder/decoder/layers.0/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Mul_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Mul_8 [Mul] outputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.0/cross_attn/Mul_8_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.0/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.0/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_567 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_568 for ONNX node: tmp_weight_568[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_569 for ONNX node: tmp_weight_569[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_570 for ONNX node: tmp_weight_570[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_571 for ONNX node: tmp_weight_571[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_572 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_573 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.0.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_574 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_575 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_3 [Add] inputs: [/model/decoder/decoder/layers.0/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_3 for ONNX node: /model/decoder/decoder/layers.0/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_3 [Add] outputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_578 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_579 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_580 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_581 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_582 for ONNX node: tmp_weight_582[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_583 for ONNX node: tmp_weight_583[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_584 for ONNX node: tmp_weight_584[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_585 for ONNX node: tmp_weight_585[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_586 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_587 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Add [Add] inputs: [model.decoder.decoder.layers.0.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_588 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_589 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear1/Add for ONNX node: /model/decoder/decoder/layers.0/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.0/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/activation/Relu for ONNX node: /model/decoder/decoder/layers.0/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_590 for ONNX node: tmp_weight_590[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_591 for ONNX node: tmp_weight_591[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.0.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_592 for ONNX node: tmp_weight_592[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_593 for ONNX node: tmp_weight_593[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.0/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_594 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_595 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.0/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Add [Add] inputs: [model.decoder.decoder.layers.0.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_596 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_597 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/linear2/Add for ONNX node: /model/decoder/decoder/layers.0/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_4 [Add] inputs: [/model/decoder/decoder/layers.0/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.0/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/Add_4 for ONNX node: /model/decoder/decoder/layers.0/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/Add_4 [Add] outputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.0/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.0.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.0.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_600 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_601 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_602 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_603 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.0/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_604 for ONNX node: tmp_weight_604[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_605 for ONNX node: tmp_weight_605[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_606 for ONNX node: tmp_weight_606[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_607 for ONNX node: tmp_weight_607[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_608 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_609 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_610 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_611 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_612 for ONNX node: tmp_weight_612[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_613 for ONNX node: tmp_weight_613[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_614 for ONNX node: tmp_weight_614[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_615 for ONNX node: tmp_weight_615[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_616 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_617 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_618 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_619 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_620 for ONNX node: tmp_weight_620[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_621 for ONNX node: tmp_weight_621[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.0.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_622 for ONNX node: tmp_weight_622[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_623 for ONNX node: tmp_weight_623[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_624 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_625 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.0.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.0.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_626 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_627 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.0/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip [Clip] inputs: [/model/decoder/decoder/Sigmoid_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip for ONNX node: /model/decoder/decoder/Clip[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_output_0 for ONNX tensor: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip [Clip] outputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_1 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_1 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_1 [Clip] inputs: [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Constant_3_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_629 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_630 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_631 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_632 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_1_output_0 for ONNX tensor: /model/decoder/decoder/Clip_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_1 [Clip] outputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_633 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_634 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub for ONNX node: /model/decoder/decoder/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_output_0 for ONNX tensor: /model/decoder/decoder/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub [Sub] outputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_2 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_2 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_2 [Clip] inputs: [/model/decoder/decoder/Sub_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_636 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_637 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_638 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_639 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_2_output_0 for ONNX tensor: /model/decoder/decoder/Clip_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_2 [Clip] outputs: [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div [Div] inputs: [/model/decoder/decoder/Clip_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div for ONNX node: /model/decoder/decoder/Div[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_output_0 for ONNX tensor: /model/decoder/decoder/Div_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div [Div] outputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log [Log] inputs: [/model/decoder/decoder/Div_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log for ONNX node: /model/decoder/decoder/Log[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_output_0 for ONNX tensor: /model/decoder/decoder/Log_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log [Log] outputs: [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add [Add] inputs: [/model/decoder/decoder/dec_bbox_head.0/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add for ONNX node: /model/decoder/decoder/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_output_0 for ONNX tensor: /model/decoder/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add [Add] outputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_1 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] inputs: [/model/decoder/decoder/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_1 for ONNX node: /model/decoder/decoder/Sigmoid_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_1_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_1 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_640 for ONNX node: tmp_weight_640[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_641 for ONNX node: tmp_weight_641[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_642 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_643 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_1/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_644 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_645 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_1/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act_1/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_1/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_1/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_646 for ONNX node: tmp_weight_646[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_647 for ONNX node: tmp_weight_647[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_648 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_649 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_650 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_651 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_1/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add for ONNX node: /model/decoder/decoder/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add [Add] outputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3808[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3808 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3808 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_652 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_653 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3803[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] inputs: [onnx::Add_3803 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3803 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_654 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_655 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3809[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3809 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3809 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_656 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_657 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3805[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] inputs: [onnx::Add_3805 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3805 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_658 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_659 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3810[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3810 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3810 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_660 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_661 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3807[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] inputs: [onnx::Add_3807 -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3807 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_662 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_663 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_664 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_665 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_666 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_667 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_668 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_669 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.1/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_670 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.1.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.1/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_671 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_672 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: _v_1675[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_673 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.1/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.1/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_1 [Add] inputs: [/model/decoder/decoder/layers.0/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_1 for ONNX node: /model/decoder/decoder/layers.1/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_1 [Add] outputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_676 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_677 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_678 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_679 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_2 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_2 for ONNX node: /model/decoder/decoder/layers.1/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_2 [Add] outputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_680 for ONNX node: tmp_weight_680[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_681 for ONNX node: tmp_weight_681[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_682 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_683 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_684 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_685 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_686 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_687 for ONNX node: tmp_weight_687[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_688 for ONNX node: tmp_weight_688[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_689 for ONNX node: tmp_weight_689[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_690 for ONNX node: tmp_weight_690[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_691 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_692 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_693 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_694 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_695 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_696 for ONNX node: tmp_weight_696[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_697 for ONNX node: tmp_weight_697[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_698 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_699 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_700 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_701 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_702 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_703 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_704 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_705 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_706 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_707 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_709 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_710 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_712 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_713 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_715 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_716 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_717 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_719 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_720 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_721 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_723 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_724 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_726 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_727 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_728 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_729 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_731 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_732 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_733 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_734 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_736 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_737 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_738 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_739 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_740 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_741 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_742 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_744 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_745 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_747 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_748 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_750 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_751 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_752 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_754 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_755 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_756 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_758 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_759 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_761 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_762 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_763 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_764 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_766 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_767 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_768 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_769 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_771 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_772 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_773 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_774 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_775 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_776 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_778 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_779 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_781 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_782 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_784 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_785 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_786 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_788 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_789 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_790 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_792 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_793 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_795 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_796 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_797 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_798 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_800 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_801 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_802 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_803 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_805 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_806 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_807 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_808 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_809 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_811 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_812 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_814 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_815 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_817 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_818 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_819 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_821 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_822 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_823 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_825 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_826 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_828 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_829 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_830 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_831 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_833 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_834 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_835 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_836 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_838 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_839 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_840 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_841 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_842 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_844 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_845 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_847 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_848 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_850 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_851 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_852 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_854 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_855 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_856 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_858 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_859 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_861 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_862 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_863 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_864 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_866 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_867 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_868 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_869 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_871 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_872 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_873 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_874 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_875 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_876 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_877 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_878 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_879 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_880 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_881 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_882 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Split_883 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_884 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_885 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_886 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_887 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.1/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.1/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Mul_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.1/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.1/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.1/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_888 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_889 for ONNX node: tmp_weight_889[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_890 for ONNX node: tmp_weight_890[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_891 for ONNX node: tmp_weight_891[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_892 for ONNX node: tmp_weight_892[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_893 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_894 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.1.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_895 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_896 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_3 [Add] inputs: [/model/decoder/decoder/layers.1/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_3 for ONNX node: /model/decoder/decoder/layers.1/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_3 [Add] outputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_899 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_900 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_901 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_902 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_903 for ONNX node: tmp_weight_903[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_904 for ONNX node: tmp_weight_904[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_905 for ONNX node: tmp_weight_905[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_906 for ONNX node: tmp_weight_906[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_907 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_908 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Add [Add] inputs: [model.decoder.decoder.layers.1.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_909 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_910 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear1/Add for ONNX node: /model/decoder/decoder/layers.1/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.1/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/activation/Relu for ONNX node: /model/decoder/decoder/layers.1/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_911 for ONNX node: tmp_weight_911[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_912 for ONNX node: tmp_weight_912[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.1.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_913 for ONNX node: tmp_weight_913[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_914 for ONNX node: tmp_weight_914[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.1/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_915 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_916 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.1/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Add [Add] inputs: [model.decoder.decoder.layers.1.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_917 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_918 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/linear2/Add for ONNX node: /model/decoder/decoder/layers.1/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_4 [Add] inputs: [/model/decoder/decoder/layers.1/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.1/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/Add_4 for ONNX node: /model/decoder/decoder/layers.1/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/Add_4 [Add] outputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.1/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.1.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.1.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_921 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_922 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_923 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_924 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.1/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_925 for ONNX node: tmp_weight_925[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_926 for ONNX node: tmp_weight_926[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_927 for ONNX node: tmp_weight_927[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_928 for ONNX node: tmp_weight_928[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_929 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_930 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_931 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_932 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_933 for ONNX node: tmp_weight_933[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_934 for ONNX node: tmp_weight_934[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_935 for ONNX node: tmp_weight_935[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_936 for ONNX node: tmp_weight_936[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_937 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_938 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_939 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_940 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_941 for ONNX node: tmp_weight_941[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_942 for ONNX node: tmp_weight_942[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.1.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_943 for ONNX node: tmp_weight_943[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_944 for ONNX node: tmp_weight_944[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_945 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_946 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.1.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.1.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_947 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_948 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.1/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_3 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_3 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_3 [Clip] inputs: [/model/decoder/decoder/Sigmoid_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip_3 for ONNX node: /model/decoder/decoder/Clip_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_3_output_0 for ONNX tensor: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_3 [Clip] outputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_4 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_4 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_4 [Clip] inputs: [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_950 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_951 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_952 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_953 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_4_output_0 for ONNX tensor: /model/decoder/decoder/Clip_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_4 [Clip] outputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub_1 [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub_1 [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_1 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_954 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_955 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub_1 for ONNX node: /model/decoder/decoder/Sub_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_1_output_0 for ONNX tensor: /model/decoder/decoder/Sub_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_1 [Sub] outputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_5 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_5 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_5 [Clip] inputs: [/model/decoder/decoder/Sub_1_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_957 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_958 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_959 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_960 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_5_output_0 for ONNX tensor: /model/decoder/decoder/Clip_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_5 [Clip] outputs: [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div_1 [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div_1 [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_1 [Div] inputs: [/model/decoder/decoder/Clip_4_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_5_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div_1 for ONNX node: /model/decoder/decoder/Div_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_1_output_0 for ONNX tensor: /model/decoder/decoder/Div_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_1 [Div] outputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log_1 [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log_1 [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_1 [Log] inputs: [/model/decoder/decoder/Div_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log_1 for ONNX node: /model/decoder/decoder/Log_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_1_output_0 for ONNX tensor: /model/decoder/decoder/Log_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_1 [Log] outputs: [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_1 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.1/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add_1 for ONNX node: /model/decoder/decoder/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_1 [Add] outputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_2 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] inputs: [/model/decoder/decoder/Add_1_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_2 for ONNX node: /model/decoder/decoder/Sigmoid_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_2_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_2 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_961 for ONNX node: tmp_weight_961[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 4)[INT8]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_962 for ONNX node: tmp_weight_962[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0/Transpose_output_0 -> (4, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_963 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_964 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.0.bias -> (512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.0_2/MatMul_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_965 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_966 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.0_2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.0_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/act_2/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] inputs: [/model/decoder/decoder/query_pos_head/layers.0_2/Add_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/act_2/Relu for ONNX node: /model/decoder/decoder/query_pos_head/act_2/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/act_2/Relu [Relu] outputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/act_2/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/act_2/Relu_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_967 for ONNX node: tmp_weight_967[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear_output_0 -> (1, 300, 512)[INT8]], [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_968 for ONNX node: tmp_weight_968[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear for ONNX node: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] inputs: [/model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear_output_0 -> (1, 300, 512)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1/Transpose_output_0 -> (512, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_969 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_970 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/MatMul [MatMul] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.query_pos_head.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] inputs: [model.decoder.query_pos_head.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_971 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_972 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add for ONNX node: /model/decoder/decoder/query_pos_head/layers.1_2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 for ONNX tensor: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/query_pos_head/layers.1_2/Add [Add] outputs: [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add for ONNX node: /model/decoder/decoder/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add [Add] outputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3880[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3880 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3880 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_973 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_974 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3875[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] inputs: [onnx::Add_3875 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3875 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_975 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_976 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3881[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3881 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3881 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_977 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_978 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_1 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3877[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] inputs: [onnx::Add_3877 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3877 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_979 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_980 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::MatMul_3882[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_1_output_0 -> (300, 1, 256)[FLOAT]], [onnx::MatMul_3882 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::MatMul_3882 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_981 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_982 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Add_2 [Add][0m
[38;5;104m[X] Searching for input: onnx::Add_3879[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] inputs: [onnx::Add_3879 -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/MatMul_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: onnx::Add_3879 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_983 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_984 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Add_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_985 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_1_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_986 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1669[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Add_2_output_0 -> (300, 1, 256)[FLOAT]], [_v_1669 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_987 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_2_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_2_output_0 -> (8, 300, 32)[FLOAT]], [/model/encoder/encoder.0/layers.0/self_attn/Constant_7_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_988 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_989 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_1_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_4 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Mul_1_output_0 -> (8, 300, 32)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_4_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_3 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_3_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/self_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_990 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Softmax_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] inputs: [/model/decoder/decoder/layers.2/self_attn/Softmax_output_0 -> (8, 300, 300)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_3_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/MatMul_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_4 [MatMul] outputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/MatMul_4_output_0 -> (8, 300, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_5 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0[0m
[38;5;104m[X] Searching for input: _v_1846[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_5_output_0 -> (300, 8, 32)[FLOAT]], [_v_1846 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_991 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_3 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_3_output_0 -> (300, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.weight -> (256, 256)[FLOAT]], [model.decoder.decoder.layers.2.self_attn.out_proj.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Using opA: 0 opB: 1[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Gemm for ONNX node: /model/decoder/decoder/layers.2/self_attn/Gemm[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.self_attn.out_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_992 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_993 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Gemm [Gemm] outputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Gemm_output_0[0m
[38;5;104m[X] Searching for input: _v_1675[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/self_attn/Gemm_output_0 -> (300, 256)[FLOAT]], [_v_1675 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_994 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] inputs: [/model/decoder/decoder/layers.2/self_attn/Reshape_4_output_0 -> (300, 1, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/self_attn/Transpose_6 for ONNX node: /model/decoder/decoder/layers.2/self_attn/Transpose_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/Transpose_6 [Transpose] outputs: [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_1 [Add] inputs: [/model/decoder/decoder/layers.1/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/self_attn/Transpose_6_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_1 for ONNX node: /model/decoder/decoder/layers.2/Add_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_1 [Add] outputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_1_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_1_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm1.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_997 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_998 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_999 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1000 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm1/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm1/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_2 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/query_pos_head/layers.1_2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_2 for ONNX node: /model/decoder/decoder/layers.2/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_2 [Add] outputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1001 for ONNX node: tmp_weight_1001[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1002 for ONNX node: tmp_weight_1002[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 8400, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1003 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1004 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.value_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.value_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1005 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1006 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/value_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1848[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/value_proj/Add_output_0 -> (1, 8400, 256)[FLOAT]], [_v_1848 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1007 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/Add_2_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1008 for ONNX node: tmp_weight_1008[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1009 for ONNX node: tmp_weight_1009[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight -> (192, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1010 for ONNX node: tmp_weight_1010[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear_output_0 -> (192, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0 -> (192)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_output_0 -> (192)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1011 for ONNX node: tmp_weight_1011[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear_output_0 -> (192, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose_output_0 -> (256, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1012 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1013 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias -> (192)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1014 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1015 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1663[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add_output_0 -> (1, 300, 192)[FLOAT]], [_v_1663 -> (5)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1016 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_1 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.weight -> (96, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1017 for ONNX node: tmp_weight_1017[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear_output_0 -> (96, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0 -> (96)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_output_0 -> (96)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1018 for ONNX node: tmp_weight_1018[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear_output_0 -> (96, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose_output_0 -> (256, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1019 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1020 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.attention_weights.bias -> (96)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.attention_weights.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1021 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1022 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0[0m
[38;5;104m[X] Searching for input: _v_1665[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/attention_weights/Add_output_0 -> (1, 300, 96)[FLOAT]], [_v_1665 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1023 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_2 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_2_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Softmax for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Softmax[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1024 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Softmax [Softmax] outputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3755[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [onnx::Mul_3755 -> (12, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1025 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1026 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: _v_1997[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [_v_1997 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8 [Unsqueeze] outputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_15_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1027 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1028 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1030 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1031 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1033 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1034 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1036 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1037 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1038 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1040 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1041 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1042 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1044 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1045 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1047 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1048 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1049 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1050 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1052 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1053 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1054 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1055 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1057 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1058 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1059 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Slice_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_1 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_1_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1060 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1061 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_2 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8_output_0 -> (1, 300, 1, 1, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/Constant_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Constant_13_output_0 -> (1)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1062 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1063 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1065 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1066 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1068 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1069 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1071 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1072 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1073 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1075 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1076 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1077 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1079 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1080 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1082 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1083 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1084 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1085 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1087 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1088 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1089 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1090 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1092 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1093 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1094 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_1 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Add [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_1_output_0 -> (1, 300, 1, 1, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Mul_2_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_output_0 -> (1, 8400, 8, 32)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_output_0[0m
[38;5;104m[X] Searching for input: _v_1749[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_output_0 -> (1, 8, 32, 8400)[FLOAT]], [_v_1749 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1095 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_4 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1096 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1097 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1099 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1100 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1102 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1103 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1105 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1106 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1107 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1109 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1110 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1111 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1113 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1114 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1116 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1117 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1118 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1119 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1121 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1122 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1123 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1124 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1126 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1127 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1128 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_4 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Unsqueeze_9_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1129 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1130 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1132 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1133 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1135 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1136 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1138 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1139 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1140 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1142 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1143 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1144 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1146 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1147 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1149 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1150 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1151 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1152 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1154 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1155 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1156 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1157 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1159 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1160 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1161 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_5 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Add_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_4_output_0 -> (8, 32, 8400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Add_2_output_0 -> (1)[INT64]], [/model/decoder/decoder/layers.0/cross_attn/Add_3_output_0 -> (1)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1162 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1163 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1165 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1166 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1168 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1169 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1171 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1172 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1173 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1175 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1176 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1177 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1179 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1180 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1182 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1183 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1184 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1185 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1187 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1188 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1189 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1190 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeTensorFromDims_1192 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeElementWise_1193 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1194 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Slice_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Slice_6 [Slice] outputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Add_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Constant_39_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1195 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1196 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_3 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Sub [Sub][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_3_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1197 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1198 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Sub for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Sub[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Sub [Sub] outputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Sub_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Sub_output_0 -> (1, 300, 8, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_1 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0[0m
[38;5;104m[X] Searching for input: _v_1850[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_1_output_0 -> (1, 8, 300, 12, 2)[FLOAT]], [_v_1850 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1199 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_5 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0[0m
[38;5;104m[X] Searching for input: onnx::Split_2305[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_5_output_0 -> (8, 300, 12, 2)[FLOAT]], [onnx::Split_2305 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1200 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1201 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1202 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1203 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Split_1204 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Split[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_0[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Split [Split] outputs: [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_4_output_0 -> (8, 32, 6400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_6_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1205 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_6 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_6_output_0 -> (8, 32, 80, 80)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_0 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_5_output_0 -> (8, 32, 1600)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_7_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1206 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_7[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_7 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_1[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_7_output_0 -> (8, 32, 40, 40)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_1 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_1[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_1 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Slice_6_output_0 -> (8, 32, 400)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_8_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1207 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_8 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Split_output_2[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_8_output_0 -> (8, 32, 20, 20)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Split_output_2 -> (8, 300, 4, 2)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/GridSample_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/GridSample_2 [GridSample] outputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Softmax_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Softmax_output_0 -> (1, 300, 8, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_2 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_2_output_0 -> (1, 8, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_9_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1208 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_9 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] inputs: [/model/decoder/decoder/layers.2/cross_attn/GridSample_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_1_output_0 -> (8, 32, 300, 4)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/GridSample_2_output_0 -> (8, 32, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Concat_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Concat_10 [Concat] outputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] inputs: [/model/decoder/decoder/layers.2/cross_attn/Concat_10_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/Reshape_9_output_0 -> (8, 1, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Mul_5[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Mul_5 [Mul] outputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] inputs: [/model/decoder/decoder/layers.2/cross_attn/Mul_5_output_0 -> (8, 32, 300, 12)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum for ONNX node: /model/decoder/decoder/layers.2/cross_attn/ReduceSum[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/ReduceSum [ReduceSum] outputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] inputs: [/model/decoder/decoder/layers.2/cross_attn/ReduceSum_output_0 -> (8, 32, 300)[FLOAT]], [/model/decoder/decoder/layers.0/cross_attn/Concat_11_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1209 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_10 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Reshape_10[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Reshape_10 [Reshape] outputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/Reshape_10_output_0 -> (1, 256, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_3 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/Transpose_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/Transpose_3 [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/Transpose_3_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1210 for ONNX node: tmp_weight_1210[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1211 for ONNX node: tmp_weight_1211[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1212 for ONNX node: tmp_weight_1212[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1213 for ONNX node: tmp_weight_1213[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1214 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1215 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] inputs: [model.decoder.decoder.layers.2.cross_attn.output_proj.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.cross_attn.output_proj.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1216 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1217 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add for ONNX node: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/cross_attn/output_proj/Add [Add] outputs: [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_3 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_3 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_3 [Add] inputs: [/model/decoder/decoder/layers.2/norm1/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/cross_attn/output_proj/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_3 for ONNX node: /model/decoder/decoder/layers.2/Add_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_3_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_3 [Add] outputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_3_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_3_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm2.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1220 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1221 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1222 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1223 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm2/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm2/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1224 for ONNX node: tmp_weight_1224[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1225 for ONNX node: tmp_weight_1225[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear1.weight -> (1024, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1226 for ONNX node: tmp_weight_1226[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear_output_0 -> (1024, 256)[INT8]], [/model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0 -> (1024)[FLOAT]], [/model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_output_0 -> (1024)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1227 for ONNX node: tmp_weight_1227[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/Transpose_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1228 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1229 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Add [Add] inputs: [model.decoder.decoder.layers.2.linear1.bias -> (1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear1/MatMul_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1230 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1231 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear1/Add for ONNX node: /model/decoder/decoder/layers.2/linear1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear1/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/activation/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] inputs: [/model/decoder/decoder/layers.2/linear1/Add_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/activation/Relu for ONNX node: /model/decoder/decoder/layers.2/activation/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/activation/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/activation/Relu [Relu] outputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/activation/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/activation/Relu_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1232 for ONNX node: tmp_weight_1232[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1233 for ONNX node: tmp_weight_1233[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.decoder.layers.2.linear2.weight -> (256, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1234 for ONNX node: tmp_weight_1234[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear_output_0 -> (256, 1024)[INT8]], [/model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1235 for ONNX node: tmp_weight_1235[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] inputs: [/model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear_output_0 -> (256, 1024)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Transpose for ONNX node: /model/decoder/decoder/layers.2/linear2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Transpose [Transpose] outputs: [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] inputs: [/model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 1024)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Transpose_output_0 -> (1024, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1236 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1237 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/MatMul for ONNX node: /model/decoder/decoder/layers.2/linear2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/MatMul [MatMul] outputs: [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/linear2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Add [Add] inputs: [model.decoder.decoder.layers.2.linear2.bias -> (256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.linear2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1238 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1239 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/linear2/Add for ONNX node: /model/decoder/decoder/layers.2/linear2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/linear2/Add [Add] outputs: [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/Add_4 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/Add_4 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/linear2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_4 [Add] inputs: [/model/decoder/decoder/layers.2/norm2/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/layers.2/linear2/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/Add_4 for ONNX node: /model/decoder/decoder/layers.2/Add_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/Add_4_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/Add_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/Add_4 [Add] outputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/Add_4_output_0[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] Searching for input: model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] inputs: [/model/decoder/decoder/layers.2/Add_4_output_0 -> (1, 300, 256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.weight -> (256)[FLOAT]], [model.decoder.decoder.layers.2.norm3.bias -> (256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm3.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: model.decoder.decoder.layers.2.norm3.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1242 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1243 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1244 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1245 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization for ONNX node: /model/decoder/decoder/layers.2/norm3/LayerNormalization[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 for ONNX tensor: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/norm3/LayerNormalization [LayerNormalization] outputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/layers.2/norm3/LayerNormalization_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1246 for ONNX node: tmp_weight_1246[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1247 for ONNX node: tmp_weight_1247[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.0.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1248 for ONNX node: tmp_weight_1248[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1249 for ONNX node: tmp_weight_1249[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1250 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1251 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.0.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.0.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1252 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1253 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.0/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1254 for ONNX node: tmp_weight_1254[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1255 for ONNX node: tmp_weight_1255[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.1.weight -> (256, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1256 for ONNX node: tmp_weight_1256[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear_output_0 -> (256, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0 -> (256)[FLOAT]], [/model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_output_0 -> (256)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1257 for ONNX node: tmp_weight_1257[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose_output_0 -> (256, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1258 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1259 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.1.bias -> (256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.1.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1260 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1261 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.1/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.1/Add_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu for ONNX node: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/act_1/Relu [Relu] outputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/act_1/Relu_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1262 for ONNX node: tmp_weight_1262[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear_output_0 -> (1, 300, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0 -> ()[FLOAT]], [/model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_output_0 -> ()[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1263 for ONNX node: tmp_weight_1263[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_bbox_head.2.layers.2.weight -> (4, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1264 for ONNX node: tmp_weight_1264[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear_output_0 -> (4, 256)[INT8]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0 -> (4)[FLOAT]], [/model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_output_0 -> (4)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1265 for ONNX node: tmp_weight_1265[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear_output_0 -> (4, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose_output_0 -> (256, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1266 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1267 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] inputs: [model.decoder.dec_bbox_head.2.layers.2.bias -> (4)[FLOAT]], [/model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_bbox_head.2.layers.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1268 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1269 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add for ONNX node: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_bbox_head.2/layers.2/Add [Add] outputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_6 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_6 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_6 [Clip] inputs: [/model/decoder/decoder/Sigmoid_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_1_output_0 -> ()[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Clip_6 for ONNX node: /model/decoder/decoder/Clip_6[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_6_output_0 for ONNX tensor: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_6 [Clip] outputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_7 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_7 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_7 [Clip] inputs: [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1271 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1272 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1273 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1274 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_7_output_0 for ONNX tensor: /model/decoder/decoder/Clip_7_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_7 [Clip] outputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sub_2 [Sub][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sub_2 [Sub][0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_6_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_2 [Sub] inputs: [/model/encoder/encoder.0/layers.0/activation/Constant_1_output_0 -> ()[FLOAT]], [/model/decoder/decoder/Clip_6_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1275 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1276 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sub_2 for ONNX node: /model/decoder/decoder/Sub_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sub_2_output_0 for ONNX tensor: /model/decoder/decoder/Sub_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sub_2 [Sub] outputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Clip_8 [Clip][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Clip_8 [Clip][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sub_2_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Constant_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_8 [Clip] inputs: [/model/decoder/decoder/Sub_2_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Constant_3_output_0 -> ()[FLOAT]], [optional input, not set], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1278 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1279 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1280 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1281 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Clip_8_output_0 for ONNX tensor: /model/decoder/decoder/Clip_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Clip_8 [Clip] outputs: [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Div_2 [Div][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Div_2 [Div][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_7_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Clip_8_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_2 [Div] inputs: [/model/decoder/decoder/Clip_7_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Clip_8_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Div_2 for ONNX node: /model/decoder/decoder/Div_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Div_2_output_0 for ONNX tensor: /model/decoder/decoder/Div_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Div_2 [Div] outputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Log_2 [Log][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Log_2 [Log][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Div_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_2 [Log] inputs: [/model/decoder/decoder/Div_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Log_2 for ONNX node: /model/decoder/decoder/Log_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Log_2_output_0 for ONNX tensor: /model/decoder/decoder/Log_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Log_2 [Log] outputs: [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Add_2 [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Add_2 [Add][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Log_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_2 [Add] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.2/Add_output_0 -> (1, 300, 4)[FLOAT]], [/model/decoder/decoder/Log_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Add_2 for ONNX node: /model/decoder/decoder/Add_2[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Add_2_output_0 for ONNX tensor: /model/decoder/decoder/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Add_2 [Add] outputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Sigmoid_3 [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Add_2_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] inputs: [/model/decoder/decoder/Add_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Sigmoid_3 for ONNX node: /model/decoder/decoder/Sigmoid_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Sigmoid_3_output_0 for ONNX tensor: /model/decoder/decoder/Sigmoid_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Sigmoid_3 [Sigmoid] outputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear][0m
[38;5;104m[X] Searching for input: model.decoder.dec_score_head.2.weight[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] inputs: [model.decoder.dec_score_head.2.weight -> (80, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_score_head.2.weight required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Registering layer: tmp_weight_1282 for ONNX node: tmp_weight_1282[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear [QuantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/enc_score_head/weight_quantizer/Constant_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear_output_0 -> (80, 256)[INT8]], [/model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0 -> (80)[FLOAT]], [/model/decoder/enc_score_head/weight_quantizer/Constant_output_0 -> (80)[INT8]], [0m
[38;5;104m[X] Registering layer: tmp_weight_1283 for ONNX node: tmp_weight_1283[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear for ONNX node: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear [DequantizeLinear] outputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Transpose [Transpose][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] inputs: [/model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear_output_0 -> (80, 256)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Transpose for ONNX node: /model/decoder/decoder/dec_score_head.2/Transpose[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Transpose [Transpose] outputs: [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/MatMul [MatMul][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Transpose_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] inputs: [/model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear_output_0 -> (1, 300, 256)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/Transpose_output_0 -> (256, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1284 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1285 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/MatMul for ONNX node: /model/decoder/decoder/dec_score_head.2/MatMul[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/MatMul [MatMul] outputs: [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/dec_score_head.2/Add [Add][0m
[38;5;104m[X] Searching for input: model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/MatMul_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Add [Add] inputs: [model.decoder.dec_score_head.2.bias -> (80)[FLOAT]], [/model/decoder/decoder/dec_score_head.2/MatMul_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: model.decoder.dec_score_head.2.bias required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1286 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1287 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/dec_score_head.2/Add for ONNX node: /model/decoder/decoder/dec_score_head.2/Add[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0 for ONNX tensor: /model/decoder/decoder/dec_score_head.2/Add_output_0[0m
[38;5;104m[X] /model/decoder/decoder/dec_score_head.2/Add [Add] outputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Sigmoid_3_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] inputs: [/model/decoder/decoder/Sigmoid_3_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Unsqueeze_3 for ONNX node: /model/decoder/decoder/Unsqueeze_3[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Unsqueeze_3_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_3_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_3 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /model/decoder/decoder/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/dec_score_head.2/Add_output_0[0m
[38;5;104m[X] Searching for input: onnx::Unsqueeze_1255[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] inputs: [/model/decoder/decoder/dec_score_head.2/Add_output_0 -> (1, 300, 80)[FLOAT]], [onnx::Unsqueeze_1255 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/decoder/Unsqueeze_4 for ONNX node: /model/decoder/decoder/Unsqueeze_4[0m
[38;5;104m[X] Registering tensor: /model/decoder/decoder/Unsqueeze_4_output_0 for ONNX tensor: /model/decoder/decoder/Unsqueeze_4_output_0[0m
[38;5;104m[X] /model/decoder/decoder/Unsqueeze_4 [Unsqueeze] outputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Gather_8 [Gather][0m
[38;5;104m[X] Parsing node: /model/decoder/Gather_8 [Gather][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Unsqueeze_4_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/Gather_8 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_4_output_0 -> (1, 1, 300, 80)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: /model/encoder/Constant_2_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Using Gather axis: 0[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1288 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Gather_8 for ONNX node: /model/decoder/Gather_8[0m
[38;5;104m[X] Registering tensor: /model/decoder/Gather_8_output_0 for ONNX tensor: /model/decoder/Gather_8_output_0[0m
[38;5;104m[X] /model/decoder/Gather_8 [Gather] outputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /model/decoder/Gather_9 [Gather][0m
[38;5;104m[X] Parsing node: /model/decoder/Gather_9 [Gather][0m
[38;5;104m[X] Searching for input: /model/decoder/decoder/Unsqueeze_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_2_output_0[0m
[38;5;104m[X] /model/decoder/Gather_9 [Gather] inputs: [/model/decoder/decoder/Unsqueeze_3_output_0 -> (1, 1, 300, 4)[FLOAT]], [/model/encoder/Constant_2_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Using Gather axis: 0[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1289 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /model/decoder/Gather_9 for ONNX node: /model/decoder/Gather_9[0m
[38;5;104m[X] Registering tensor: /model/decoder/Gather_9_output_0 for ONNX tensor: /model/decoder/Gather_9_output_0[0m
[38;5;104m[X] /model/decoder/Gather_9 [Gather] outputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Split [Split][0m
[38;5;104m[X] Parsing node: /postprocessor/Split [Split][0m
[38;5;104m[X] Searching for input: /model/decoder/Gather_9_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_output_0[0m
[38;5;104m[X] /postprocessor/Split [Split] inputs: [/model/decoder/Gather_9_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Constant_output_0 -> (4)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1290 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1291 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1292 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1293 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1294 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1295 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Split_1296 for ONNX node: /postprocessor/Split[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_0 for ONNX tensor: /postprocessor/Split_output_0[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_1 for ONNX tensor: /postprocessor/Split_output_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_2 for ONNX tensor: /postprocessor/Split_output_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Split_output_3 for ONNX tensor: /postprocessor/Split_output_3[0m
[38;5;104m[X] /postprocessor/Split [Split] outputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze [Squeeze] inputs: [/postprocessor/Split_output_0 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze for ONNX node: /postprocessor/Squeeze[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_output_0 for ONNX tensor: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze [Squeeze] outputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_1 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_1 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_1[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_1 [Squeeze] inputs: [/postprocessor/Split_output_1 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_1 for ONNX node: /postprocessor/Squeeze_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_1_output_0 for ONNX tensor: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_1 [Squeeze] outputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_2 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_2 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_2[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_2 [Squeeze] inputs: [/postprocessor/Split_output_2 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_2 for ONNX node: /postprocessor/Squeeze_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_2_output_0 for ONNX tensor: /postprocessor/Squeeze_2_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_2 [Squeeze] outputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Squeeze_3 [Squeeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Squeeze_3 [Squeeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Split_output_3[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_3 [Squeeze] inputs: [/postprocessor/Split_output_3 -> (1, 300, 1)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Squeeze_3 for ONNX node: /postprocessor/Squeeze_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Squeeze_3_output_0 for ONNX tensor: /postprocessor/Squeeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Squeeze_3 [Squeeze] outputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_2_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul [Mul] inputs: [/postprocessor/Squeeze_2_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1297 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1298 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul for ONNX node: /postprocessor/Mul[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_output_0 for ONNX tensor: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Mul [Mul] outputs: [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Sub [Sub] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub for ONNX node: /postprocessor/Sub[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sub_output_0 for ONNX tensor: /postprocessor/Sub_output_0[0m
[38;5;104m[X] /postprocessor/Sub [Sub] outputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_1 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_1 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_3_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/encoder.0/layers.0/activation/Constant_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul_1 [Mul] inputs: [/postprocessor/Squeeze_3_output_0 -> (1, 300)[FLOAT]], [/model/encoder/encoder.0/layers.0/activation/Constant_2_output_0 -> ()[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1299 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1300 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_1 for ONNX node: /postprocessor/Mul_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_1_output_0 for ONNX tensor: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Mul_1 [Mul] outputs: [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub_1 [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub_1 [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Sub_1 [Sub] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub_1 for ONNX node: /postprocessor/Sub_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sub_1_output_0 for ONNX tensor: /postprocessor/Sub_1_output_0[0m
[38;5;104m[X] /postprocessor/Sub_1 [Sub] outputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Add [Add][0m
[38;5;104m[X] Parsing node: /postprocessor/Add [Add][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_output_0[0m
[38;5;104m[X] /postprocessor/Add [Add] inputs: [/postprocessor/Squeeze_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Add for ONNX node: /postprocessor/Add[0m
[38;5;104m[X] Registering tensor: /postprocessor/Add_output_0 for ONNX tensor: /postprocessor/Add_output_0[0m
[38;5;104m[X] /postprocessor/Add [Add] outputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Add_1 [Add][0m
[38;5;104m[X] Parsing node: /postprocessor/Add_1 [Add][0m
[38;5;104m[X] Searching for input: /postprocessor/Squeeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_1_output_0[0m
[38;5;104m[X] /postprocessor/Add_1 [Add] inputs: [/postprocessor/Squeeze_1_output_0 -> (1, 300)[FLOAT]], [/postprocessor/Mul_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Add_1 for ONNX node: /postprocessor/Add_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Add_1_output_0 for ONNX tensor: /postprocessor/Add_1_output_0[0m
[38;5;104m[X] /postprocessor/Add_1 [Add] outputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Sub_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze [Unsqueeze] inputs: [/postprocessor/Sub_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze for ONNX node: /postprocessor/Unsqueeze[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_output_0 for ONNX tensor: /postprocessor/Unsqueeze_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze [Unsqueeze] outputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_1 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Sub_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_1 [Unsqueeze] inputs: [/postprocessor/Sub_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_1 for ONNX node: /postprocessor/Unsqueeze_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_1_output_0 for ONNX tensor: /postprocessor/Unsqueeze_1_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_1 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_2 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Add_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_2 [Unsqueeze] inputs: [/postprocessor/Add_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_2 for ONNX node: /postprocessor/Unsqueeze_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_2_output_0 for ONNX tensor: /postprocessor/Unsqueeze_2_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_2 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_3 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Add_1_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_3 [Unsqueeze] inputs: [/postprocessor/Add_1_output_0 -> (1, 300)[FLOAT]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_3 for ONNX node: /postprocessor/Unsqueeze_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_3_output_0 for ONNX tensor: /postprocessor/Unsqueeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_3 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Concat [Concat][0m
[38;5;104m[X] Parsing node: /postprocessor/Concat [Concat][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_1_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_2_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_3_output_0[0m
[38;5;104m[X] /postprocessor/Concat [Concat] inputs: [/postprocessor/Unsqueeze_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_1_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_2_output_0 -> (1, 300, 1)[FLOAT]], [/postprocessor/Unsqueeze_3_output_0 -> (1, 300, 1)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Concat for ONNX node: /postprocessor/Concat[0m
[38;5;104m[X] Registering tensor: /postprocessor/Concat_output_0 for ONNX tensor: /postprocessor/Concat_output_0[0m
[38;5;104m[X] /postprocessor/Concat [Concat] outputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Tile [Tile][0m
[38;5;104m[X] Parsing node: /postprocessor/Tile [Tile][0m
[38;5;104m[X] Searching for input: orig_target_sizes[0m
[38;5;104m[X] Searching for input: onnx::Tile_3498[0m
[38;5;104m[X] /postprocessor/Tile [Tile] inputs: [orig_target_sizes -> (1, 2)[INT64]], [onnx::Tile_3498 -> (2)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1301 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Tile for ONNX node: /postprocessor/Tile[0m
[38;5;104m[X] Registering tensor: /postprocessor/Tile_output_0 for ONNX tensor: /postprocessor/Tile_output_0[0m
[38;5;104m[X] /postprocessor/Tile [Tile] outputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_4 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Tile_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_21_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_4 [Unsqueeze] inputs: [/postprocessor/Tile_output_0 -> (1, 4)[INT64]], [/model/decoder/Constant_21_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /model/decoder/Constant_21_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_4 for ONNX node: /postprocessor/Unsqueeze_4[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_4_output_0 for ONNX tensor: /postprocessor/Unsqueeze_4_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_4 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: Cast_3039 [Cast][0m
[38;5;104m[X] Parsing node: Cast_3039 [Cast][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_4_output_0[0m
[38;5;104m[X] Cast_3039 [Cast] inputs: [/postprocessor/Unsqueeze_4_output_0 -> (1, 1, 4)[INT64]], [0m
[38;5;104m[X] Casting to type: float32[0m
[38;5;104m[X] Registering layer: Cast_3039 for ONNX node: Cast_3039[0m
[38;5;104m[X] Registering tensor: onnx::Mul_3505 for ONNX tensor: onnx::Mul_3505[0m
[38;5;104m[X] Cast_3039 [Cast] outputs: [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_2 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_2 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Concat_output_0[0m
[38;5;104m[X] Searching for input: onnx::Mul_3505[0m
[38;5;104m[X] /postprocessor/Mul_2 [Mul] inputs: [/postprocessor/Concat_output_0 -> (1, 300, 4)[FLOAT]], [onnx::Mul_3505 -> (1, 1, 4)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_2 for ONNX node: /postprocessor/Mul_2[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_2_output_0 for ONNX tensor: /postprocessor/Mul_2_output_0[0m
[38;5;104m[X] /postprocessor/Mul_2 [Mul] outputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sigmoid [Sigmoid][0m
[38;5;104m[X] Parsing node: /postprocessor/Sigmoid [Sigmoid][0m
[38;5;104m[X] Searching for input: /model/decoder/Gather_8_output_0[0m
[38;5;104m[X] /postprocessor/Sigmoid [Sigmoid] inputs: [/model/decoder/Gather_8_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sigmoid for ONNX node: /postprocessor/Sigmoid[0m
[38;5;104m[X] Registering tensor: /postprocessor/Sigmoid_output_0 for ONNX tensor: /postprocessor/Sigmoid_output_0[0m
[38;5;104m[X] /postprocessor/Sigmoid [Sigmoid] outputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Flatten [Flatten][0m
[38;5;104m[X] Parsing node: /postprocessor/Flatten [Flatten][0m
[38;5;104m[X] Searching for input: /postprocessor/Sigmoid_output_0[0m
[38;5;104m[X] /postprocessor/Flatten [Flatten] inputs: [/postprocessor/Sigmoid_output_0 -> (1, 300, 80)[FLOAT]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1302 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Flatten for ONNX node: /postprocessor/Flatten[0m
[38;5;104m[X] Registering tensor: /postprocessor/Flatten_output_0 for ONNX tensor: /postprocessor/Flatten_output_0[0m
[38;5;104m[X] /postprocessor/Flatten [Flatten] outputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/TopK [TopK][0m
[38;5;104m[X] Parsing node: /postprocessor/TopK [TopK][0m
[38;5;104m[X] Searching for input: /postprocessor/Flatten_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Constant_18_output_0[0m
[38;5;104m[X] /postprocessor/TopK [TopK] inputs: [/postprocessor/Flatten_output_0 -> (1, 24000)[FLOAT]], [/model/decoder/Constant_18_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_convertToScalar_1303 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/TopK for ONNX node: /postprocessor/TopK[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1304 required by ONNX-TRT[0m
[38;5;104m[X] Registering tensor: scores_1305 for ONNX tensor: scores[0m
[38;5;104m[X] Registering tensor: /postprocessor/TopK_output_1 for ONNX tensor: /postprocessor/TopK_output_1[0m
[38;5;104m[X] /postprocessor/TopK [TopK] outputs: [scores -> (1, 300)[FLOAT]], [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Div [Div][0m
[38;5;104m[X] Parsing node: /postprocessor/Div [Div][0m
[38;5;104m[X] Searching for input: /postprocessor/TopK_output_1[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] /postprocessor/Div [Div] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Constant_14_output_0 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1306 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1307 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Div for ONNX node: /postprocessor/Div[0m
[38;5;104m[X] Registering tensor: /postprocessor/Div_output_0 for ONNX tensor: /postprocessor/Div_output_0[0m
[38;5;104m[X] /postprocessor/Div [Div] outputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Mul_3 [Mul][0m
[38;5;104m[X] Parsing node: /postprocessor/Mul_3 [Mul][0m
[38;5;104m[X] Searching for input: /postprocessor/Div_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Constant_14_output_0[0m
[38;5;104m[X] /postprocessor/Mul_3 [Mul] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/postprocessor/Constant_14_output_0 -> ()[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeShuffle_1308 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: ONNXTRT_Broadcast_1309 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Mul_3 for ONNX node: /postprocessor/Mul_3[0m
[38;5;104m[X] Registering tensor: /postprocessor/Mul_3_output_0 for ONNX tensor: /postprocessor/Mul_3_output_0[0m
[38;5;104m[X] /postprocessor/Mul_3 [Mul] outputs: [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Sub_2 [Sub][0m
[38;5;104m[X] Parsing node: /postprocessor/Sub_2 [Sub][0m
[38;5;104m[X] Searching for input: /postprocessor/TopK_output_1[0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_3_output_0[0m
[38;5;104m[X] /postprocessor/Sub_2 [Sub] inputs: [/postprocessor/TopK_output_1 -> (1, 300)[INT64]], [/postprocessor/Mul_3_output_0 -> (1, 300)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Sub_2 for ONNX node: /postprocessor/Sub_2[0m
[38;5;104m[X] Registering tensor: labels_1310 for ONNX tensor: labels[0m
[38;5;104m[X] /postprocessor/Sub_2 [Sub] outputs: [labels -> (1, 300)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze][0m
[38;5;104m[X] Parsing node: /postprocessor/Unsqueeze_5 [Unsqueeze][0m
[38;5;104m[X] Searching for input: /postprocessor/Div_output_0[0m
[38;5;104m[X] Searching for input: /model/encoder/Constant_7_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_5 [Unsqueeze] inputs: [/postprocessor/Div_output_0 -> (1, 300)[INT64]], [/model/encoder/Constant_7_output_0 -> (1)[INT64]], [0m
[38;5;104m[X] Registering layer: /postprocessor/Unsqueeze_5 for ONNX node: /postprocessor/Unsqueeze_5[0m
[38;5;104m[X] Registering tensor: /postprocessor/Unsqueeze_5_output_0 for ONNX tensor: /postprocessor/Unsqueeze_5_output_0[0m
[38;5;104m[X] /postprocessor/Unsqueeze_5 [Unsqueeze] outputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/Tile_1 [Tile][0m
[38;5;104m[X] Parsing node: /postprocessor/Tile_1 [Tile][0m
[38;5;104m[X] Searching for input: /postprocessor/Unsqueeze_5_output_0[0m
[38;5;104m[X] Searching for input: /model/decoder/Concat_5_output_0[0m
[38;5;104m[X] /postprocessor/Tile_1 [Tile] inputs: [/postprocessor/Unsqueeze_5_output_0 -> (1, 300, 1)[INT64]], [/model/decoder/Concat_5_output_0 -> (3)[INT64]], [0m
[38;5;104m[X] Registering layer: ONNXTRT_ShapeSlice_1311 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/Tile_1 for ONNX node: /postprocessor/Tile_1[0m
[38;5;104m[X] Registering tensor: /postprocessor/Tile_1_output_0 for ONNX tensor: /postprocessor/Tile_1_output_0[0m
[38;5;104m[X] /postprocessor/Tile_1 [Tile] outputs: [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Static check for parsing node: /postprocessor/GatherElements [GatherElements][0m
[38;5;104m[X] Parsing node: /postprocessor/GatherElements [GatherElements][0m
[38;5;104m[X] Searching for input: /postprocessor/Mul_2_output_0[0m
[38;5;104m[X] Searching for input: /postprocessor/Tile_1_output_0[0m
[38;5;104m[X] /postprocessor/GatherElements [GatherElements] inputs: [/postprocessor/Mul_2_output_0 -> (1, 300, 4)[FLOAT]], [/postprocessor/Tile_1_output_0 -> (1, 300, 4)[INT64]], [0m
[38;5;104m[X] Using Gather axis: 1[0m
[38;5;104m[X] Registering layer: ONNXTRT_castHelper_1312 required by ONNX-TRT[0m
[38;5;104m[X] Registering layer: /postprocessor/GatherElements for ONNX node: /postprocessor/GatherElements[0m
[38;5;104m[X] Registering tensor: boxes_1313 for ONNX tensor: boxes[0m
[38;5;104m[X] /postprocessor/GatherElements [GatherElements] outputs: [boxes -> (1, 300, 4)[FLOAT]], [0m
[38;5;104m[X] Marking labels_1310 as output: labels[0m
[38;5;11m[W] ModelImporter.cpp:804: Make sure output labels has Int64 binding.[0m
[38;5;104m[X] Marking boxes_1313 as output: boxes[0m
[38;5;104m[X] Marking scores_1305 as output: scores[0m
[38;5;14m[I] Building engine with configuration:
    Flags                  | [TF32]
    Engine Capability      | EngineCapability.STANDARD
    Memory Pools           | [WORKSPACE: 1024.00 MiB, TACTIC_DRAM: 24105.06 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]
    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]
    Profiling Verbosity    | ProfilingVerbosity.DETAILED
    Preview Features       | [PROFILE_SHARING_0806][0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/encoder/encoder.0/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[400,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.0/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.1/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_2: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] /model/decoder/decoder/layers.2/self_attn/MatMul_1: broadcasting input1 to make tensors conform, dims(input0)=[300,1,256][NONE] dims(input1)=[1,256,256][NONE].[0m
[38;5;104m[X] Original: 1920 layers[0m
[38;5;104m[X] After dead-layer removal: 1920 layers[0m
[38;5;104m[X] Graph construction completed in 0.0225153 seconds.[0m
[38;5;104m[X] After adding DebugOutput nodes: 1920 layers[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3619[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3619 with ONNXTRT_Broadcast[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3614[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3614 with ONNXTRT_Broadcast_99[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3620[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3620 with ONNXTRT_Broadcast_101[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3616[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3616 with ONNXTRT_Broadcast_103[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3621[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3621 with ONNXTRT_Broadcast_105[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3618[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3618 with ONNXTRT_Broadcast_107[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_116[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.weight with ONNXTRT_Broadcast_121[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm1.bias with ONNXTRT_Broadcast_123[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear1.bias with ONNXTRT_Broadcast_131[0m
[38;5;104m[X] Running: ConstShuffleFusion on /model/encoder/encoder.0/layers.0/activation/Constant_output_0[0m
[38;5;104m[X] ConstShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/activation/Constant_output_0 with ONNXTRT_Broadcast_133[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.linear2.bias with ONNXTRT_Broadcast_145[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.weight with ONNXTRT_Broadcast_149[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.encoder.encoder.0.layers.0.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.encoder.encoder.0.layers.0.norm2.bias with ONNXTRT_Broadcast_151[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.proj.bias with ONNXTRT_Broadcast_275[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.weight with ONNXTRT_Broadcast_279[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_output.norm.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_output.norm.bias with ONNXTRT_Broadcast_281[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_score_head.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_score_head.bias with ONNXTRT_Broadcast_289[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.0.bias with ONNXTRT_Broadcast_295[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.1.bias with ONNXTRT_Broadcast_303[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.enc_bbox_head.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.enc_bbox_head.layers.2.bias with ONNXTRT_Broadcast_311[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3736[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3736 with ONNXTRT_Broadcast_332[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3731[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3731 with ONNXTRT_Broadcast_334[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3737[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3737 with ONNXTRT_Broadcast_336[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3733[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3733 with ONNXTRT_Broadcast_338[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3738[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3738 with ONNXTRT_Broadcast_340[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3735[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3735 with ONNXTRT_Broadcast_342[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.self_attn.out_proj.bias with ONNXTRT_Broadcast_351[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.weight with ONNXTRT_Broadcast_356[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm1.bias with ONNXTRT_Broadcast_358[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.value_proj.bias with ONNXTRT_Broadcast_366[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_375[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_382[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.cross_attn.output_proj.bias with ONNXTRT_Broadcast_575[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.weight with ONNXTRT_Broadcast_579[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm2.bias with ONNXTRT_Broadcast_581[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear1.bias with ONNXTRT_Broadcast_589[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.linear2.bias with ONNXTRT_Broadcast_597[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.weight with ONNXTRT_Broadcast_601[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.0.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.0.norm3.bias with ONNXTRT_Broadcast_603[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.0.bias with ONNXTRT_Broadcast_611[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.1.bias with ONNXTRT_Broadcast_619[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.0.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.0.layers.2.bias with ONNXTRT_Broadcast_627[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1426) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1426) [Constant] with ONNXTRT_Broadcast_632[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1433) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1433) [Constant] with ONNXTRT_Broadcast_639[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3808[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3808 with ONNXTRT_Broadcast_653[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3803[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3803 with ONNXTRT_Broadcast_655[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3809[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3809 with ONNXTRT_Broadcast_657[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3805[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3805 with ONNXTRT_Broadcast_659[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3810[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3810 with ONNXTRT_Broadcast_661[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3807[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3807 with ONNXTRT_Broadcast_663[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.self_attn.out_proj.bias with ONNXTRT_Broadcast_672[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.weight with ONNXTRT_Broadcast_677[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm1.bias with ONNXTRT_Broadcast_679[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.value_proj.bias with ONNXTRT_Broadcast_685[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_694[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_701[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.cross_attn.output_proj.bias with ONNXTRT_Broadcast_896[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.weight with ONNXTRT_Broadcast_900[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm2.bias with ONNXTRT_Broadcast_902[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear1.bias with ONNXTRT_Broadcast_910[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.linear2.bias with ONNXTRT_Broadcast_918[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.weight with ONNXTRT_Broadcast_922[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.1.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.1.norm3.bias with ONNXTRT_Broadcast_924[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.0.bias with ONNXTRT_Broadcast_932[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.1.bias with ONNXTRT_Broadcast_940[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.1.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.1.layers.2.bias with ONNXTRT_Broadcast_948[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1834) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1834) [Constant] with ONNXTRT_Broadcast_953[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 1841) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 1841) [Constant] with ONNXTRT_Broadcast_960[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3880[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3880 with ONNXTRT_Broadcast_974[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3875[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3875 with ONNXTRT_Broadcast_976[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3881[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3881 with ONNXTRT_Broadcast_978[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3877[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3877 with ONNXTRT_Broadcast_980[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::MatMul_3882[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::MatMul_3882 with ONNXTRT_Broadcast_982[0m
[38;5;104m[X] Running: ConstShuffleFusion on onnx::Add_3879[0m
[38;5;104m[X] ConstShuffleFusion: Fusing onnx::Add_3879 with ONNXTRT_Broadcast_984[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.self_attn.out_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.self_attn.out_proj.bias with ONNXTRT_Broadcast_993[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.weight with ONNXTRT_Broadcast_998[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm1.bias with ONNXTRT_Broadcast_1000[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.value_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.value_proj.bias with ONNXTRT_Broadcast_1006[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.sampling_offsets.bias with ONNXTRT_Broadcast_1015[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.attention_weights.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.attention_weights.bias with ONNXTRT_Broadcast_1022[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.cross_attn.output_proj.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.cross_attn.output_proj.bias with ONNXTRT_Broadcast_1217[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.weight with ONNXTRT_Broadcast_1221[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm2.bias with ONNXTRT_Broadcast_1223[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear1.bias with ONNXTRT_Broadcast_1231[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.linear2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.linear2.bias with ONNXTRT_Broadcast_1239[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.weight[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.weight with ONNXTRT_Broadcast_1243[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.decoder.layers.2.norm3.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.decoder.layers.2.norm3.bias with ONNXTRT_Broadcast_1245[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.0.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.0.bias with ONNXTRT_Broadcast_1253[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.1.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.1.bias with ONNXTRT_Broadcast_1261[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_bbox_head.2.layers.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_bbox_head.2.layers.2.bias with ONNXTRT_Broadcast_1269[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 2242) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2242) [Constant] with ONNXTRT_Broadcast_1274[0m
[38;5;104m[X] Running: ConstShuffleFusion on (Unnamed Layer* 2249) [Constant][0m
[38;5;104m[X] ConstShuffleFusion: Fusing (Unnamed Layer* 2249) [Constant] with ONNXTRT_Broadcast_1281[0m
[38;5;104m[X] Running: ConstShuffleFusion on model.decoder.dec_score_head.2.bias[0m
[38;5;104m[X] ConstShuffleFusion: Fusing model.decoder.dec_score_head.2.bias with ONNXTRT_Broadcast_1287[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear1/Transpose with ONNXTRT_Broadcast_129[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/linear2/Transpose with ONNXTRT_Broadcast_143[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_output/proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_output/proj/Transpose with ONNXTRT_Broadcast_273[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_score_head/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_score_head/Transpose with ONNXTRT_Broadcast_287[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.0/Transpose with ONNXTRT_Broadcast_293[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.1/Transpose with ONNXTRT_Broadcast_301[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/enc_bbox_head/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/enc_bbox_head/layers.2/Transpose with ONNXTRT_Broadcast_309[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_364[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_373[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_380[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_573[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear1/Transpose with ONNXTRT_Broadcast_587[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/linear2/Transpose with ONNXTRT_Broadcast_595[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.0/Transpose with ONNXTRT_Broadcast_609[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.1/Transpose with ONNXTRT_Broadcast_617[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.0/layers.2/Transpose with ONNXTRT_Broadcast_625[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_683[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_692[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_699[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_894[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear1/Transpose with ONNXTRT_Broadcast_908[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/linear2/Transpose with ONNXTRT_Broadcast_916[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.0/Transpose with ONNXTRT_Broadcast_930[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.1/Transpose with ONNXTRT_Broadcast_938[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.1/layers.2/Transpose with ONNXTRT_Broadcast_946[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/value_proj/Transpose with ONNXTRT_Broadcast_1004[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Transpose with ONNXTRT_Broadcast_1013[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/attention_weights/Transpose with ONNXTRT_Broadcast_1020[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/output_proj/Transpose with ONNXTRT_Broadcast_1215[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear1/Transpose with ONNXTRT_Broadcast_1229[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/linear2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/linear2/Transpose with ONNXTRT_Broadcast_1237[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.0/Transpose with ONNXTRT_Broadcast_1251[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.1/Transpose with ONNXTRT_Broadcast_1259[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_bbox_head.2/layers.2/Transpose with ONNXTRT_Broadcast_1267[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/dec_score_head.2/Transpose[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/dec_score_head.2/Transpose with ONNXTRT_Broadcast_1285[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_2 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape with /model/encoder/encoder.0/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_1 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_113[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_113[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Transpose_5 with /model/encoder/encoder.0/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/encoder.0/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/encoder.0/layers.0/self_attn/Reshape_4 with /model/encoder/encoder.0/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/encoder/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/encoder/Transpose_1 with /model/encoder/Reshape_1[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape with /model/decoder/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_1 with /model/decoder/Transpose_1[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/Reshape_2 with /model/decoder/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape with /model/decoder/decoder/layers.0/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape with /model/decoder/decoder/layers.1/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape with /model/decoder/decoder/layers.2/cross_attn/Transpose[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_2 with /model/decoder/decoder/layers.0/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape with /model/decoder/decoder/layers.0/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_1 with /model/decoder/decoder/layers.0/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_348[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_348[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Transpose_5 with /model/decoder/decoder/layers.0/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/self_attn/Reshape_4 with /model/decoder/decoder/layers.0/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_384[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_384 with /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_384 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_384 + /model/decoder/decoder/layers.0/cross_attn/Transpose_2 with /model/decoder/decoder/layers.0/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Transpose_1 with /model/decoder/decoder/layers.0/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.0/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.0/cross_attn/Reshape_10 with /model/decoder/decoder/layers.0/cross_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_2 with /model/decoder/decoder/layers.1/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape with /model/decoder/decoder/layers.1/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_1 with /model/decoder/decoder/layers.1/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_669[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_669[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Transpose_5 with /model/decoder/decoder/layers.1/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/self_attn/Reshape_4 with /model/decoder/decoder/layers.1/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_703[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_703 with /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_703 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_703 + /model/decoder/decoder/layers.1/cross_attn/Transpose_2 with /model/decoder/decoder/layers.1/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Transpose_1 with /model/decoder/decoder/layers.1/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.1/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.1/cross_attn/Reshape_10 with /model/decoder/decoder/layers.1/cross_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_2 with /model/decoder/decoder/layers.2/self_attn/Transpose_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape with /model/decoder/decoder/layers.2/self_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_1 with /model/decoder/decoder/layers.2/self_attn/Transpose_4[0m
[38;5;104m[X] Running: ShuffleErasure on ONNXTRT_ShapeShuffle_990[0m
[38;5;104m[X] Removing ONNXTRT_ShapeShuffle_990[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Transpose_5[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Transpose_5 with /model/decoder/decoder/layers.2/self_attn/Reshape_3[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/self_attn/Reshape_4[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/self_attn/Reshape_4 with /model/decoder/decoder/layers.2/self_attn/Transpose_6[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1024[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1024 with /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on ONNXTRT_ShapeShuffle_1024 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing ONNXTRT_ShapeShuffle_1024 + /model/decoder/decoder/layers.2/cross_attn/Transpose_2 with /model/decoder/decoder/layers.2/cross_attn/Reshape_9[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Transpose_1[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Transpose_1 with /model/decoder/decoder/layers.2/cross_attn/Reshape_5[0m
[38;5;104m[X] Running: ShuffleShuffleFusion on /model/decoder/decoder/layers.2/cross_attn/Reshape_10[0m
[38;5;104m[X] ShuffleShuffleFusion: Fusing /model/decoder/decoder/layers.2/cross_attn/Reshape_10 with /model/decoder/decoder/layers.2/cross_attn/Transpose_3[0m
[38;5;104m[X] QDQ graph optimizer - constant folding of Q/DQ initializers[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_5[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_9[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_13[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_17[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_19[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_23[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_27[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_31[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_35[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_39[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_43[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_47[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_51[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_55[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_59[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_63[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_67[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_71[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_75[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_79[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_83[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_87[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_89[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_91[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_95[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_126[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_140[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_155[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_159[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_163[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_167[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_171[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_173[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_177[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_181[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_185[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_189[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_193[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_197[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_199[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_203[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_207[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_211[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_215[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_219[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_223[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_225[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_229[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_233[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_237[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_241[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_245[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_249[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_251[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_255[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_257[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_259[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_263[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_270[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_284[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_290[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_298[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_306[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_317[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_325[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_361[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_370[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_377[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_570[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_584[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_592[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_606[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_614[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_622[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_680[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_689[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_696[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_891[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_905[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_913[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_927[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_935[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_943[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1001[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1010[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1017[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1212[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1226[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1234[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1248[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1256[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1264[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1282[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_2[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_10[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_18[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_24[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_32[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_40[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_48[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_56[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_64[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_72[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_80[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_88[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_92[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_127[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_156[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_164[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_172[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_178[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_186[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_194[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_200[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_208[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_216[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_224[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_230[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_238[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_246[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_252[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_258[0m
[38;5;104m[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_264[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_285[0m
[38;5;104m[X] Removing /model/decoder/enc_score_head/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_299[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_318[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_362[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_378[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_585[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_607[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_623[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_690[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_892[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_914[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_936[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1002[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1018[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1227[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1249[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1265[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_3[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_4[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_7[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_8[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_11[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_12[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_15[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_16[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_21[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_22[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_25[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_26[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_29[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_30[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_37[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_38[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_34[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_41[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_42[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_45[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_46[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_49[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_50[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_57[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_58[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_54[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_61[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_62[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_65[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_66[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_69[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_70[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_77[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_78[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_74[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_81[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_82[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_85[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_86[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_93[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_94[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_124[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_125[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_138[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_139[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_153[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_154[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_157[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_158[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_161[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_162[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_165[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_166[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_169[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_170[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_175[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_176[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_179[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_180[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_183[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_184[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_187[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_188[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_191[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_192[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_195[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_196[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_201[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_202[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_205[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_206[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_209[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_210[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_213[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_214[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_217[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_218[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_221[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_222[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_227[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_228[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_231[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_232[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_235[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_236[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_239[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_240[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_243[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_244[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_247[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_248[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_253[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_254[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_261[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_262[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_359[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_268[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_360[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_269[0m
[38;5;104m[X] Removing /model/decoder/enc_output/proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_282[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_score_head/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_283[0m
[38;5;104m[X] Removing /model/decoder/enc_score_head/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_296[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_297[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_304[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_305[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_315[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_316[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_323[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_324[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_368[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_369[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_568[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_569[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_582[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_583[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_590[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_591[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_604[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_605[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_612[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_613[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_620[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_621[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_640[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_641[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_646[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_647[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_687[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_688[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_889[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_890[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_903[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_904[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_911[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_912[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_925[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_926[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_933[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_934[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_941[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_942[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_961[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_962[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_967[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_968[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1008[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1009[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1210[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1211[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1224[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1225[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1232[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1233[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1246[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1247[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1254[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1255[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1262[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1263[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_6[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_14[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_20[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_28[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_36[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_44[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_52[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_60[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_68[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_76[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_84[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_90[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_96[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_141[0m
[38;5;104m[X] Removing /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_160[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_168[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_174[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_182[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_190[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_198[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_204[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_212[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_220[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_226[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_234[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_242[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_250[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_256[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_260[0m
[38;5;104m[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_271[0m
[38;5;104m[X] Removing /model/decoder/enc_output/proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_291[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_307[0m
[38;5;104m[X] Removing /model/decoder/enc_bbox_head/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_326[0m
[38;5;104m[X] Removing /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_371[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_571[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_593[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.0/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_615[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_681[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_697[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_906[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.1/linear1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_928[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_944[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1011[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1213[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1235[0m
[38;5;104m[X] Removing /model/decoder/decoder/layers.2/linear2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1257[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_1283[0m
[38;5;104m[X] Removing /model/decoder/decoder/dec_score_head.2/weight_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_33[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_53[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Running: ConstQDQInitializersFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing tmp_weight_73[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/Constant_1_output_0[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.2/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.1/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_4 to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/decoder/decoder/layers.0/self_attn/Softmax to be part of self-attention pattern.[0m
[38;5;104m[X] Found /model/encoder/encoder.0/layers.0/self_attn/MatMul_3 to be part of self-attention pattern.[0m
[38;5;104m[X] Found and reassigned Myelin backends for Self-Attention nodes[0m
[38;5;104m[X] After Myelin optimization: 464 layers[0m
[38;5;104m[X] QDQ graph optimizer - constant folding of Q/DQ initializers[0m
[38;5;104m[X] QDQ graph optimizer forward pass - DQ motions and fusions[0m
[38;5;104m[X] QDQ graph optimizer backward pass[0m
[38;5;104m[X] QDQ graph optimizer quantization pass - Generate quantized ops[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: SplitDQAcrossFanOut on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.0/Add with /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.0/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.0/blocks.1/Add with /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.0/Add with /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.1/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.1/blocks.1/Add with /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.0/Add with /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.2/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.2/blocks.1/Add with /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.0/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.0/Add with /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Running: EltReluFusion on /model/backbone/res_layers.3/blocks.1/Add[0m
[38;5;104m[X] EltReluFusion: Fusing /model/backbone/res_layers.3/blocks.1/Add with /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_1/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_1/norm/BatchNormalization with /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_2/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_2/norm/BatchNormalization with /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/conv1/conv1_3/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/conv1/conv1_3/norm/BatchNormalization with /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: ScaleActivationFusion on /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization[0m
[38;5;104m[X] ScaleActivationFusion: Fusing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization with /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_1.conv.weight with /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_2.conv.weight with /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.conv1.conv1_3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.conv1.conv1_3.conv.weight with /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight with /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight with /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight with /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.2.conv.weight with /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.0.conv.weight with /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.input_proj.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.input_proj.1.conv.weight with /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.0.conv.weight with /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight with /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight with /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight with /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.lateral_convs.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.lateral_convs.1.conv.weight with /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight with /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight with /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.fpn_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight with /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.0.conv.weight with /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight with /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight with /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.0.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight with /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.downsample_convs.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.downsample_convs.1.conv.weight with /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight with /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight with /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.encoder.pan_blocks.1.conv3.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight with /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.0.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.0.conv.weight with /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.1.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.1.conv.weight with /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsQuantizeFusion on model.decoder.input_proj.2.conv.weight[0m
[38;5;104m[X] ConstWeightsQuantizeFusion: Fusing model.decoder.input_proj.2.conv.weight with /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/backbone/MaxPool[0m
[38;5;104m[X] Swapping /model/backbone/MaxPool with /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_2[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_3[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_4[0m
[38;5;104m[X] Running: SplitQAcrossPrecedingFanIn on /model/encoder/Concat_5[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize[0m
[38;5;104m[X] Swapping /model/encoder/Resize with /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Running: VanillaSwapWithFollowingQ on /model/encoder/Resize_1[0m
[38;5;104m[X] Swapping /model/encoder/Resize_1 with /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Running: HorizontalMergeQNodes on /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Eliminating /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 which duplicates (Q) /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/lateral_convs.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.0/act/Sigmoid with /model/encoder/lateral_convs.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv1/act/Sigmoid with /model/encoder/fpn_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv2/act/Sigmoid with /model/encoder/fpn_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul) with /model/encoder/fpn_blocks.0/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.0/conv3/act/Sigmoid with /model/encoder/fpn_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/lateral_convs.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/lateral_convs.1/act/Sigmoid with /model/encoder/lateral_convs.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv1/act/Sigmoid with /model/encoder/fpn_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv2/act/Sigmoid with /model/encoder/fpn_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul) with /model/encoder/fpn_blocks.1/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/fpn_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/fpn_blocks.1/conv3/act/Sigmoid with /model/encoder/fpn_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/downsample_convs.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.0/act/Sigmoid with /model/encoder/downsample_convs.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv1/act/Sigmoid with /model/encoder/pan_blocks.0/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv2/act/Sigmoid with /model/encoder/pan_blocks.0/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul) with /model/encoder/pan_blocks.0/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.0/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.0/conv3/act/Sigmoid with /model/encoder/pan_blocks.0/conv3/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/downsample_convs.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/downsample_convs.1/act/Sigmoid with /model/encoder/downsample_convs.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv1/act/Sigmoid with /model/encoder/pan_blocks.1/conv1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv2/act/Sigmoid with /model/encoder/pan_blocks.1/conv2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul) with /model/encoder/pan_blocks.1/Add[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul[0m
[38;5;104m[X] Running: PointWiseFusion on PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul)[0m
[38;5;104m[X] PointWiseFusion: Fusing PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul) with PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)[0m
[38;5;104m[X] Running: PointWiseFusion on /model/encoder/pan_blocks.1/conv3/act/Sigmoid[0m
[38;5;104m[X] PointWiseFusion: Fusing /model/encoder/pan_blocks.1/conv3/act/Sigmoid with /model/encoder/pan_blocks.1/conv3/act/Mul[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/norm/BatchNormalization + /model/backbone/conv1/conv1_1/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/norm/BatchNormalization + /model/backbone/conv1/conv1_2/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/norm/BatchNormalization + /model/backbone/conv1/conv1_3/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization + /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization[0m
[38;5;104m[X] Running: QConvOrDeconvScaleFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.0/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/lateral_convs.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/lateral_convs.1/conv/Conv with PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/fpn_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/fpn_blocks.1/conv3/conv/Conv with PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.0/conv/Conv with PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.0/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.0/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/downsample_convs.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/downsample_convs.1/conv/Conv with PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv1/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv with PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: GenericConvActFusion on /model/encoder/pan_blocks.1/conv3/conv/Conv[0m
[38;5;104m[X] GenericConvActFusion: Fusing /model/encoder/pan_blocks.1/conv3/conv/Conv with PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear into /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear and /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear) into /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1 and /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear and /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2[0m
[38;5;104m[X] Removing /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear and /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1 and /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear and /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear and /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear) into /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear[0m
[38;5;104m[X] Removing /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0 and /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear) into /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1 into /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2 and /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_2[0m
[38;5;104m[X] Removing /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0 into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeDoubleInputNodes on /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] QuantizeDoubleInputNodes: fusing (/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0 and /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear) into /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear_clone_0[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeSingleInputNodes on /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Removing /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear_clone_1[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] Removing /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: QuantizeGenericNodes on PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] QuantizeGenericNodes: fusing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear into PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] Removing /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear with /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear with /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear with /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Running: ConstWeightsFusion on model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear[0m
[38;5;104m[X] ConstWeightsFusion: Fusing model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear with /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv with /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv with /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] Running: ConvEltwiseSumFusion on model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv[0m
[38;5;104m[X] ConvEltwiseSumFusion: Fusing model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv with /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] QDQ graph optimizer quantization epilogue pass[0m
[38;5;104m[X] QDQ optimization pass[0m
[38;5;104m[X] QDQ graph optimizer constant fold dangling QDQ pass[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] Swap the layer type of /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] Swap the layer type of /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 from QUANTIZE to kQDQ[0m
[38;5;104m[X] Running: QDQToCopy on /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] Swap the layer type of /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 from QUANTIZE to kQDQ[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Running: ConvSwishGeToSsTransformation on model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Modifying configuration of model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] After vertical fusions: 86 layers[0m
[38;5;104m[X] After dupe layer removal: 86 layers[0m
[38;5;104m[X] After final dead-layer removal: 86 layers[0m
[38;5;104m[X] After tensor merging: 86 layers[0m
[38;5;104m[X] After slice removal: 86 layers[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_5[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_5_/model/encoder/lateral_convs.0/act/Mul_output_0_clone_1 to /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_4[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Generating copy for /model/encoder/Resize_1_output_0 to /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_3[0m
[38;5;104m[X] Generating copy for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] Eliminating concatenation /model/encoder/Concat_2[0m
[38;5;104m[X] Generating copy for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 because input does not support striding.[0m
[38;5;104m[X] Retargeting /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1 to /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0[0m
[38;5;104m[X] After concat removal: 85 layers[0m
[38;5;104m[X] Trying to split Reshape and strided tensor[0m
[38;5;104m[X] Graph optimization time: 0.110042 seconds.[0m
[38;5;104m[X] Building graph using backend strategy 2[0m
[38;5;13m[V] Local timing cache in use. Profiling results in this builder pass will not be stored.[0m
[38;5;104m[X] Constructing optimization profile number 0 [1/1].[0m
[38;5;104m[X] Applying generic optimizations to the graph for inference.[0m
[38;5;104m[X] Reserving memory for host IO tensors. Host: 0 bytes[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(3276800,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1228800,409600,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_chw_int8int8_tilesize16x16_k32_fltsteps1_threadspercta256_r3s3_u2v2_scalebias_relu Tactic: 0x11764d94950382f8 Time: 0.00590821[0m
[38;5;104m[X] Tactic Name: ampere_first_layer_filter3x3_imma_fwd Tactic: 0x9ae0c0d2fb3a01e5 Time: 0.00650872[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00716736 seconds. Fastest Tactic: 0x11764d94950382f8 Time: 0.00590821[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x11764d94950382f8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,409600:4,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize16x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x3d988d07a78b0918 Time: 0.00508622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8 Tactic: 0x5cc792a989a1d1a6 Time: 0.00498754[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00971215[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00968259[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0358918 seconds. Fastest Tactic: 0x5cc792a989a1d1a6 Time: 0.00498754[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5cc792a989a1d1a6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1:16,640,1) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0391941[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0178712[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0145132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0149714[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0300276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0254453[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0374767[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0140827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.044756[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0265846[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0289511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0159482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0298107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0157202[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0306347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0305038[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0337707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0404717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0112277[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.015152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0142164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0110968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0434347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0141951[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0188753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0119482[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0171253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0233088[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0367595[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0306967[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.030176[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0307888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0452413[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0261908[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0144988[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0139418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0183916[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.011254[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0432027[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0140733[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0141876[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0449627[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0434333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0171808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0111502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0161031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0112071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0149148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0313881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0155748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0180952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0196173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0149213[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0121798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0456213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0143413[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.015663[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0143031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0152955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0263015[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0315627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0440773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0144213[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.031903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0414068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0158017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0156892[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0117995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0355776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0329842[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0343648[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0131007[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0357643[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.030977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.044544[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0122773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0322696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0147506[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0113259[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0181805[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.040979[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.020128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0135424[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0147877[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0406921[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0468973[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0170693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.0309227[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0170603[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.018989[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0187324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0262105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0524648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0271557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0121813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0194921[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0305978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0180879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.055325[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0124393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0279938[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0427133[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0106687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0158842[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.014835[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0262843[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0171355[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0197158[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0436067[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.016221[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.031073[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0305503[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0313047[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.014906[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0142404[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0119162[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0154366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.04202[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0194738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0141396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.013661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0299804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0120347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0236679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0177746[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0162047[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0157687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.01488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.027241[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0178465[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.010665[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0529691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0145734[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0303067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0428573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0302187[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0176651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0394548[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0300631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0519726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0152553[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.012781[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.011472[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0178807[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0190382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0137587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0148554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0119482[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.477129 seconds. Fastest Tactic: 0x2958c78a91e58cda Time: 0.010665[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2958c78a91e58cda[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,409600:32,640,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0282133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.02101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.036416[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0379982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0177173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0207511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0321358[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0188444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0279422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0217407[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0313125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0312417[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0176275[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0261251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0290827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0318041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0292427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0148012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0247733[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0164922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0199278[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0158623[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0152092[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0214293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0279182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0433387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0296853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0184606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0195769[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0145883[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0233479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0312834[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0120838[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0452573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0162545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0272821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0173525[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0485562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0203319[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0307093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0152257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0287858[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0260775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0277612[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0301431[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0123729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.020522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0357291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0253714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0218367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0268751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0164257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0200916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0196549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.038291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0437507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0161722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0294898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0267922[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0196148[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0160122[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.049475[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.017248[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0126119[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0142498[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0188996[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0146991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.013818[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.04428[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0262663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0316752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0201807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0304252[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0197026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0272476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0192889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.044968[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.021876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0150836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.027159[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0156145[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0170992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0271303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0345333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0190684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0264763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0296738[0m
[38;5;104m[X] model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.254942 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0120838[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(3276800,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0205678[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0202447[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00562769 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0202447[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0351776[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0218693[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0166907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0168091[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0403129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.032447[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0510888[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0154085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0398803[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0242004[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0375633[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0205766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0412788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0196756[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0274527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0418707[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0303593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0362827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0131512[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0163845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0139716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0128718[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0384462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0183315[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0259725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0130429[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.022619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0298747[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0504914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0272533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.040576[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0428373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0404587[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0237248[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0151392[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0137297[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0210433[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0120072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0382945[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0137801[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0146328[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0392379[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0384142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.020027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0129128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.0189849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0134409[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.0170192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0433493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.0168235[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0261161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0259873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0161879[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0138321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0639804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0178498[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0174709[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0176247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0164663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.0339968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.04722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.0603822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0142347[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0413807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0375277[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0166461[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0194886[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0141182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0507474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0290489[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0307384[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0144462[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0510369[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0276062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0389653[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0140324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0287324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0167739[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0118396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.0257337[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0371745[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0263426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0154153[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0153168[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0366304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0644747[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0228821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.045248[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0190133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0260644[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0227335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.0319438[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.073408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0245524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0133177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0263721[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0450587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0206074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0756352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0134814[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0345813[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0382364[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0122263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0190584[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0179879[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0367072[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0216227[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0267298[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0390483[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0180553[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.046532[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.04624[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0481752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.0185831[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0156752[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0137182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.020924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.0598649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.0265214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0161778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0154444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0412409[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0142093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0322453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0208044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0200376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0185357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.016848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0258199[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0256229[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.012101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0748651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0165958[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0436653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0387876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0275282[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0261547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0369632[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.0433147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0738795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.0160696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0146597[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0143773[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0221607[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0267298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0166987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0169147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0135488[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.413429 seconds. Fastest Tactic: 0x575184c61f040550 Time: 0.0118396[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x575184c61f040550[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xde3a6a3727f31f34 Time: 0.948224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0257822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0189007[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbbff0ceb48c87bac Time: 0.914475[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x9c9fd7d74c020c9d Time: 0.490496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0340341[0m
[38;5;104m[X] Fast skip Tactic:0xb97409e537081e4c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb97409e537081e4c Time: 3.57821[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0348757[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe5f40c565f9c8a09 Time: 0.0475347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0139089[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x4e679e1c8dcfbe3c Time: 0.94208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0173269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0282142[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x645c57c8d2bdcafa Time: 0.0528305[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x13cb22041bcdf2f5 Time: 0.858795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0163632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0255878[0m
[38;5;104m[X] Fast skip Tactic:0xa1513318dd2f5314 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa1513318dd2f5314 Time: 2.3511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0200734[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0289671[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0273469[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.015599[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0238667[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2d0a836ca3b48b55 Time: 0.618059[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x307c1c762709b00e Time: 0.834549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0267602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0277787[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9826a9122a4e1bac Time: 0.408576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0266355[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0127747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.022806[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0158575[0m
[38;5;104m[X] Fast skip Tactic:0x9c391ea4731a473c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9c391ea4731a473c Time: 1.86573[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xe6fb49f176c8ac20 Time: 0.240928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0196762[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x136deb7724d5b954 Time: 0.993621[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x718a86dfcb201f10 Time: 0.210277[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0149426[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x415d0d459f475d7a Time: 0.0705963[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0145544[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x7f9cac2d273e24da Time: 0.0400047[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x730183d1b4e07af0 Time: 0.0603129[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0211313[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xab1e201ca705d0dd Time: 0.627925[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x2c80f3b4623a1878 Time: 0.213675[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4dc022b90c990350 Time: 0.0373997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0273452[0m
[38;5;104m[X] Fast skip Tactic:0x8286a69028b3e3f0 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x8286a69028b3e3f0 Time: 4.07936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0429547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0290987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0176112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0187721[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0139209[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x4480741a5fe7a6b0 Time: 0.0308519[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x8c7efb20a3cfa7ec Time: 0.573504[0m
[38;5;104m[X] Fast skip Tactic:0xb7622317db162586 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xb7622317db162586 Time: 1.0753[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0226738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0286844[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0101249[0m
[38;5;104m[X] Fast skip Tactic:0x89a3827f636f5c26 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x89a3827f636f5c26 Time: 1.04243[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xdb35ad3ee7e5f3a9 Time: 0.0637067[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xfe34f7b3aa1f6429 Time: 0.0414495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0446107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0131307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0268931[0m
[38;5;104m[X] Fast skip Tactic:0x3836d6c48cd9dbee which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3836d6c48cd9dbee Time: 1.20627[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x0f57663c97a6a89c Time: 0.478549[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop128_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5801f8b0fa1b5d5c Time: 0.80736[0m
[38;5;104m[X] Fast skip Tactic:0x48fd1cd53c4157a8 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x48fd1cd53c4157a8 Time: 4.66902[0m
[38;5;104m[X] Fast skip Tactic:0x31768067dfa77e0e which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x31768067dfa77e0e Time: 1.10944[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x0a6a5850a77efc64 Time: 0.953344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0151864[0m
[38;5;104m[X] Fast skip Tactic:0xe47e7c8e9e121924 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xe47e7c8e9e121924 Time: 3.69766[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x09cde4f526284108 Time: 0.111027[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x244ad5cff0ca2eb5 Time: 0.504395[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x853ead83f0b1020c Time: 0.766741[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0482286[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0192634[0m
[38;5;104m[X] Fast skip Tactic:0x3620fc3660c7e024 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x3620fc3660c7e024 Time: 1.91773[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xaca2d8f22e95cba6 Time: 0.588117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0283431[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x23f62d21795a35ce Time: 0.836949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0133606[0m
[38;5;104m[X] Fast skip Tactic:0x6b2a895dc9dde74c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x6b2a895dc9dde74c Time: 2.06822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269079[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x5600d95cf91aed70 Time: 0.0371473[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x032a0ef3f4005984 Time: 0.778133[0m
[38;5;104m[X] Fast skip Tactic:0xdf8cd3fb81a9e498 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xdf8cd3fb81a9e498 Time: 3.94954[0m
[38;5;104m[X] Fast skip Tactic:0x2e05c6cb8ae0ad7c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop32_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x2e05c6cb8ae0ad7c Time: 4.20448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0255444[0m
[38;5;104m[X] Fast skip Tactic:0xd916513230270d0c which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xd916513230270d0c Time: 1.15302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0261858[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296124[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0112679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0197992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0352427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0243756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.020884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.026656[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0135697[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xecb45af50ce22fe9 Time: 0.0352917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0196499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.01936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0375419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0434053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0156975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0275815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0261465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0192723[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0150228[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.049123[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0166456[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0105087[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0172368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0128706[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0121394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0438853[0m
[38;5;104m[X] Fast skip Tactic:0x5642a4e167e8f364 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x5642a4e167e8f364 Time: 2.1033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0252968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0308509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0186809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0282098[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0176264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0264903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0172768[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x9efe56660532ec2c Time: 0.471435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0445347[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.02147[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x9ebc2bdb9bc0f238 Time: 0.123961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138951[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop1_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xa25e76bff47b753d Time: 0.776875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0264919[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x11aaa3b552fd1244 Time: 0.688128[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0xb99e8d5a01f89b1d Time: 0.559723[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xbe2275b488688066 Time: 0.871765[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x34abf9381f0785c4 Time: 0.551936[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0144893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.016354[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop64_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0xc52cdc7983541cc4 Time: 0.421845[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop8_CVecSplit1_GroupSize32_InnerLoop2 Tactic: 0x06f777ac34a0a24e Time: 0.644661[0m
[38;5;104m[X] Fast skip Tactic:0xc1336bcfda004054 which exceed time limit during pre-run[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0xc1336bcfda004054 Time: 1.7367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.026167[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize32_ThrdOuterLoop4_CVecSplit1_GroupSize32_InnerLoop4 Tactic: 0x36ca788956376575 Time: 0.450912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0172027[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize128_ThrdOuterLoop2_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x76dcfa8e7440813a Time: 0.0385742[0m
[38;5;104m[X] Tactic Name: sm72_xmma_convolution_groupSdcrr_INT8NCxHW32_R3_S3_U1_V1_CtaSize64_ThrdOuterLoop16_CVecSplit1_GroupSize32_InnerLoop1 Tactic: 0x54c7919e8f324660 Time: 0.111136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0260308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0276734[0m
[38;5;104m[X] model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.671113 seconds. Fastest Tactic: 0x13463e9bf9ae0d73 Time: 0.0101249[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x13463e9bf9ae0d73[0m
[38;5;104m[X] =============== Computing costs for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,102400,320,1) -> Int8(6553600,102400,320,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,102400:4,320,1) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0324742[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0319641[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0054505 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0319641[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,640,2) -> Int8(409600,1:16,1280,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0361696[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x995d5ad5e9303e56 Time: 0.0385078[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xe5bdecc8bb9a34a6 Time: 0.0293804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xe4178cc990684323 Time: 0.0357621[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0xc4b553abfc1248b6 Time: 0.0680683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xff206037866ee222 Time: 0.0364181[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x4a450cfbbca82879 Time: 0.0945547[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x6f5617e02d77ed49 Time: 0.0279129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.045312[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.026002[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x72429fdd8edbc060 Time: 0.0726827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xd88720af5f42ddd9 Time: 0.0355563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x7f77f410bba621b0 Time: 0.0720085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xd3c24bf177aa20b2 Time: 0.0346891[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0310196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x38dd8ef91041de84 Time: 0.0746325[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0327409[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0372302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x1334f0476854c158 Time: 0.0233209[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x28fc6d098690127d Time: 0.0342731[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xdbca7f7d947cc004 Time: 0.0276217[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x929a6031d651e5dc Time: 0.0205271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0450493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x29560d08ad6265f8 Time: 0.0317576[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x87c58af32351e7ce Time: 0.0478888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x99df0b6ffeb918b7 Time: 0.0237326[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x1609af06beffc03e Time: 0.0387141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x7b31febaf949e121 Time: 0.0366005[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x0def7612174fd74d Time: 0.0930827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0305765[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xfb666a9dc48122b3 Time: 0.0683861[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x4be1a678abddb6de Time: 0.0734699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.04544[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0260119[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x1544f0d475c1dde3 Time: 0.0312805[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0xe1a648b06dcd8d86 Time: 0.0293324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x94683118dc4ddf86 Time: 0.0404006[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0xa60c3259c62a72b2 Time: 0.0219447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.044624[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x10fbfe2fb4e3fa64 Time: 0.0323733[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0xe419464bacefaa01 Time: 0.0281867[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.045908[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.044816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xb0822e52ce5ffda0 Time: 0.0264213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x168bfebca976e6df Time: 0.0199492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xbbc7450ce6b700cb Time: 0.022164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x97256eca1773c25b Time: 0.0245143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x3a69d57a58b2244f Time: 0.019411[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xf02301c3ec69dfd8 Time: 0.0744192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xf2975a40a49a2513 Time: 0.028672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xe2e5dde09a747979 Time: 0.0474827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0xeaee26edfc12d61a Time: 0.0471533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x63488217a1c11228 Time: 0.0338144[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x52ee8a3dde2622d7 Time: 0.0252762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x5395dfd64a62e320 Time: 0.0701781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0xa602f8dbfaad9b90 Time: 0.0312301[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x6ae9191bec55cace Time: 0.0308238[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x8488d521e86ffa13 Time: 0.0334176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x312cbe24705ad924 Time: 0.0220153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x707495e4d1a11c4a Time: 0.038899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x4e5062015678787b Time: 0.0860853[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x01f1e3e59bf9282c Time: 0.108771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x5fdbe1f0eb5bc307 Time: 0.0314211[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x831f294257f5b782 Time: 0.0707051[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0378584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x5a64ef0b2a79e480 Time: 0.0350432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x57d3d57c088d23b1 Time: 0.0332771[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0x2ce7c97150c7ab5d Time: 0.0241631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xc90f6cdf67d12970 Time: 0.0945253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0320805[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0331918[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x0e632e097e8223fc Time: 0.0268144[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0x82bb83c140802925 Time: 0.0950773[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0311166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.04564[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x8137dbda294ddd94 Time: 0.0258248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0316053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xbe784bf72795274c Time: 0.0190922[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc2 Tactic: 0x575184c61f040550 Time: 0.0233628[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc2 Tactic: 0x6db12833cdbb8711 Time: 0.046436[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.037523[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xa15ac9f3db43d64f Time: 0.0480457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0xabb60d2d5de91374 Time: 0.0273698[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xe002e2337ab81321 Time: 0.0274699[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0372373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x143fa53fe1851c14 Time: 0.0770411[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0xe754199967d1b7dc Time: 0.0377825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x86aeed3baa507927 Time: 0.083608[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x4c765a1b1447fb4f Time: 0.0378216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x2a1b52da665c7825 Time: 0.0484389[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xdef7203c42d7c162 Time: 0.0415526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x3c9b845352ae1e15 Time: 0.04736[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x58d0b488628a1e4b Time: 0.0793323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0260201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x9c6005943f9b3f30 Time: 0.0240312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x65bad33eabdd2872 Time: 0.0483551[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xc10497d201b78613 Time: 0.0825344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0x1079af95a36adc85 Time: 0.0370975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0xdcc12a051dad1d48 Time: 0.0811691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x1dce95e2819e1bb4 Time: 0.0248175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xb88a1ade2d891d16 Time: 0.0476933[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0446053[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0xd805711548cbfb38 Time: 0.0219333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x3493b0dfb178fea3 Time: 0.0235506[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0x0bdc20f2bfa0047b Time: 0.0340608[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x35e8e534253c3f54 Time: 0.0687552[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0x6800ec4a301e49b4 Time: 0.0367765[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0xe6f0b31a70a4297b Time: 0.0493471[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0460387[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc1 Tactic: 0x2d4363f247b235fa Time: 0.0342379[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc1 Tactic: 0x2b703512ef5de6cc Time: 0.0854187[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc2 Tactic: 0x6cda4ffb44ba19f8 Time: 0.0844667[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xe38eba281375e790 Time: 0.0878213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xe1a88232514a64a4 Time: 0.036[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna8_alignc1 Tactic: 0x49c954e0d565dcc8 Time: 0.0345792[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna4_alignc2 Tactic: 0xa3b33ca207085535 Time: 0.0239429[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0x5c96be222065deda Time: 0.0365973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0x465b990c301ed718 Time: 0.107627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc1 Tactic: 0xad445c0457f5292e Time: 0.048573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x215df605cb04ca9b Time: 0.0288933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0x24e2f8fe0a26ed1c Time: 0.0281271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0xbccc10746f66dd87 Time: 0.0703659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc4 Tactic: 0x2ce903f36c40427f Time: 0.0258412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna2_alignc4 Tactic: 0xf4650b69ae861f49 Time: 0.0371271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0xf72854bb65b80294 Time: 0.0347947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0x3fd6db81999003c8 Time: 0.0234197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x7339ca361a9f0197 Time: 0.0270925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0x7686c4cddbbd2610 Time: 0.0264451[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.026967[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc2 Tactic: 0xc06ff01a88b618fa Time: 0.0468653[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna8_alignc4 Tactic: 0x2958c78a91e58cda Time: 0.021066[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc1 Tactic: 0x902e3bb29ea21f17 Time: 0.0885547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xb53d20a90f7dda27 Time: 0.0231026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna1_alignc4 Tactic: 0x74329f4e934edcdb Time: 0.0740011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.045216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0307772[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x4f3b05c9df79e692 Time: 0.0478248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0373084[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna1_alignc4 Tactic: 0xbabc10e772f3c13c Time: 0.075232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna1_alignc2 Tactic: 0xd784415b3545e023 Time: 0.0817024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor16x8x32_aligna8_alignc1 Tactic: 0xde757186555ee783 Time: 0.032895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x32_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna8_alignc2 Tactic: 0xa3bdf6203b8fbc17 Time: 0.0272418[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna4_alignc4 Tactic: 0xddba7fee89e9dcbf Time: 0.0253707[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_aligna2_alignc1 Tactic: 0x2faa96a39bf9b680 Time: 0.0390033[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor8x8x16_aligna2_alignc4 Tactic: 0x2e0e3c208c8c2827 Time: 0.0482103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc4 Tactic: 0xae0903d69ccb34f3 Time: 0.0296293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x16x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc1 Tactic: 0x66f78cec60e335af Time: 0.0318293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x32x64_stage1_warpsize4x1x1_g1_tensor16x8x32_aligna4_alignc2 Tactic: 0x18719b1940bc3c33 Time: 0.0259725[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.434161 seconds. Fastest Tactic: 0xbe784bf72795274c Time: 0.0190922[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbe784bf72795274c[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,102400:32,320,1) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0285867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0195591[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0343371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0351296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0168677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0210327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0344267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0167835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0140107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0256919[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.020187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0127064[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0290533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0364139[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0163017[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0242971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0271262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0344928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.026912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0135782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.027095[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0167616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0203043[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0156485[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0151981[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0214393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.030304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0431013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0131102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0303457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0196631[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0184634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.018915[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.014502[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0232384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0357077[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0155738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0449467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0184326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0270802[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0178027[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0482149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0216447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0351584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0150868[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0269965[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0191336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0257058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0264107[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0296844[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0185084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0222033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0355189[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0279902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0235968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0271179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.019277[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0203369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.019803[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0376711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.043576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0164145[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0369344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0263385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0196643[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.015296[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0493227[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0173605[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0163337[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0192468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0176152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0205321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0194477[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.044332[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0259175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0317207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0218267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0346741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0291627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0264295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0282996[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0447627[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0218413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0224917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0134976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0266281[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0149088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0172112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0265091[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0343061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0249135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0112366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0266716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0365504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0135595[0m
[38;5;104m[X] model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.25566 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.0112366[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/MaxPool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,102400:4,320,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads855 Tactic: 0xfa45342d0e1d409a Time: 0.0115936[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads1017 Tactic: 0xa88280db27a5d09f Time: 0.0101986[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll6_tThreads225 Tactic: 0x3acbbd865df539ed Time: 0.0104397[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads513 Tactic: 0x3d35b618fd3968f1 Time: 0.0103227[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll5_tThreads841 Tactic: 0xbeae815d02985cde Time: 0.0127727[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll4_tThreads513 Tactic: 0xe09c44661dba1b5c Time: 0.0090929[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads1017 Tactic: 0xb331e89337ca2bad Time: 0.0105303[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll3_tThreads841 Tactic: 0xc4335d27b08156bf Time: 0.0125922[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll3_tThreads255 Tactic: 0xbadabf84bb0f736c Time: 0.0084528[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads855 Tactic: 0x80d8e857bc044afb Time: 0.0126131[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll2_tThreads841 Tactic: 0x199aaf5950022512 Time: 0.0129239[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll2_tThreads255 Tactic: 0x67734dfa5b8c00c1 Time: 0.00784645[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads791 Tactic: 0xf307ae442c39b4a3 Time: 0.0139107[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll4_tThreads791 Tactic: 0x2eae5c3accbac70e Time: 0.0128882[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads1017 Tactic: 0x9fe4504b077a26fb Time: 0.011317[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll4_tThreads841 Tactic: 0x63077323e21b2f73 Time: 0.0118588[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll1_tThreads225 Tactic: 0x9dff93820f6f4021 Time: 0.0103247[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads1017 Tactic: 0x923de46f0f4ddb62 Time: 0.0112281[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads513 Tactic: 0xc9170fb073b2e283 Time: 0.00950489[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll6_tThreads855 Tactic: 0xd3ce7ffb6015b945 Time: 0.0119097[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll6_tThreads791 Tactic: 0x072517eca2b23ed1 Time: 0.011923[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll4_tThreads225 Tactic: 0x1340f65033fdc032 Time: 0.0089499[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads513 Tactic: 0x47a86a624f206290 Time: 0.00982588[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll2_tThreads513 Tactic: 0x9a01981cafa3113d Time: 0.00981584[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll2_tThreads225 Tactic: 0x69dd2a2a81e4ca53 Time: 0.00886091[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll6_tThreads841 Tactic: 0x4a8c38f58c13d6ac Time: 0.0119649[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll3_tThreads855 Tactic: 0x5d711a295c873956 Time: 0.0122602[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll4_tThreads255 Tactic: 0x1dee9180e9950aa0 Time: 0.00841147[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll3_tThreads791 Tactic: 0x899a723e9e20bec2 Time: 0.0134895[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP14_tQ14_tR3_tS3_tU2_tV2_tUnroll1_tThreads841 Tactic: 0xedb816f1de89af60 Time: 0.0161285[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads791 Tactic: 0xa01139e8f028471d Time: 0.0172389[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll1_tThreads1017 Tactic: 0x898e8c271f222050 Time: 0.0150632[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll5_tThreads255 Tactic: 0xc04763fe0916790d Time: 0.00929926[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads513 Tactic: 0x6e2321b421289b4f Time: 0.0122408[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll5_tThreads855 Tactic: 0x27ecc653ee9e3337 Time: 0.0125768[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll1_tThreads255 Tactic: 0x9351f452d5078ab3 Time: 0.00927911[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP4_tQ56_tR3_tS3_tU2_tV2_tUnroll5_tThreads1017 Tactic: 0xbee85cb73ffdd634 Time: 0.0112597[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_custom_tP4_tQ32_tRS3_tUV2 Tactic: 0x0165782a59f89027 Time: 0.00681251[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll5_tThreads225 Tactic: 0xcee9042ed37eb39f Time: 0.0100878[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ7_tR3_tS3_tU2_tV2_tUnroll3_tThreads225 Tactic: 0xb474d8546167b9fe Time: 0.00919236[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ28_tR3_tS3_tU2_tV2_tUnroll1_tThreads855 Tactic: 0x74fa51ff328fc089 Time: 0.0154177[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP3_tQ56_tR3_tS3_tU2_tV2_tUnroll2_tThreads791 Tactic: 0x543380407ea3cd6f Time: 0.0141031[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_tiled_INT8NCxHW4_kMAX_tP7_tQ8_tR3_tS3_tU2_tV2_tUnroll6_tThreads255 Tactic: 0x3465da56879df37f Time: 0.00901132[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kMAX Tactic: 0x1f6c40e3e09ec730 Time: 0.00622064[0m
[38;5;104m[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.0999951 seconds. Fastest Tactic: 0x1f6c40e3e09ec730 Time: 0.00622064[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x1f6c40e3e09ec730[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,102400:32,320,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/MaxPool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX Tactic: 0x94215b398b8eb3ba Time: 0.00652[0m
[38;5;104m[X] /model/backbone/MaxPool (CaskPooling[0x8000002f]) profiling completed in 0.00225703 seconds. Fastest Tactic: 0x94215b398b8eb3ba Time: 0.00652[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0x94215b398b8eb3ba[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0209427[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0204467[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00575168 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0204467[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0130683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0147613[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0100772[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102513[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0118816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0132537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0131233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0108115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0149556[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0100207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0129904[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.014176[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0132583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0115903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0121863[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0124626[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0103234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0139489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0118742[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0118845[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0117263[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00893305[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0130605[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0138956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0092062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0137165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0111765[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.011648[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0729747 seconds. Fastest Tactic: 0x5f5aa01645d48746 Time: 0.00893305[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5f5aa01645d48746[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0152412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00952533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0154187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0183573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00958842[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0108404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0159126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00788068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00826433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0160782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00958964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0073753[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0124148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0166075[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00891004[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0127569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0128878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0158288[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0117065[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0096576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0136192[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0107816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.012499[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00858229[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00896533[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0124606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0156247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0201023[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00688784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0137259[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.0106487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00944059[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00831116[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00884267[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0126546[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0164592[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00940652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0232576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0101644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0128915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0110276[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0220887[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0108494[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0162128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00993192[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0117668[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00968076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0160701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0114183[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0126803[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0111438[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0125867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0166227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0139707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0133054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0152383[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0104917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.01254[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0122389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0194074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.022762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00977554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0168501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0163652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0124496[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00918342[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0223652[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0111175[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00968868[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0121128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00801625[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00939496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00892351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0202817[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0126511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0143213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0112107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0160757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0133914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0164043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.013017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.020581[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0126467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0104094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00704289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0116256[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.00901247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0100251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.011538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0162621[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0124101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00674474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0149394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0169253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.00708044[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.26757 seconds. Fastest Tactic: 0x9dafb2758560cc1d Time: 0.00674474[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9dafb2758560cc1d[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.021912[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.020936[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00536319 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.020936[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.010427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0168128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.011949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0152528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0123706[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0116353 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.010427[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0175629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0134242[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0240244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.01613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0167387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0139302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0169131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0137152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0143782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.016445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0157653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.01676[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0138893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0172469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0135185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0134076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0126546[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0170827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0135859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0156398[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0136021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0173285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0242286[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0570113 seconds. Fastest Tactic: 0xb936321f82fd390c Time: 0.0126546[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xb936321f82fd390c[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00832208[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00825288[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0084744[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00855959[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0114169 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00825288[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0113298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.010367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0124547[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00714781 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.010367[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00957165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00788688[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00817509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00907791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00822999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00900912[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.012061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00872342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0100737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00945126[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.009536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00935496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0080701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0083432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00899172[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0106293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0110593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00815792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00855959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00825002[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00878512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0104107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00982651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00907185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00811505[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0082802[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00792335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00855658[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0109158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00975238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00906119[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00883256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0109825[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0839851 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00788688[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x0f47434ace2a7d18, 0.0204467 ms[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(102400,1:16,640,4) -> Int8(102400,1:16,640,4)] got cached result: CaskConvolution, tactic 0x5f5aa01645d48746, 0.00893305 ms[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv [Int8(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: CaskConvolution, tactic 0x9dafb2758560cc1d, 0.00674474 ms[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0226652[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0217427[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00539178 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0217427[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(1638400,25600,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0125037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.021548[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0141831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0180856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0145554[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0115407 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.0125037[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1), Float(51200,25600:32,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0149959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0147279[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0222393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0176152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0177909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0114592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0150655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0149426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.011907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0170667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0170933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0172672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.011242[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0154158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0117797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0133786[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0136947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0180401[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0134814[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0132291[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0119836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0147775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0227598[0m
[38;5;104m[X] model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0541049 seconds. Fastest Tactic: 0x0e07dc8353bf7e9f Time: 0.011242[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0e07dc8353bf7e9f[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(102400,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00326618[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00306304 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00326618[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(12800,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00306251[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00286547 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00306251[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,25600,160,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,25600:4,160,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0160681[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0153847[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00555045 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0153847[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,640,4) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.00741807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00670229[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00865176[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00606235[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.00733959[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00746359[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00734446[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0062799[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00663738[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.00922349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00726655[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00657297[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.007344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0111744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00701133[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00746785[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00623961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0077423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00688784[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0115461[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0113326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.00770982[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00722791[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00661773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00810845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00759564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00655101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0111854[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0733242 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00606235[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,25600:32,160,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00751526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0060961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00636538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.00825496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00659576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00664763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00921196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.00766473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.00939881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00611297[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0121661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00971337[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00722338[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00973989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00777253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00935111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0116572[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00640242[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00765115[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0080414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00663676[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00680729[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00696222[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00755792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00776955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.011264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00642749[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0058749[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00828774[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00683624[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.00915575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00977645[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00775516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0096323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00833067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00758885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00679211[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0121238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00686193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00975086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00670272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0118827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.00928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0115299[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0123876[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00820449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00768436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00941926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00775273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00798375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00712919[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00853173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00948681[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00647323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00845973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.00940918[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00642646[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0100925[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0093517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.009416[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00887691[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0123749[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00832234[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00798832[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0085272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00783529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00687608[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00672896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0113532[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.00734122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00640342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00697178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00944533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00875159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.009512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00826433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0116182[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00761721[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00702778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.011456[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0084072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00673728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0113618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.00922148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00827916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00680729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.010727[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.255678 seconds. Fastest Tactic: 0x705baf38e41eee0b Time: 0.0058749[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x705baf38e41eee0b[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0281227[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0219167[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00529755 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0219167[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00899789[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0136128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00905917[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0115303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00927733[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0122249 seconds. Fastest Tactic: 0x23b890da05937b9e Time: 0.00899789[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x23b890da05937b9e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0212127[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.011708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0203652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.010303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0128464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0137254[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0132997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0111324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0150344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0126262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0103486[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0129009[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0135548[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0134797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00992847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00965516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0103302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0131869[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00972068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0117477[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00992063[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0211013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0206544[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0555457 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00965516[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00718661[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.007002[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0073069[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0071798[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0117585 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.007002[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00693921[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00670421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00816182[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00785196 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00670421[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.00944296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00543174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00616883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0061122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00631889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.00884267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00638913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00680468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00668629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00617837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00626213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00625403[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00557031[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00558311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00699955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00611743[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00618706[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00559591[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00608272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00557209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0086039[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00677717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00650298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00613915[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00549281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00574011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00549159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00597257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00660601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00628267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00600134[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00601276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00712193[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0892246 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00543174[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.02956[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0268217[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00543623 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0268217[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0107788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0087986[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0133075[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00825106[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0106493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0108229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0102888[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0085264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00876445[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0143991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0101615[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00870537[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0101657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0178218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00924425[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0107572[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00841093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.010654[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00906148[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0184421[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0181827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0110077[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0100317[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00875104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0117885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0105587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00874393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0177628[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0710891 seconds. Fastest Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00825106[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfc2fdbdaf1a06f8b[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.01038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00746074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0080927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0117697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00902458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00820579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0125191[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.010613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00678293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0142413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0077698[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00637525[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0190844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0131861[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00973501[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0138057[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0109013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.012495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0180272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00876827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.010449[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0114571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00904822[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00929126[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00929392[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0106457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.00857381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0108642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0178167[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.00963596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00778394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00762376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00716573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0164185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0121931[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00922176[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0132874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0129838[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0102474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0100317[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00784025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0145025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00935733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0110813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00920937[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0192154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00831037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.00969539[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.013017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00907445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0180497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00749748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0141582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0178167[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0194276[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0113227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0107854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.014336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0105527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0111904[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00969478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00960549[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0144373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00889993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0142778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00876472[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0131984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0141942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0142098[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.012559[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0194341[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0116105[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0105463[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0114428[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0109275[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00768073[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00724425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0178296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0106283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00797816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00844667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0128324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.010555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.014376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0104743[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0181176[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0108136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00885867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.00949807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0180267[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0120305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00901708[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0178605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0140991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0100935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.00955429[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00942844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0139938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0095936[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.277663 seconds. Fastest Tactic: 0x214f03e23f252333 Time: 0.00637525[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x214f03e23f252333[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0289236[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0226332[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00527263 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0226332[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(819200,6400,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00992784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0164815[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0096832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0130215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0101747[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0123918 seconds. Fastest Tactic: 0xa8b56a226b057463 Time: 0.0096832[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xa8b56a226b057463[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0218033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0116995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0193855[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0098949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0133206[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0129112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0125286[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0113298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0142138[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0126131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00973105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0127581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0126495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0127396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00880983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00984596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0105833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0136661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00989929[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0104158[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00910011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.021488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0197346[0m
[38;5;104m[X] model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0573413 seconds. Fastest Tactic: 0xad886d4d69834922 Time: 0.00880983[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xad886d4d69834922[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00283381[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00366365 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00283381[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00283417[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00356392 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00283417[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0276693[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0258314[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00545055 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0258314[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0105697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.00713464[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0131848[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.00689632[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.010458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.010729[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00669376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.00696044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00701733[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0141627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00660183[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00699[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00650934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0177555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00770449[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0106157[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00707978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00696444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.00758909[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0181665[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0182355[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0109282[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00644123[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.00706778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0116962[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00691635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00713963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0176719[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0745204 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00644123[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.00870974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00733333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00752426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0115682[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00624356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00581517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0074989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0107874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0142284[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00786257[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0191597[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00821333[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00968259[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0138487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.010826[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00761648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0181564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.00874995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00679957[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.00849733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00879439[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.00925261[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0084024[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.00979748[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00881095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0179571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.00693616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.00711762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0122701[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.00832729[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.013335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00768388[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.00863699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0145294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0068554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0111074[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00637625[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0193523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00601295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00754133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00909607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.018482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0142276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0180929[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0193896[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.00920043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.00693769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0144169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00704133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00719387[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00900884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00689088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00887186[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.00869306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0122244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0141924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.00856123[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00830075[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0143236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.00866243[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.012862[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.019443[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00869771[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00890779[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00939881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0111712[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00831636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.00775442[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.018057[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0106157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00750744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00649497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00770885[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00863371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0144983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00757096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0182995[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0101033[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00926637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0183832[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0124725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00891256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0179492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0142871[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.006956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.00876116[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0103554[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.265542 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00581517[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0501242[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.038061[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00518739 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.038061[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0116189[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0201324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00939585[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00931437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0119924[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0125095 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00931437[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0345824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0105197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0341067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0115781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0112085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0189132[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0184629[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00991027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.021482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0108267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0115005[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0111061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0188065[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0189073[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0115391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0086854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0087546[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0114503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00966644[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0151207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0116929[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0341579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0344053[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.076706 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.0086854[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00814959[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00779634[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.00834667[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0079807[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0112719 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00779634[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00655394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00546501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00554562[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00828349 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00546501[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0104577[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00416013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00617541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00476861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00629857[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.009728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0053054[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00705378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.00947259[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00443298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00453766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00483834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00430906[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00496455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00720794[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00484375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00492235[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00421773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00451805[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00435726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.00956282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.00969295[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00645395[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00615253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00486987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00427569[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00436294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00482773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00494902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00611278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00549666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00604076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.00985851[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.090008 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00416013[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0537509[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0486964[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00533816 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0486964[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0171845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0104533[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.02239[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0102468[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0170421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0173013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00968442[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0103266[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0103783[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.0244602[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00960945[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0104183[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00963413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0309488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.011039[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0172331[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.010447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.010011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0110255[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0317256[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0318128[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0176747[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00960762[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0104061[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0192954[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.00989208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0105457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0308703[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0723777 seconds. Fastest Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00960762[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x43b9fdc4b56fb1b6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0136282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0113586[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0190163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00857874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00735304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0104353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0172645[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.00940148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0240632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0117315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.00945659[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0333525[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0111588[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0148118[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.022208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.017368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0104453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0313222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00946637[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.01328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0137557[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.014252[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0127723[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0157828[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0102833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.013769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.031073[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.016157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0103276[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00751787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0103835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.02956[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0200025[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0127668[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0215827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.010677[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0129321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0163012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.00975086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0244038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00920303[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0180783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00874257[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0335755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00856643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0161112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.010589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0139538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0312582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.00826797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0241501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0310691[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0335392[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0133169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0101812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0242766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.009664[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0104171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.013827[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00923157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0138048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0135219[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.020118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0241943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0135381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0112494[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0242865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0136909[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0202918[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0335552[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0134588[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0130482[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0135987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0175163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0109395[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.010524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0310099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0171195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0104713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0084032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0105983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00887635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.024355[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00924684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0318458[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0157188[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0138475[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.016129[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0317382[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0196273[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0138462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0310914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0241349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00799568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0161067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136905[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.014976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0161681[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.309287 seconds. Fastest Tactic: 0xbb88763c3b0e94d4 Time: 0.00735304[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xbb88763c3b0e94d4[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0507794[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0388326[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00528654 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0388326[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(409600,1600,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0126199[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.022902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0101893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0101017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0130392[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0125248 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0101017[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1), Float(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0350848[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0101505[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0345952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0116406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0112096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0189754[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0185434[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00936681[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.021596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0109034[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0114105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.010964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0188154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0188101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0113013[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00825834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00846[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.011434[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00939437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0147701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.011513[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0346677[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0349824[0m
[38;5;104m[X] model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.058699 seconds. Fastest Tactic: 0x2d8ab2aa0639fda9 Time: 0.00825834[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2d8ab2aa0639fda9[0m
[38;5;104m[X] =============== Computing costs for /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm50_xmma_pooling_CHWPacked_NCxHW4_kAVERAGE Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00266709[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00462715 seconds. Fastest Tactic: 0xb4d3d3158ab4fbc4 Time: 0.00266709[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xb4d3d3158ab4fbc4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f])[0m
[38;5;104m[X] Tactic Name: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE Tactic: 0xd9375d43b61ffbcb Time: 0.00296005[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool (CaskPooling[0x8000002f]) profiling completed in 0.00412981 seconds. Fastest Tactic: 0xd9375d43b61ffbcb Time: 0.00296005[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskPooling Tactic: 0xd9375d43b61ffbcb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.0518842[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.0481082[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00533152 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.0481082[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0171573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.0103994[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0224242[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.010124[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0171968[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0172357[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00800178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0102077[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0103282[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.024339[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.00789408[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0102229[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00790574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0309556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0110066[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0172779[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0103625[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0084808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0108318[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0321057[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.0317634[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0176719[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.00793956[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0102726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0192308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0083408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0104573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0308567[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0744826 seconds. Fastest Tactic: 0x19e870769dcaba51 Time: 0.00789408[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x19e870769dcaba51[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0134763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0105063[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0113028[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0190507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.00882976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.00742661[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00719433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.017528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0241989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.011989[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.0332868[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.00805156[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.0151821[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0229184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0176595[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.00732452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0314531[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0136785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.00928207[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0121413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0137865[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0146263[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0128443[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0159898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0136555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0312407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0104048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0103903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0203144[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0128907[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0221473[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00863918[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0124946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0245722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.00921657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0182647[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00877502[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0336576[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00854747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.00929807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0140027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0317236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0243215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.0312844[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0336512[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0116565[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0102491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0244305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00933926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0103615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0138462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00912692[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0136269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0136269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0205107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0242819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.0136486[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00830361[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0243109[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.013597[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0212133[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0336331[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0122933[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.01277[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0118783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0177752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.0115825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0109395[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0312524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0172869[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0104487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00862961[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00742684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00859323[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.024435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00894512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0319098[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.015809[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0143236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0320291[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0204147[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0139342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0312611[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0242735[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.00754987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0136256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00823259[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.241461 seconds. Fastest Tactic: 0x322f337abc345152 Time: 0.00719433[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x322f337abc345152[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0935227[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0707755[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00532244 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0707755[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0182389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0335285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0142404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0128812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0188593[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0118151 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0128812[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.061248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0149421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0607787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0176196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0321571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.031712[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.013414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0373938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.0103399[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0174699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.010458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0320262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0321484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0174971[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0114016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0114105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0113287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0138918[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0244282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.017778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0607804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0615502[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0574864 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.0103399[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0105547[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00942044[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0107988[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00978438[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0115447 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00942044[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00705955[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00545015[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00544792[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00900919 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00544792[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0114496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00393763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00697422[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00454443[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00718865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0113216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00543088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00723086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0109932[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00408975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00418733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00452958[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00401168[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00531725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00745078[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00491137[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00503292[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383277[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00414973[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00419786[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0111026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0112316[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00690568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00679445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00522067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00391529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00415526[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00451085[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00477653[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.005859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00604286[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00664282[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0112569[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.105098 seconds. Fastest Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00383277[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65fbe45b4cb1d8a5[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.106651[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.092096[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00610738 seconds. Fastest Tactic: 0x0f47434ace2a7d18 Time: 0.092096[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x0f47434ace2a7d18[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xcaab64dc011b5654 Time: 0.0303913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x8966ea5b2b2e562a Time: 0.017[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.0407846[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfc2fdbdaf1a06f8b Time: 0.0167797[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x00509b61c47b6aa4 Time: 0.0303564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0305018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.0123623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xb60ee5b2b8916a65 Time: 0.0168155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.0170235[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfb9fa0e8667ec2a1 Time: 0.044636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x19e870769dcaba51 Time: 0.0124255[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.016976[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0123599[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.0575751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.0176674[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.0306036[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.0169803[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.013079[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x312224288ff5a21a Time: 0.0174581[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.0582204[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x6e1e2f0c0cc0141c Time: 0.059088[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x5f5aa01645d48746 Time: 0.0311719[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0x43b9fdc4b56fb1b6 Time: 0.0124599[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xc347d433621f53c4 Time: 0.0169408[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.0342816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x231b33528b85cbc3 Time: 0.0129994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.0171563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r3s3 Tactic: 0xfbeaf4deb8c57f4f Time: 0.0573387[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0786318 seconds. Fastest Tactic: 0x08af511817b7463e Time: 0.0123599[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x08af511817b7463e[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4f6730716f78f4a3 Time: 0.0233984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.0171003[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0189007[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xeb9516439d87ac23 Time: 0.0338016[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x70ccdad7e8ced9ab Time: 0.0135113[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xbb88763c3b0e94d4 Time: 0.0110397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0109447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3d41ef6de22d9b6 Time: 0.0305619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x99ccdec1bdaffe62 Time: 0.0158536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x60edc1e1753cd5b8 Time: 0.0440467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0197308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x214f03e23f252333 Time: 0.0159314[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x0405e3a763219823 Time: 0.061312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4749124f62d8bd23 Time: 0.0110087[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.025783[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.0397239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.0307762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd3f592fae61c7986 Time: 0.0101928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.0578204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xef01fb6e433afa50 Time: 0.0237696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x671c943720ba8655 Time: 0.0142591[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3cda2ee55a7d0cc2 Time: 0.0207266[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.0236423[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0xa792e2a2dcc5e78f Time: 0.0245996[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.0216267[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x09727a53770225e8 Time: 0.0277292[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x50fc4fe76305f898 Time: 0.0165491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.0235655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x554e2e252e28b3fd Time: 0.0577476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x2ce02f6643ed65f2 Time: 0.0293253[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd30e9f770878c3fd Time: 0.0168437[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x1d53511430a5d47e Time: 0.00986008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x705baf38e41eee0b Time: 0.0170144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x256x32_tapsperload3_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x5faf0e15e3864b9d Time: 0.0559289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.0358485[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x61d05b8ef3670baa Time: 0.0216833[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x3a7df5a005634aca Time: 0.0389002[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0136085[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x13463e9bf9ae0d73 Time: 0.0206482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xea83ffb21dc4d00d Time: 0.0294196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xe87f92c4e18f25c9 Time: 0.0159924[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0444627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9262f8f95beb428d Time: 0.0144951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.0323055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0136171[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x446f06d5a2e0bae3 Time: 0.0618382[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.0132862[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize128x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x520022919f4e0d5c Time: 0.0293556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x19b79d0c4ff629da Time: 0.0151898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.0238606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0578222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0xa5d08c37b22f092f Time: 0.0113689[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd9f3bbc3e16b16ac Time: 0.0441427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf8d4389f60adfa3c Time: 0.057424[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0616231[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x4e4c4bf050b40a1b Time: 0.0189849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x028e842ce51fbd9d Time: 0.0167632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.0443547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.0144484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0169141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.0238392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0143831[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.0238568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xf33711e7c9ed4673 Time: 0.0235627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0361835[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x4cf18ad3e295ea18 Time: 0.0442693[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd277f13d771603ee Time: 0.023456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.0111932[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.0440813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9d0f90e0cec890bb Time: 0.0235783[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0370738[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.0618631[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.0207341[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.0211847[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.0192616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.0310051[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.019072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x53554c607d072468 Time: 0.0172619[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.0577724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea88b51105501f96 Time: 0.0305881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.0170464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0137143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.0103535[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.0134097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.0442747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.0144351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.0590116[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0273403[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.0240419[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta256_r3s3_u1v1_scalebias_relu Tactic: 0x9463f245c167b8a3 Time: 0.0293413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.0591236[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r3s3 Tactic: 0x8a60cb2150513f2e Time: 0.0349792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0238037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x5c382479e824f65e Time: 0.0576409[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x8b2f6e180c235792 Time: 0.0442027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2f34f689bfca5071 Time: 0.0113927[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x9dafb2758560cc1d Time: 0.0292062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x9f6920f4a40549f4 Time: 0.0236423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0121863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload9_threadspercta128_r3s3_u1v1_scalebias_relu Tactic: 0x252c6f5607ea114c Time: 0.0292169[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.313247 seconds. Fastest Tactic: 0x1d53511430a5d47e Time: 0.00986008[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x1d53511430a5d47e[0m
[38;5;104m[X] =============== Computing costs for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0941787[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.071168[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.00531662 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.071168[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(204800,400,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.0191289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0362293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.0150112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.0135863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.0199586[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0119005 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.0135863[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1), Float(6400,400:32,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0616836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.015216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0611378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0178038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.0111388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0321522[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0318264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0137894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0373807[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.010348[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.0177095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0104267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0320446[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0321076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.0177785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.0115836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.0116995[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0114119[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0139578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0246499[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0180598[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0611076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0616231[0m
[38;5;104m[X] model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu (CaskConvolution[0x80000009]) profiling completed in 0.0570242 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.010348[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0147163[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0119977[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0150739[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0127305[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107406 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0119977[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00640584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00464296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00450034[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00931516 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00450034[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0138684[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00368797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00834507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00462773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00857791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0135868[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00573686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00865477[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0131951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00428649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00439775[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00455957[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00378727[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0058502[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00902804[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00549823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0055568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00382194[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00410953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00396562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0134997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0133104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00837893[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00822686[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00569184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00385385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0039695[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00394717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00465378[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0067584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00604248[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00823415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0134396[0m
[38;5;104m[X] model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0933668 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00368797[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]}[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023])[0m
[38;5;13m[V] Compiler backend is used during engine build.[0m
[38;5;104m[X]  (foreignNode) Set user's cuda kernel library[0m
[38;5;104m[X] Subgraph compilation completed in 3.04 seconds.[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0472747[0m
[38;5;104m[X] {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} (Myelin[0x80000023]) profiling completed in 3.07209 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0472747[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00855521[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00832598[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00898751[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00858038[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00892997[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0157889 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00832598[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00859733[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00831896[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00908685[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00858639[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00896898[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0156532 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00831896[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00595753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.0079045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.0055808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00669312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.0060769[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00680315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00677376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00670443[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00745553[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00588351[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00676074[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00577545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00687521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00646851[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00609591[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00783702[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00579035[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00557405[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00550592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00755721[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00658133[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00648349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00580322[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00600248[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00665161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00672618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00594377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00663425[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0798447 seconds. Fastest Tactic: 0x712e1cc2c7813ee9 Time: 0.00550592[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x712e1cc2c7813ee9[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00566075[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00595943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00785985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00555576[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00629032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00669802[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00602952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00682732[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00675541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00666816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00744296[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.0058721[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00677888[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00576975[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00570143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00680149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00632131[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00594377[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00547008[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00775491[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00571643[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00545462[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00543484[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00742353[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.00566979[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00627714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00637686[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.006432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.0056499[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00638933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00661145[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00730179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00568244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00549718[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00585638[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00635734[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.006528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00657966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00585507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00650687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.00756385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.0063684[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.1193 seconds. Fastest Tactic: 0x712e1cc2c7813ee9 Time: 0.00543484[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x712e1cc2c7813ee9[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00562489[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00672704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.0065646[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00612112[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.007076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00670314[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00719591[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00548582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00537126[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00689719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00803175[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00750104[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00719478[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00585993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00649764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.0065623[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00602[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00574717[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.00740035[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00594527[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00667755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00598914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00632614[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00741547[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00607574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00563982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00534823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00791615[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00693268[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00601257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00618529[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.0059741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00761382[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00604305[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00600781[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00669504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00583173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00621254[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00574989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00674112[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00539269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00562933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00660183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00525367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00578465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.0058115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00611375[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.0073178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.0061665[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00610444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.0059346[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00609649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00509107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00733449[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.0057271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00571282[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00513149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00609455[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00648616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00643652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00629116[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00567702[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00540869[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.0068652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00610832[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00578501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.005776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00664157[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00615195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00586798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.0067136[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00609881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00609241[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00633077[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.005072[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00567557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00786977[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00556605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00553041[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00570866[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00759491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00562773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00540714[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.0058706[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00545721[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00684103[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00600629[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00575798[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00543862[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00591607[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00631024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00543071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.0052485[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00705311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00619516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00586012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00627042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00602057[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0063517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00742352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00665182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00787126[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00534671[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00712102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00694533[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00567666[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00755081[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00633016[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.0054068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00557351[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00524483[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00689524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00616728[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.0054228[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00533181[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00624672[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00596095[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00556285[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00538822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00599829[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.0052545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00582271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.0063998[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00688022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00529829[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00825002[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.366837 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.005072[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00526017[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00623941[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00610134[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00572041[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00656335[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.0062244[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00673963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00513953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.0050447[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00645026[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00740764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00694596[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00675435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00546903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.0060501[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00613353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00556302[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00539097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.0068948[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00553215[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00628168[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00553985[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00587266[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00687086[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00565785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00528467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00503705[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00736696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00651139[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00561191[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00574989[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.0055068[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00707022[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00556889[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00553443[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00621156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00545222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00591214[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00572981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00676843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c128_scalebias Tactic: 0x7ced03e1ef3cd509 Time: 0.00538976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00559396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00655728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00527133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00580193[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.0058545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00612538[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00737878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00624099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.0060641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00592075[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00605334[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00515627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00733217[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00572222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.00573035[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00512032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00606817[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00646933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00612752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0063203[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00571245[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00542262[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00694911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00609009[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00580432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00578244[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.0066606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00619062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00590297[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00673728[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00610347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00609765[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.0063515[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00510642[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00567575[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00784992[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00555787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00555174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.0056857[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00757167[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00568154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00544379[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00577104[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00548407[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00685758[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00601276[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00575853[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00547515[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00589885[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00628504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00544241[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.0052385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00708555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00617738[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00589118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00629635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00596991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00640725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00733519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00664847[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00788986[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00534366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00716414[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00695622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00576166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00760097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00637947[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00537058[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00564836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00529371[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.006952[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00618924[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00545686[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00544981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00630179[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00595962[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00564[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00545204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00605334[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00527283[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.0058332[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00635613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00682101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00534654[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.0082227[0m
[38;5;104m[X] model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.387854 seconds. Fastest Tactic: 0x5e4918ccf433630e Time: 0.00503705[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4918ccf433630e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.0073433[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00721271[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00908281[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00849893[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00877923[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146644 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00721271[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_xregs_interior_c32_nn_v1 Tactic: 0xc27fa49e07d992c2 Time: 0.00732591[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x64_relu_interior_c32_nn_v1 Tactic: 0x7a2c2a831965ff85 Time: 0.00723586[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_medium_c32_nn_v1 Tactic: 0xeb494e2e67f079d4 Time: 0.00911885[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_interior_c32_nn_v1 Tactic: 0x9fc2bcaa51428a78 Time: 0.00849867[0m
[38;5;104m[X] Tactic Name: ampere_int8x4_icudnn_int8x4_128x32_relu_small_c32_nn_v1 Tactic: 0x0f47434ace2a7d18 Time: 0.00880112[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0146556 seconds. Fastest Tactic: 0x7a2c2a831965ff85 Time: 0.00723586[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x7a2c2a831965ff85[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00583412[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00494196[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.00531336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00437153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00568895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00412997[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00426761[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00750673[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00440603[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00423021[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.0040904[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00541953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00437624[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00730319[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00617363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00489183[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00436973[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00533621[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00431042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.0043297[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00744035[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00735675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00547095[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00437112[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.00407532[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00414946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00569148[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00438372[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0799133 seconds. Fastest Tactic: 0xc92a09063ffbca7b Time: 0.00407532[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc92a09063ffbca7b[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0xefa70d52218f5041 Time: 0.00533943[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xf55b89e036296eee Time: 0.00583228[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xbbb1d0ea0561d9d7 Time: 0.00489507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x381eb065135d177f Time: 0.005289[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x3bc66347b699d42d Time: 0.00725496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x3c9d1170320511a8 Time: 0.00437209[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xd5c954d4cf40fc8b Time: 0.00568714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x22ebff09f6ab32eb Time: 0.00411356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5fc630a1f9282571 Time: 0.00424938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x7179001c32510482 Time: 0.00748468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa988c7900111b00e Time: 0.00440196[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x8704ff8228354135 Time: 0.00422899[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x08af511817b7463e Time: 0.00411499[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x8ac36fa34eef1169 Time: 0.00542194[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x390abe22d1f5c0a5 Time: 0.00434978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x03f51e99a1ba2de7 Time: 0.00435338[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x587a26cec3345539 Time: 0.00730365[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x710c542aeceab96c Time: 0.00614594[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x764c3b623721cf29 Time: 0.00418706[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x23d3685ccc1a0944 Time: 0.00485055[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x76bc2f187b341446 Time: 0.00433884[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x5693be686f87be25 Time: 0.00536042[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0x712e1cc2c7813ee9 Time: 0.00432492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xa6cc01c449c5ee73 Time: 0.00435615[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage2_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x717edd7ae088c4df Time: 0.005891[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x999feddf5d2ebcf4 Time: 0.00730342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x788dd0382d5ebd44 Time: 0.00394943[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xb9c3d3aeb7254d76 Time: 0.00741878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x1015276bc74e51b5 Time: 0.00551239[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xe48ddbce86546fd1 Time: 0.00742329[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x5e4d4364875d8f2b Time: 0.00432983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5db06b1b995a8a61 Time: 0.00441909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x4038901e8b8f2d99 Time: 0.00549159[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x4c75821f16638e21 Time: 0.00527067[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x49d420f1e88b2809 Time: 0.00436876[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x9dc5f54395173bcf Time: 0.00397333[0m
[38;5;104m[X] Tactic Name: sm86_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage5_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0xc92a09063ffbca7b Time: 0.004064[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_t1r1s1 Tactic: 0x937b84b4175ec19c Time: 0.00413643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize256x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa6235a0b3508ed71 Time: 0.00565966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle Tactic: 0xa9ecd3e1169c5aad Time: 0.00434258[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf2621d7e2ce6fdfc Time: 0.004879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_i8i8_i8i32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_wo_epi_swizzle_simple_t1r1s1 Tactic: 0x110bc624618980a7 Time: 0.00427583[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.124862 seconds. Fastest Tactic: 0x788dd0382d5ebd44 Time: 0.00394943[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x788dd0382d5ebd44[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00571896[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00449081[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00443537[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00477364[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.0080513[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00421026[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00825756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00543002[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00541351[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00799492[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.00444281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.00449052[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00813737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00465926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00727838[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00453751[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00640402[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00429607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.0047052[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00505325[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00673216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00466963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00491639[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.00431918[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.0047043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00562578[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.00529151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00559627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00779039[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00469919[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00478187[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00642421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00533587[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00453319[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00391045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.00418747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00451503[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00740846[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.0043405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00481691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00462978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00477714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00592094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00479314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00647898[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00752711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00458886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00766594[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00429771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00671296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00440393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00546711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00436959[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00449081[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0040921[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00480945[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00398895[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00446094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00506101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.00802616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00632393[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00505762[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00834587[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.00452194[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00455452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00646605[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00451877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00429101[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00404536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00448512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00409691[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00495765[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00464637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00384687[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00482468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.00592056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00626726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00624731[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00645251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00465926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00643426[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00623151[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00629555[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.0060261[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00825262[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00463079[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00457395[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00449707[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00479802[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00482347[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00584589[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00562009[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00490792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00763588[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00428184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00449436[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00425573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.0043122[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00459982[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00439003[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00455467[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00496643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00462816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00456811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00658238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00435782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00773261[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00508557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00477714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00450782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00511143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00755129[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00468474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00676907[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.0058446[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00741926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.00490218[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00618529[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.00439368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00753564[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00492047[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00399238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00472293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00462261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00395696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00543862[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.36337 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00384687[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x31276c9cc1913670 Time: 0.00575062[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5e1b696f91f572c0 Time: 0.00449877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xdb2ceb83bdb264c9 Time: 0.00445084[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x15fad4362e913239 Time: 0.00472714[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9d607f92bc49571b Time: 0.00805994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x881d70ee6f8bc650 Time: 0.00416474[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x8e961765e9b5e3ee Time: 0.00825262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x601b41d38fc4645b Time: 0.00538632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4f35593c356e2e7e Time: 0.00543157[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x9f47704ddd29929e Time: 0.00800711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x322f337abc345152 Time: 0.0044313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x596666386c88024b Time: 0.0044854[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xcefcf2172874c12e Time: 0.00825496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x79a4e52543793dbe Time: 0.00466667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4632bfc06f0b5fff Time: 0.00728556[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xbd2f48ecbf742d59 Time: 0.00452843[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x53604f016bff6d61 Time: 0.00643385[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x850b80ab7d925385 Time: 0.00435144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd9c6b8a2f7935fa5 Time: 0.0047076[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdfb027065697c23b Time: 0.00508008[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x1d9b1bf0b28cc357 Time: 0.00674837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xde3cb6dda9a9f049 Time: 0.00467155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x65920facc9ae819d Time: 0.00490651[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xcae7b5888d47fe1f Time: 0.004304[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xc4dfbf32da2071d6 Time: 0.00472413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8d50646eff0cde6d Time: 0.00563236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5e4918ccf433630e Time: 0.005271[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x6106e2811713d7ee Time: 0.00561102[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xafcb7da5486bc979 Time: 0.00770327[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x429236a031bfe3e7 Time: 0.00466296[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x87620679fb5f37df Time: 0.00479787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa9815e06b127c3d5 Time: 0.00628879[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xe56748b5b7870ba4 Time: 0.00533198[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x81994a658cdf908d Time: 0.00451344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xc22b2f91e37e472a Time: 0.00391913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xed8f60f5aa2efd98 Time: 0.0041932[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x4b8c9beb00181107 Time: 0.00453867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x70ff342513dcddd5 Time: 0.00738481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xf207dff9d0b58a85 Time: 0.00431384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32 Tactic: 0x2728bbf79375f71d Time: 0.00483032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x179c6422445ceb76 Time: 0.00462568[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc5f4c4b3e5ec8f6e Time: 0.00477547[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xc37005486323d39b Time: 0.00597067[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x58e405fffd827823 Time: 0.00482865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xc6627e11680191d5 Time: 0.00645764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x7247cc5dea3981f1 Time: 0.00751407[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x136c072084f41e49 Time: 0.00456679[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdfdddae7a4bcc830 Time: 0.00762836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x599d6bb582ecb830 Time: 0.00429238[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x4b860619b6031662 Time: 0.00668928[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfac85bfa6e8a95c6 Time: 0.00442231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x7dcb6d55062fd530 Time: 0.00547462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa5a7024b355e2bbc Time: 0.00438344[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x235fad4a7171cb36 Time: 0.00449166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x99901a83d7176ba9 Time: 0.0040839[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x368150d268fe20d3 Time: 0.00482164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x495da4d52da4de96 Time: 0.00398844[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xffb9fbd2bfa6c47d Time: 0.00446307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x26a29d5b8b3f62af Time: 0.00508234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x80cce6d8f75dc163 Time: 0.0079967[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x01bc9ada86b72c5f Time: 0.00631849[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x2d01166056519c42 Time: 0.00507216[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage1_warpsize4x2x1_g1_tensor8x8x16 Tactic: 0x85047b8e34ed27fa Time: 0.00836293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x18f10c3bd17f3940 Time: 0.0045133[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x99d5ca59733a76be Time: 0.00453362[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x0f6ba1e5b0320393 Time: 0.00645949[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2a3be0cb61f5a9c8 Time: 0.00449024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32 Tactic: 0xe37d64a29ca3101c Time: 0.00434479[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x5314c155321a63a7 Time: 0.00404614[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x11b75d98d540ee52 Time: 0.00449294[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb3718d2455749f91 Time: 0.00412049[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x444f830f80ff098b Time: 0.00499741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x32x64_stage4_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x4b476758492c15b0 Time: 0.00457249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x483ad1560c6e5e27 Time: 0.00385104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x96x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x0fbfd5fa9864ab49 Time: 0.00480427[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x192x64_stage3_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x89c1d75fe77808d5 Time: 0.0059142[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x256x64_stage3_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x596d7302ab180539 Time: 0.00627753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x192x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xd25c9876338da5ac Time: 0.00625086[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xea7e3523ffa8ae75 Time: 0.00644287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xee2fce9480a52be7 Time: 0.00464696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0x623894f465c90f26 Time: 0.00643036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb2cc5e08f6b66610 Time: 0.00620267[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16 Tactic: 0xb81aeaba4cbc0d97 Time: 0.00630682[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x7883d01837952a88 Time: 0.00606856[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage1_warpsize2x4x1_g1_tensor8x8x16 Tactic: 0xfb1f0c938b867bc9 Time: 0.00824872[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0x23cd610b930e6789 Time: 0.00463585[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb649da27d0e9770f Time: 0.00453924[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x2df835e0f3719216 Time: 0.00447971[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0xfaea3ed8eff52856 Time: 0.00481569[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16 Tactic: 0x58be15b6f024df52 Time: 0.00485936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x204424cb4da28c02 Time: 0.00585114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5947ea3454b6a27b Time: 0.00564302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xeb33cf0799237780 Time: 0.00489878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x41b4640eb250ffd0 Time: 0.00762497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x2640501019a61dc2 Time: 0.00426869[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x3818ca0093333b50 Time: 0.00451171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3ffcb62b1c6bb94f Time: 0.00422967[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xf56c0ac895d82363 Time: 0.00429169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8e1e9d670448aca7 Time: 0.00460113[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x31de506085a332d4 Time: 0.00437042[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x955d593b1135a423 Time: 0.00454558[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x83e18e91fd965e25 Time: 0.00499343[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcddae68de84cc6ee Time: 0.00464444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x3912ca79eb9a8be1 Time: 0.00457585[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x96x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xa92dc4358ef627d4 Time: 0.00660225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x296cf726a94959d6 Time: 0.00437167[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0xb12f6d354b73f48d Time: 0.00772945[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16 Tactic: 0xdd517393a24bd0f4 Time: 0.00510174[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize192x64x64_stage3_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xa086b8faeb42b254 Time: 0.00480213[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x128x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x764ba04bb839d539 Time: 0.00450883[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x0807ee84b7a22819 Time: 0.00510966[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage3_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xc2b1c1cb14a83367 Time: 0.00755864[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0x8bac1801ff920aa5 Time: 0.00466504[0m
[38;5;104m[X] Tactic Name: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw0_c256_scalebias Tactic: 0xc39d8a5d95d69acd Time: 0.00675925[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xb0ee01628ff73107 Time: 0.00582308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x96x64_stage3_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xd6abd0bb62b08d93 Time: 0.00744675[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x96x64_stage3_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x2ff8bc85c8ced89e Time: 0.0049258[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage1_warpsize4x1x1_g1_tensor8x8x16_t1r1s1 Tactic: 0x00f33fe3dac544e3 Time: 0.00618904[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage1_warpsize2x2x1_g1_tensor8x8x16_t1r1s1 Tactic: 0xeb43af4c79f37067 Time: 0.0044021[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x716fcb85e712b30e Time: 0.00752948[0m
[38;5;104m[X] Tactic Name: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage1_warpsize2x2x1_g1_tensor8x8x16_simple_t1r1s1 Tactic: 0xa7ed996d55bb4583 Time: 0.00490321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x311b82feb19aef19 Time: 0.00396851[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize48x128x64_stage3_warpsize1x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xb14c1f9154f6db4e Time: 0.00472593[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x96x64_stage5_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x932469cec5625217 Time: 0.00462758[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xfa5f2e15625aa266 Time: 0.00397143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x521e608938d9d612 Time: 0.00543759[0m
[38;5;104m[X] model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.359903 seconds. Fastest Tactic: 0x483ad1560c6e5e27 Time: 0.00385104[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x483ad1560c6e5e27[0m
[38;5;104m[X] =============== Computing costs for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00978225[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00874967[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100235[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00909636[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0114562 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00874967[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00601486[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00448811[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00437846[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00878164 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00437846[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0109984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0033344[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00699467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00396511[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00714939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0106113[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00489306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00712874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0103648[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00385668[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00393876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00396976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00336491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00491482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.007376[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00462553[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00475808[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00348278[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00369993[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00365264[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0105783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00670976[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0068665[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00481783[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0035722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00353224[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.003557[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00411865[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00559342[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00503355[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00674368[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0105553[0m
[38;5;104m[X] model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0930245 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.0033344[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00247103[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00236543[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00239596[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00251613[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00240015[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00224379[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00309906[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00268373[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00248359[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00239688[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00246831[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.466384 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00224379[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240783[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00238103[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00239169[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00251348[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00240315[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00228296[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00310568[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00268467[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00249067[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00240399[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00241335[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.0296088 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00228296[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00243908[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00237602[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00240698[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00252415[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.0024132[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00227178[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00311111[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.002681[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.0024749[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00238857[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00243714[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.029685 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00227178[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00230341[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00251902[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00241971[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00280784[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00267699[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00262142[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00353112[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00335488[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00334752[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00309472[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00253026[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00252808[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00233234[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.0027848[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00256228[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.0023792[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00350345[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.00293698[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.00269368[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.00279618[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00250587[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00241366[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00266965[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00317755[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00227955[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00276284[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00271592[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 1.32012 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00227955[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00253091[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00275392[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00306711[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00391888[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00255853[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.213181 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00253091[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00388043[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00392496[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00415539[0m
[38;5;104m[X] PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) (PointWiseV2[0x80000028]) profiling completed in 0.136645 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00388043[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c[0m
[38;5;104m[X] =============== Computing costs for /model/encoder/Resize[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,400,20,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00995106[0m
[38;5;104m[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.123618 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00995106[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00276637[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00275051[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00302059[0m
[38;5;104m[X] /model/encoder/Resize (Resize[0x8000001f]) profiling completed in 0.123293 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00275051[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0161879[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0132205[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0167387[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0138918[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0111094 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0132205[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00740707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00563822[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00560729[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0083057 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00560729[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0153396[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00512663[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00923301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00510545[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00940029[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0150312[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00640242[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00955825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.012452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00495325[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00507911[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0051543[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00526483[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00631245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0100367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00616165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00624909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00480198[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00514297[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.0047345[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0146713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0126349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00922032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00924195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0062398[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00487374[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00469022[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00541626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00538168[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00753564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00656565[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00908512[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0127992[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0879869 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00469022[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0115366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00487559[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0113579[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00820215[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00492596[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00571914[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00809321[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00522133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00466281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0115182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00484669[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00659367[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00462363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00564551[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00584092[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00493647[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00573397[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00492816[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00455856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0114798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0117326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00777352[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00605886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00800229[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00778667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00767151[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00587659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00497608[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00473089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00563111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.012256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00455423[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00612092[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0910661 seconds. Fastest Tactic: 0x709ddd0e503c7fd7 Time: 0.00455423[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x709ddd0e503c7fd7[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0121025[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00625541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00764363[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00620899[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00581517[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0198024[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00856233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00781519[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0117863[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00598934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0116984[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0198093[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00787225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0119737[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00774546[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00649518[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00591832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00581315[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.020155[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0058183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0198036[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00799518[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0568151 seconds. Fastest Tactic: 0x6fd15a9d85252b17 Time: 0.00581315[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6fd15a9d85252b17[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6fd15a9d85252b17, 0.00581315 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0303893[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0239421[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00491057 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0239421[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00901391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0148044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00719183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00690764[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00931022[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.01262 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00690764[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0232284[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00665496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0189227[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00767176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00564302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0122133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0117458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00620741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0135138[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00548302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00756622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00557156[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0120267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0121322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00753256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00566744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00552394[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00570902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00632755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00948089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00771491[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.0210393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0191609[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0596245 seconds. Fastest Tactic: 0x45f7566cdb2b10fb Time: 0.00548302[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x45f7566cdb2b10fb[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00297886[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00282594[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00280027[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00290746[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00283788[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00283751[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00350333[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00305484[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00294559[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00281134[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00294307[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.515491 seconds. Fastest Tactic: 0x0000000000000002 Time: 0.00280027[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000002[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00370262[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00421053[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00554982[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00560853[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00628543[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.00968228[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00877785[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0103557[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0164983[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0196549[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00349534[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00339136[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00462539[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00659284[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00329579[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.0035163[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00369747[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00456533[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.0061153[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00385005[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00337056[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) (PointWiseV2[0x80000028]) profiling completed in 1.68252 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00329579[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00675797[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00415236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00674432[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00561333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0044421[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00432711[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00542435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00447445[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00436544[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00689306[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00435131[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00518777[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00393462[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00425424[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00444643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00437583[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00447801[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00439354[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00388701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00687369[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00685975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00521167[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.0047055[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0053716[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00525817[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00506667[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00443986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00416946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00416197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00414696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00718071[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00425789[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00462115[0m
[38;5;104m[X] model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0982324 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00388701[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00834773[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00500776[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00856725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00658154[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00492659[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.0048388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00629313[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00515988[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00478385[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00868704[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.0048606[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.005856[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00441881[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00479208[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.0049542[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00518285[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00496627[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00501301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00433981[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00863234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00866845[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00608601[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00521417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0062637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00613527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00595334[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00493114[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00472683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00471406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00484406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00903813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00482225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00506311[0m
[38;5;104m[X] model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0938847 seconds. Fastest Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00433981[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x2eba0b6a8ec55fa3[0m
[38;5;104m[X] =============== Computing costs for /model/encoder/Resize_1[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0321629[0m
[38;5;104m[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00190876 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0321629[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1 (Resize[0x8000001f])[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.00403315[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00360407[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00450897[0m
[38;5;104m[X] /model/encoder/Resize_1 (Resize[0x8000001f]) profiling completed in 0.00812924 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.00360407[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 0x0000000000000005[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0167659[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0134647[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0177089[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.01422[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0104219 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0134647[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00940355[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00885923[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0115086[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00726954 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00885923[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0154618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0110803[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0106417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0101256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0109096[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0152402[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00979261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0111566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.014164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00922032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00927141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0100649[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0112281[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00913701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0116097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00946755[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00945541[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.0103884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0108828[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00983435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0149946[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0144951[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0111652[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0109571[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00905341[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0101359[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00888197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0103825[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00862496[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0097027[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00843867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00984314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0132722[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0922676 seconds. Fastest Tactic: 0xe742f4598442d2f1 Time: 0.00843867[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xe742f4598442d2f1[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3276800,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1:16,2560,32) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.0106933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00998902[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0105207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00761261[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00851333[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00594788[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00736626[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0101304[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00904447[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0106953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00910875[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00757001[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00648308[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00615467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00675712[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.0102798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00651097[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00932563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00638873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0106287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.0108834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00715415[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00702355[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00743301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00723722[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00714916[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00606197[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00682383[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00882947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00600762[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0112441[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00879719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00627714[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0864488 seconds. Fastest Tactic: 0x458f02d2b10db57c Time: 0.00594788[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x458f02d2b10db57c[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110311[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.0104297[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00727235[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0134349[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00816442[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0180643[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00824768[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00787002[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0106913[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0130757[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0107453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0180581[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00742495[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0110328[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00780726[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00854756[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0130125[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0127854[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0183646[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0101763[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0180474[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.007584[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0543036 seconds. Fastest Tactic: 0xfdf7509af98902e0 Time: 0.00727235[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xfdf7509af98902e0[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0xfdf7509af98902e0, 0.00727235 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(204800,6400:4,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0219167 ms[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: CaskConvolution, tactic 0x23b890da05937b9e, 0.00899789 ms[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv [Int8(25600,6400:32,80,1) -> Float(25600,6400:32,80,1)] got cached result: CaskConvolution, tactic 0x2d8ab2aa0639fda9, 0.00965516 ms[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1), Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00537075[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00423831[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.0042298[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.0039694[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00385447[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.003796[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00398159[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.0038373[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00383669[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00385631[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00535755[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0288627 seconds. Fastest Tactic: 0x0000000000000005 Time: 0.003796[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000005[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1), Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00809829[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00666039[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00829425[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.00760873[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.00789284[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.0112505[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00922465[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.011609[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0168384[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0201613[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00706533[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.0066652[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.00658719[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00707[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00606197[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.005992[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00596762[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00669781[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00879691[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.0068689[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00601581[0m
[38;5;104m[X] PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.045648 seconds. Fastest Tactic: 0x000000000000001a Time: 0.00596762[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001a[0m
[38;5;104m[X] =============== Computing costs for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,6400,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,640,8) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,6400:32,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00624356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00710037[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00632332[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00566509[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00788391[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00605448[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00545566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00767903[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00706089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00645641[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00750388[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00803149[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00601181[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.0062319[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00668117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00723563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00642195[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00758861[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00588183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00640886[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00641169[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00528217[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00698133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00539389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.0052645[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00515069[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00626015[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.0065874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00704133[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00606953[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00656983[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00690046[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00645005[0m
[38;5;104m[X] model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0898867 seconds. Fastest Tactic: 0x65a38dbc9e991257 Time: 0.00515069[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x65a38dbc9e991257[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0123398[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0122152[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0130544[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0127364[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0103422 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0122152[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.0113813[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.0101207[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.0115925[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00694328 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.0101207[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Float(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0145795[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.0101337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.0123044[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.0101796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0124646[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0141573[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0120152[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0127435[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0137318[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.0103683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.010574[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.0100935[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0102697[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.0116256[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0126143[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0115185[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.0117234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.0102507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0110393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00985161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0139613[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0138287[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0119947[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.0122225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0114002[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.0102801[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00958964[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0107417[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.0109832[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.0138381[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.0120941[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.012104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0139324[0m
[38;5;104m[X] model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0809146 seconds. Fastest Tactic: 0x9ec201b34455146e Time: 0.00958964[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x9ec201b34455146e[0m
[38;5;104m[X] =============== Computing costs for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,6400:4,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1:16,1280,16) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.017938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.010471[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0105567[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0130236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00845493[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0315636[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119657[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0105103[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175528[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0115366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0173483[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0315433[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.011301[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.017609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0104293[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00873655[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0129218[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0115204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0321222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0316247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.0106843[0m
[38;5;104m[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0518591 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.00845493[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0179251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.01043[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.010536[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.0130564[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.0084632[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0119566[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.010534[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0176[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.0116028[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0174699[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0316053[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.011322[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176943[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0104787[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00857792[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.0129456[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.0114201[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0321377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0102161[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0315384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.010697[0m
[38;5;104m[X] model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0543305 seconds. Fastest Tactic: 0xc722efd60bc6ea84 Time: 0.0084632[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc722efd60bc6ea84[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(204800,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0xff6944b17d5b2e32, 0.0132205 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x5e4f6d7c83746fd6, 0.00560729 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv [Int8(25600,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x9ec201b34455146e, 0.00469022 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(819200,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1:16,1280,32) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) [Int8(25600,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x709ddd0e503c7fd7, 0.00455423 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6fd15a9d85252b17, 0.00581315 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(204800,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(12800,1:16,320,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x6fd15a9d85252b17, 0.00581315 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(51200,1600:4,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x69c4e2ca38eadce2, 0.0239421 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: CaskConvolution, tactic 0x85c1a5f7f239cf84, 0.00690764 ms[0m
[38;5;104m[X] model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv [Int8(6400,1600:32,40,1) -> Float(6400,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x45f7566cdb2b10fb, 0.00548302 ms[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add))[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(204800,1600,40,1), Float(204800,1600,40,1) -> Int8(204800,1600,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000002, 0.00280027 ms[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) [Float(6400,1600:32,40,1), Float(6400,1600:32,40,1) -> Int8(6400,1600:32,40,1)] got cached result: PointWiseV2, tactic 0x0000000000000018, 0.00329579 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,320,8) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) [Int8(6400,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: CaskConvolution, tactic 0x2eba0b6a8ec55fa3, 0.00388701 ms[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00997804[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00876198[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0102109[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00912519[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0107786 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00876198[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00569618[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00485488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00499821[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00832089 seconds. Fastest Tactic: 0x733ba2a91a48d431 Time: 0.00485488[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x733ba2a91a48d431[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0110018[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00480518[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00694182[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00539837[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00716255[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0107083[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00553163[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00720476[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0104877[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00500824[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00514724[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00532622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00482164[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00545858[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00741713[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.0052555[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00539974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00500107[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00530099[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00532063[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0104873[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.01052[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00691091[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00701178[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0053523[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00495184[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.0051497[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00493867[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00530878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00626094[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00563307[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00678165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.010715[0m
[38;5;104m[X] model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0885748 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00480518[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(102400,1600:4,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(6400,1:16,320,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] RunnerBuilder of layer implementation CaskJitConv cannot handle striding for node model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0179211[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00766012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104487[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00623111[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00743609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314153[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0117057[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103683[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0175938[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00690177[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0173733[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0315617[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.0110231[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176404[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0103095[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00860527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00577508[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00664701[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0320339[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00712488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.031393[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.010626[0m
[38;5;104m[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0541344 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00577508[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0178785[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00763515[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.0104723[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00627931[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00741594[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0314521[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0117035[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.0103793[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.01771[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00687216[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.0173056[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0314482[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.01105[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0176747[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.0103425[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00865204[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00581444[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00662191[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.031999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.0071183[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0313552[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.010706[0m
[38;5;104m[X] model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0535789 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00581444[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.0146917[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.0120442[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0151743[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0126811[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0105393 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.0120442[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00674794[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00504406[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00480487[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0084653 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00480487[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.013936[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00346986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00840373[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00450609[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.0085104[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0134251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.0058115[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00865696[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.0111563[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.00435366[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00437458[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00452987[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.00349389[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00577784[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00905975[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00553076[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00561725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00375994[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00406413[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00398171[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0129986[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0113582[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00825418[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00832468[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.00573794[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00381249[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00395846[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.0038465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00465778[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00673834[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00601791[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.0081933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0115163[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0897823 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00346986[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(204800,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1:16,640,32) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(6400,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.010554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.0035465[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.0101236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00751289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00434452[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.005233[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00726469[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00387833[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00373262[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0104457[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.00406554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00598324[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.0040999[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00508719[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00531403[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00362991[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00529727[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00393562[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00410849[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.0103118[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.010741[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00697978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00548302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00706222[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00695978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00684451[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00533604[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00446507[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.00396336[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00503753[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0111836[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00372302[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00556373[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0929961 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.0035465[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.0110135[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00559876[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xfdf7509af98902e0 Time: 0.00702[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00471782[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc722efd60bc6ea84 Time: 0.00531166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.0178639[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.00780353[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00697289[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x3ac8602b2543f50d Time: 0.0107337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00500012[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x01cd56dfbdb5c0ee Time: 0.010584[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xafad4a0ea10d6400 Time: 0.0179554[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xbd976ef514eaa406 Time: 0.00712011[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.0108934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6c9b9925c4cc67b0 Time: 0.00687826[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00599162[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xc985777c89c6b3a4 Time: 0.00436045[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6fd15a9d85252b17 Time: 0.00485117[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.0181878[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0x6176c23707257237 Time: 0.00519746[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3_swish Tactic: 0xad8a45d1c06da185 Time: 0.0179144[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00724707[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.0594207 seconds. Fastest Tactic: 0xc985777c89c6b3a4 Time: 0.00436045[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc985777c89c6b3a4[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(3200,1:16,160,8) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul)[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) [Int8(1600,400:32,20,1) -> Int8(1600,400:32,20,1)] got cached result: CaskConvolution, tactic 0xc985777c89c6b3a4, 0.00436045 ms[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0271696[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.0216833[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.00506062 seconds. Fastest Tactic: 0x69c4e2ca38eadce2 Time: 0.0216833[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x69c4e2ca38eadce2[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x64x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x23b890da05937b9e Time: 0.00824377[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xac914b235d066808 Time: 0.0134622[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0xa8b56a226b057463 Time: 0.00656774[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x85c1a5f7f239cf84 Time: 0.00628859[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3_alignc4 Tactic: 0x2bcbba39f608bf10 Time: 0.00855002[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0136677 seconds. Fastest Tactic: 0x85c1a5f7f239cf84 Time: 0.00628859[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x85c1a5f7f239cf84[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0210113[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00628405[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xea2b7420057a01c1 Time: 0.0188527[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00758933[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xd14bd6d95fefd45e Time: 0.00436073[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.0122225[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x55edef142e02adaa Time: 0.0116892[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00613314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.0134942[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x45f7566cdb2b10fb Time: 0.00482453[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0edd5d0285e564d4 Time: 0.00750032[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.00493365[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x0e07dc8353bf7e9f Time: 0.0119874[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.0120819[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xad886d4d69834922 Time: 0.00752166[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x2d8ab2aa0639fda9 Time: 0.00550592[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3 Tactic: 0xb936321f82fd390c Time: 0.00531234[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00466978[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00619714[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00945481[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00777798[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r3s3 Tactic: 0x47b1629a4bff4800 Time: 0.020805[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0191022[0m
[38;5;104m[X] model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0629871 seconds. Fastest Tactic: 0xd14bd6d95fefd45e Time: 0.00436073[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xd14bd6d95fefd45e[0m
[38;5;104m[X] =============== Computing costs for PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add))[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,400,20,1), Float(51200,400,20,1) -> Int8(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240338[0m
[38;5;104m[X] Tactic: 0x0000000000000001 Time: 0.00254352[0m
[38;5;104m[X] Tactic: 0x0000000000000002 Time: 0.00240161[0m
[38;5;104m[X] Tactic: 0x0000000000000003 Time: 0.0027706[0m
[38;5;104m[X] Tactic: 0x0000000000000004 Time: 0.00250332[0m
[38;5;104m[X] Tactic: 0x0000000000000005 Time: 0.00241917[0m
[38;5;104m[X] Tactic: 0x0000000000000006 Time: 0.00337627[0m
[38;5;104m[X] Tactic: 0x0000000000000007 Time: 0.00298152[0m
[38;5;104m[X] Tactic: 0x0000000000000008 Time: 0.00263693[0m
[38;5;104m[X] Tactic: 0x0000000000000009 Time: 0.00249576[0m
[38;5;104m[X] Tactic: 0x000000000000001c Time: 0.00238766[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0500929 seconds. Fastest Tactic: 0x000000000000001c Time: 0.00238766[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x000000000000001c[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1600,400:32,20,1), Float(1600,400:32,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028])[0m
[38;5;104m[X] Tactic: 0x000000000000000a Time: 0.00337972[0m
[38;5;104m[X] Tactic: 0x000000000000000b Time: 0.00392408[0m
[38;5;104m[X] Tactic: 0x000000000000000c Time: 0.00545617[0m
[38;5;104m[X] Tactic: 0x000000000000000d Time: 0.0054634[0m
[38;5;104m[X] Tactic: 0x000000000000000e Time: 0.0064919[0m
[38;5;104m[X] Tactic: 0x000000000000000f Time: 0.00949304[0m
[38;5;104m[X] Tactic: 0x0000000000000010 Time: 0.00866653[0m
[38;5;104m[X] Tactic: 0x0000000000000011 Time: 0.0108408[0m
[38;5;104m[X] Tactic: 0x0000000000000012 Time: 0.0147891[0m
[38;5;104m[X] Tactic: 0x0000000000000013 Time: 0.0194679[0m
[38;5;104m[X] Tactic: 0x0000000000000014 Time: 0.00282649[0m
[38;5;104m[X] Tactic: 0x0000000000000015 Time: 0.00320346[0m
[38;5;104m[X] Tactic: 0x0000000000000016 Time: 0.0042096[0m
[38;5;104m[X] Tactic: 0x0000000000000017 Time: 0.00592412[0m
[38;5;104m[X] Tactic: 0x0000000000000018 Time: 0.00248059[0m
[38;5;104m[X] Tactic: 0x0000000000000019 Time: 0.00268373[0m
[38;5;104m[X] Tactic: 0x000000000000001a Time: 0.00314949[0m
[38;5;104m[X] Tactic: 0x000000000000001b Time: 0.00416105[0m
[38;5;104m[X] Tactic: 0x000000000000001d Time: 0.00512711[0m
[38;5;104m[X] Tactic: 0x000000000000001e Time: 0.00292847[0m
[38;5;104m[X] Tactic: 0x000000000000001f Time: 0.00251364[0m
[38;5;104m[X] PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) (PointWiseV2[0x80000028]) profiling completed in 0.0651739 seconds. Fastest Tactic: 0x0000000000000018 Time: 0.00248059[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0x0000000000000018[0m
[38;5;104m[X] =============== Computing costs for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,400:4,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,1:16,160,8) -> Int8(6400,1:16,320,16) ***************[0m
[38;5;104m[X] Skipping CaskConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1600,400:32,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x0e1225de5ed2825a Time: 0.00617758[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc6cdb1e47323bb01 Time: 0.00310963[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x838704902abae3eb Time: 0.00602972[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x214bdfa026549ff2 Time: 0.00514823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x2f5bc3e6bb27ae43 Time: 0.00357742[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x458f02d2b10db57c Time: 0.00392[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xb7cf5027c4b93d29 Time: 0.00492298[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_swish Tactic: 0x7251b68d123da92b Time: 0.00341932[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe2d66a9164dfe333 Time: 0.00321939[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_swish Tactic: 0x179844a379940fc2 Time: 0.00627516[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xec71f23be6dfba13 Time: 0.003467[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_swish Tactic: 0x4798bd5eea3be0d6 Time: 0.0046267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x4a25dfdaea3c22a0 Time: 0.00336948[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0xe19ca9292536af94 Time: 0.00381515[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xcd11ae6379963c47 Time: 0.00408884[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x44824770683c7b80 Time: 0.00323446[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0xfbca5e767c4ed4f2 Time: 0.00394918[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x00c7d39818f4aff2 Time: 0.00341725[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x2eba0b6a8ec55fa3 Time: 0.00325841[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x208d75dd219769a1 Time: 0.00623269[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x22aa7a0240f7ac24 Time: 0.00626173[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0xc1a4243b1f1cfe6d Time: 0.00473239[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x30905ef9f5106c22 Time: 0.00429607[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x698ab7d6de17ffeb Time: 0.00485766[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x8cca66d35c4f08ca Time: 0.00472744[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x65a38dbc9e991257 Time: 0.00461823[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1_swish Tactic: 0x70f060961873cac4 Time: 0.00403894[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x5dcda6f3b1eea89a Time: 0.00353325[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x98ea6780adda9b75 Time: 0.0033934[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x779395d7339f2fc2 Time: 0.00369524[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_swish Tactic: 0x57f2a1d1b8552d02 Time: 0.00653272[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish Tactic: 0x709ddd0e503c7fd7 Time: 0.00322974[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_swish Tactic: 0x552ac687d7891695 Time: 0.00423156[0m
[38;5;104m[X] model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) (CaskConvolution[0x80000009]) profiling completed in 0.105937 seconds. Fastest Tactic: 0xc6cdb1e47323bb01 Time: 0.00310963[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul)[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xc6cdb1e47323bb01[0m
[38;5;104m[X] =============== Computing costs for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,400:4,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_interior_nn_v1 Tactic: 0xa4ae2d82115c3e83 Time: 0.00984878[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_interior_nn_v1 Tactic: 0xff6944b17d5b2e32 Time: 0.00875077[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x32_relu_small_nn_v1 Tactic: 0xfe80445f7bb61a99 Time: 0.0100144[0m
[38;5;104m[X] Tactic Name: ampere_fp32_icudnn_int8x4_128x64_relu_small_nn_v1 Tactic: 0x69c4e2ca38eadce2 Time: 0.00901535[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0111279 seconds. Fastest Tactic: 0xff6944b17d5b2e32 Time: 0.00875077[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0xff6944b17d5b2e32[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0xf04572b287451f42 Time: 0.00574048[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x733ba2a91a48d431 Time: 0.00422967[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4 Tactic: 0x5e4f6d7c83746fd6 Time: 0.00408[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0118171 seconds. Fastest Tactic: 0x5e4f6d7c83746fd6 Time: 0.00408[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x5e4f6d7c83746fd6[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(3200,400:32,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009])[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32 Tactic: 0xb5f710b331ba8377 Time: 0.0108635[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6d377e4222886190 Time: 0.00338909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x1e55f8b415964e81 Time: 0.00695089[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0xdc1f355deb032b87 Time: 0.00394805[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x84942841d92b0552 Time: 0.00713464[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcd229658c16b33cd Time: 0.0105637[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x8141573686849b61 Time: 0.00494384[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x991db9fd58152c33 Time: 0.00715257[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x91930a570b557437 Time: 0.010356[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x960e9baa2a6cad5b Time: 0.003796[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xa71946688cad8664 Time: 0.00385251[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x3f2b14dec582741e Time: 0.00393236[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x5bd8221bd57baf93 Time: 0.0034326[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x271b998fe31732ef Time: 0.00495812[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0x6986b0c6a136276e Time: 0.00727165[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x6e9c17a33c93d9b0 Time: 0.00470926[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x128x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x844ea9c00f711f19 Time: 0.00478232[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x65fbe45b4cb1d8a5 Time: 0.00343314[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x2468d082d0ff7c9a Time: 0.0036752[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x33a5c6dd086942c1 Time: 0.00364707[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x128x64_stage4_warpsize4x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9003f5f7ff9b1aec Time: 0.0103247[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32_t1r1s1 Tactic: 0x60b880e28fee7a0c Time: 0.0103337[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xb86b920539eb9c79 Time: 0.00671488[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xdf7e1bd6a496d667 Time: 0.00682449[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x7720f198395e7d3d Time: 0.0047907[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r1s1 Tactic: 0xcf64a2ae51bf6b36 Time: 0.00353909[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0x9ec201b34455146e Time: 0.00357141[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32 Tactic: 0x445983715412fbda Time: 0.00363038[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0x5f1a472d416ff35e Time: 0.00410237[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32 Tactic: 0xa2f15b0a75b7dcec Time: 0.00560267[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32 Tactic: 0xe742f4598442d2f1 Time: 0.00505616[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1 Tactic: 0xbeb5d91e1874a437 Time: 0.00668245[0m
[38;5;104m[X] Tactic Name: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x256x64_stage4_warpsize2x4x1_g1_tensor16x8x32 Tactic: 0x748f926574b7bdc4 Time: 0.0106993[0m
[38;5;104m[X] model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv (CaskConvolution[0x80000009]) profiling completed in 0.0949777 seconds. Fastest Tactic: 0x6d377e4222886190 Time: 0.00338909[0m
[38;5;104m[X] Skipping CaskFlattenConvolution: No valid tactics for model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 0x6d377e4222886190[0m
[38;5;104m[X] =============== Computing costs for {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]}[0m
[38;5;104m[X] *************** Autotuning format combination: Int64(2,1), Float(1638400,6400,80,1), Float(409600,1600,40,1), Float(102400,400,20,1) -> Int64(300,1), Float(1200,4,1), Float(300,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023])[0m
[38;5;104m[X]  (foreignNode) Set user's cuda kernel library[0m
[38;5;104m[X] Subgraph compilation completed in 9.474 seconds.[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.409941[0m
[38;5;104m[X] {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} (Myelin[0x80000023]) profiling completed in 9.60634 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.409941[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Myelin Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00495765[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0169477[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00585937[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0229281 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00495765[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00948592[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0181311[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00408078[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0152663 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00408078[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0287369[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0239909[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0287796[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00669435 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0239909[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.216075[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0191567[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.216213[0m
[38;5;104m[X] /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00717903 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0191567[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0101518[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0118573[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0101194[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0126491 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0101194[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0294169[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0252145[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.029528[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00656741 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0252145[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1228800,409600,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.215765[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0226603[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.216027[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00697789 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0226603[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00858174[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0142724[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00857709[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00846376 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00857709[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.030016[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0131434[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0301031[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00703465 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0131434[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:4,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.216187[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.030657[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0159705[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00680863 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0159705[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00894961[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0502949[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00894456[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00805236 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00894456[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0108628[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0145118[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0109526[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0078809 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0108628[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,640,1) -> Int8(409600,409600:32,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.217093[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0240655[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.217131[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00698899 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0240655[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(1228800,409600,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0117021[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0502293[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0117572[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0072201 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0117021[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,409600:4,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0130699[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0149811[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00522533[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00834682 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00522533[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,409600:32,640,1) -> Int8(409600,1:16,640,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.030088[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0122423[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0300818[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00661206 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0122423[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0264295[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0114272[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0263992[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00649865 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0114272[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0640267[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0120038[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0640142[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00648988 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0120038[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0100097[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0116973[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00469022[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00818201 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00469022[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0199391[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0100668[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0199172[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.006962 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0100668[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0114272 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,640,2) -> Int8(102400,102400:32,320,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0120038 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(819200,102400:4,320,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00469022 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,102400:32,320,1) -> Int8(204800,1:16,640,2)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0100668 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(1638400,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0504427[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0125859[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0504869[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00743487 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0125859[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1:16,1280,4) -> Int8(204800,102400:32,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.138923[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.022501[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.138948[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00722911 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.022501[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,102400:32,320,1) -> Int8(1638400,102400:4,320,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.015935[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0265666[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00726887[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/MaxPool_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00935595 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00726887[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0107183[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0253006[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.010704[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.0093354 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.010704[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0113941[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0174496[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0113582[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00776535 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0113582[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0289982[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0140284[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00417267[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00718263 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00417267[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0151151[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00991404[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0149894[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00648877 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00991404[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00687717[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00881095[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00370418[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00805447 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370418[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0108022[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00791987[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102675[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00665709 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00791987[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00949244[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00768703[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00439551[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00740373 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00439551[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0109368[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00655059[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0108797[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00668101 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00655059[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0278791[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00705044[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0278907[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00595418 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00705044[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102413[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00743656[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102277[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00659784 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00743656[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.026464[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00762861[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0037107[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00718502 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0037107[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.01367[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00665412[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0136538[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0064478 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00665412[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0344128[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00709742[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0342432[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00592656 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00709742[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00630159[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00738412[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00336991[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00792552 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00336991[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0102125[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00583117[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0102077[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00670243 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00583117[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00665412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709742 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336991 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00439551 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705044 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0037107 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00665412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709742 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336991 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0415076[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00934311[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0415052[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0185391 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00934311[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0221927[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00890639[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222227[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00672547 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00890639[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0415763[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00929215[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0416593[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00650967 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00929215[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0222471[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0101899[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222343[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.0/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00620359 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0101899[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0052475[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00911424[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00529101[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.007677 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0052475[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.008408[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0081561[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00375083[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00767211 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00375083[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00932948[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00711149[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.009336[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00665665 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00711149[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0278436[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00747401[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0277596[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00600798 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00747401[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0224064[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00761818[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0223609[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00607282 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00761818[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0225564[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00773042[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0225415[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00630264 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00773042[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00928474[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00762836[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00927941[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00684708 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00762836[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0407573[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00783851[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0407976[0m
[38;5;104m[X] /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00617672 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00783851[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00439551 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00655059 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705044 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00743656 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0037107 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00665412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709742 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336991 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00583117 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00665412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709742 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336991 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(1638400,25600,160,1) -> Float(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00934311 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.0/blocks.0/act/Relu_output_0 -> <out>) [Float(51200,25600:32,160,1) -> Float(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00890639 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0052475 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00375083 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00711149 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00747401 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(1638400,25600,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00761818 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00773042 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00762836 ms[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(51200,25600:32,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00783851 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00439551 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705044 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0037107 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00665412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709742 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336991 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00439551 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00655059 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,25600,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00705044 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00743656 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,25600:4,160,1) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0037107 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00665412 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,640,4) -> Int8(51200,25600:32,160,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00709742 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(409600,25600:4,160,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00336991 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,25600:32,160,1) -> Int8(102400,1:16,640,4)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00583117 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129748[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0078013[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0130059[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00647645 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0078013[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0226084[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00781023[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0225493[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00604482 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00781023[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00468548[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00794413[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00408039[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00828159 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00408039[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:4,80,1) -> Int8(12800,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00939437[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0118084[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00430236[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00705885 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00430236[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,6400:32,80,1) -> Int8(102400,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00400851[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555911[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268134[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0082548 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268134[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022104[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541488[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0220093[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.006219 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00541488[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0123349[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540336[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123749[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00641114 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00540336[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(819200,6400,80,1) -> Float(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0220827[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563609[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.02207[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00620557 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563609[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,6400:32,80,1) -> Float(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0123234[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00674517[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123284[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.1/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00651237 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00674517[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00379976[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0067503[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00382073[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00822472 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00379976[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00541178[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00583136[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00296194[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00756012 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00296194[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0100151[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550907[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00994415[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00651945 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00550907[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0155312[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00568787[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0156417[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00618357 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00568787[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0125784[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00623546[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0126155[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00625738 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00623546[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0127447[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00623664[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0128008[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00639098 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00623664[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00578556[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00617304[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00576828[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00724091 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00576828[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0222343[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00642995[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0223303[0m
[38;5;104m[X] /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00593003 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00642995[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00625245[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00647323[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00324225[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00779656 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00324225[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00919639[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.006008[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00921052[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00670966 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.006008[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0158143[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00625462[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0157745[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00625954 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00625462[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00649744[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00645641[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00647016[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00701042 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00645641[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0155593[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00627654[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00309798[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00699485 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00309798[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0078013 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781023 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00408039 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00632433[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0056416[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00630058[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00704652 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0056416[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0078013 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781023 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00408039 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00541488 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.1/blocks.0/act/Relu_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00540336 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00379976 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00296194 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00550907 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00568787 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00623546 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00623664 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x0000000000000000, 0.00576828 ms[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(25600,6400:32,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00642995 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324225 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00625462 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00309798 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0078013 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781023 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00408039 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324225 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.006008 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00625462 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00645641 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00309798 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0078013 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781023 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00408039 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0056416 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0079934[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562862[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0079873[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.006755 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00562862[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0131643[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00573306[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.013145[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00632317 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00573306[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00361462[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567684[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267025[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00838271 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267025[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1600:4,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00635915[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559467[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00253349[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00745515 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00253349[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00314596[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551047[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00257337[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00848982 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00257337[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0123368[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557547[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123496[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00632831 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557547[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00748682[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550138[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00749867[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00680671 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00550138[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(409600,1600,40,1) -> Float(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0124014[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552411[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0123295[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00635384 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552411[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(12800,1600:32,40,1) -> Float(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00749701[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548074[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00745695[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.2/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00679239 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00548074[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00301953[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00535788[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.002992[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00880216 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.002992[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00397257[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555314[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0025968[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00790513 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0025968[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00705022[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551152[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00703933[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00685183 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551152[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00928503[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552516[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00929392[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00648813 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552516[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00767103[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544895[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00773333[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00676697 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544895[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00781495[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544034[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00779485[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00675617 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544034[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00419493[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00585525[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00418506[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0082401 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00418506[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0128012[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0063521[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0128279[0m
[38;5;104m[X] /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00638221 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0063521[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00444971[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564516[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00295212[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00830426 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00295212[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00738713[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566924[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00737948[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00694352 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00566924[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00955642[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571444[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00956495[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00649566 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00571444[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00469559[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00570486[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00469589[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00753938 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00469559[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00945185[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00570938[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00299533[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00764144 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00299533[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562862 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267025 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00453996[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00578373[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00450782[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00758523 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00450782[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562862 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267025 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(409600,1600,40,1) -> Float(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557547 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/act/Relu_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00550138 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.002992 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0025968 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00551152 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552516 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00544895 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00544034 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00418506 ms[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear [Float(12800,1600:32,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0063521 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00295212 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571444 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299533 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562862 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267025 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00295212 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566924 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571444 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00469559 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299533 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562862 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267025 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00450782 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00575448[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567286[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00577563[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00716475 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00567286[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00845547[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566545[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0084952[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0066967 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00566545[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00313461[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00566039[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00241631[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00782268 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00241631[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00457731[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567178[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267349[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00829184 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267349[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00289076[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00574554[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00230348[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00871508 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00230348[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00734423[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567449[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00735281[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00682731 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00567449[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0050361[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549491[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00507313[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00742485 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0050361[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,400,20,1) -> Float(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00733519[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558222[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00733611[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00685979 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558222[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,400:32,20,1) -> Float(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00505972[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553058[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00503339[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/backbone/res_layers.3/blocks.0/act/Relu_output_0) (Reformat[0x80000006]) profiling completed in 0.00744124 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00503339[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00268023[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00550348[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00269299[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00899828 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00268023[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00313052[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558347[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0023811[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.0084568 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0023811[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00534485[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545721[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00529524[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00732562 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00529524[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00595638[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554719[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00593647[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00711401 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00554719[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00504979[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552708[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00505713[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00735256 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00504979[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00513986[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567756[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00513444[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00728764 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00513444[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00329746[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548651[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00331501[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00840496 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00329746[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00793879[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00557387[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0079231[0m
[38;5;104m[X] /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00669516 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00557387[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00338166[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558933[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0026857[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00865481 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026857[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00559236[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561387[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00564427[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00735179 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00559236[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,400,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00621314[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563111[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00617038[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00712163 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563111[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00373926[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560231[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00374258[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807447 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00373926[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00605276[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00572764[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00300896[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00787486 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00300896[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00567286 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566545 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00241631 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00351585[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567232[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0035328[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00812314 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00351585[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00567286 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566545 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00241631 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(204800,400,20,1) -> Float(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00567449 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/act/Relu_output_0 -> <out>) [Float(6400,400:32,20,1) -> Float(204800,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0050361 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0023811 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00554719 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00513444 ms[0m
[38;5;104m[X] /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear [Float(6400,400:32,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00557387 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300896 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00241631 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00369055[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551519[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00370856[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/input_proj.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807923 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00369055[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00324225 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.006008 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00625462 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00645641 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,6400:4,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00309798 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0078013 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,640,8) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00781023 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00408039 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(51200,1:16,640,8)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0056416 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.022917[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0063515[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0228935[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00605733 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0063515[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0233564[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00564213[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0233465[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00623418 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00564213[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) long-strided -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.042064[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563253[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0419907[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00592343 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00563253[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129686[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.006016[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0129858[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00633612 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.006016[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00636841[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00615971[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00340691[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00774845 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00340691[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) long-strided -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103916[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00595734[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103816[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00652567 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00595734[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0223531[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00654264[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0222784[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00598637 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00654264[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0229596[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00660727[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0229028[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00611233 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00660727[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.01038[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00712488[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103463[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00660848 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00712488[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.041728[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00708311[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0416933[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00585134 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00708311[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129641[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00731409[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0129826[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.0063615 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00731409[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00638974[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00753185[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00339879[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00797659 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00339879[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103683[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00707333[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0104029[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00661012 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00707333[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.027538[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00726493[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00381491[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_3_/model/encoder/input_proj.0/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00718805 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00381491[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00295212 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566924 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571444 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00469559 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299533 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562862 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267025 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x0000000000000000, 0.00450782 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00873956[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00637746[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00879523[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00674925 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00637746[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00900575[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00630601[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0089353[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00658213 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00630601[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) long-strided -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0136777[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00608446[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0136653[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00622898 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00608446[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00565966[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00600629[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00569347[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00714629 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00565966[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00368094[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571227[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0026972[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00835759 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0026972[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00452324[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00582511[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00451921[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00756301 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00451921[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00777997[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552254[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0077423[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00687407 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552254[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00806502[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00572438[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00807568[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00675743 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00572438[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00451993[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567123[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00451921[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00756144 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00451921[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0130609[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558631[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0130658[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00631549 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00558631[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00566888[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567557[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00566111[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00713121 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00566111[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00361015[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00567666[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268749[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00838803 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268749[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00451229[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00572005[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00451459[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00758976 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00451229[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0100035[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0056208[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00269067[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_2_/model/encoder/input_proj.1/norm/BatchNormalization_output_0_clone_1) (Reformat[0x80000006]) profiling completed in 0.00767359 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00269067[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(25600,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00278738[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555244[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00226917[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00921584 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00226917[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00428184[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00572222[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00429825[0m
[38;5;104m[X] /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear (Reformat[0x80000006]) profiling completed in 0.00761237 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00428184[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,400:4,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267349 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00230348 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00358779[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561973[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240629[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00811286 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00240629[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00272009[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553495[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00522883[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00807692 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00272009[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00365647[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553285[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238408[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00805047 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238408[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00489384[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553006[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00494416[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00750023 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00489384[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00445667[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00582161[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0052535[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0138736 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00445667[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00369055 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0026793[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00562222[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267401[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00915264 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267401[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00374483[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543449[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00371757[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00806142 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00371757[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00270331[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552691[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00272711[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00911978 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00270331[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0111794[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00591794[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112274[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/lateral_convs.0/norm/BatchNormalization_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00640537 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591794[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00356846[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555209[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00239695[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00805406 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00239695[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00272356[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556036[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.005253[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00791571 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00272356[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00363559[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005696[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00239573[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00801092 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00239573[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00493914[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558293[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00494133[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00743609 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00493914[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00445525[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561511[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00524683[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00743459 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00445525[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00374175[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055952[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00237647[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0080255 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00237647[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0037331[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551607[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00366284[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00808766 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00366284[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00267273[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565785[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268672[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00921426 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00267273[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00588276[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565804[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00584111[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00712056 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00565804[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(102400,1,5120,256) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0110531[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00591813[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0110366[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00650644 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591813[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00229078[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055696[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00549753[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00759638 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00229078[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00355711[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558987[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00354746[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00808186 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00354746[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00357821[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556782[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00354021[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00814334 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00354021[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00490156[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00559111[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00490337[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00746251 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00490156[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1,400,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00443382[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0167077[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0044306[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00703851 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0044306[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00369548[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551327[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00238164[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00812056 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00238164[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00270108[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558205[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268843[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00913251 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268843[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00369512[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054991[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00369934[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00809766 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00369512[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00588969[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00568949[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00592524[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00711096 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00568949[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(25600,1:4,1280,64) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.011322[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00591925[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112533[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00674981 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591925[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00372978[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543243[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00368774[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00806746 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00368774[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00269978[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558045[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00266607[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00917353 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00266607[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00373974[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544619[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00370797[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00804086 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00370797[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00276328[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055395[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00277201[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0091288 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00276328[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(3200,400:32,20,1) -> Float(1:4,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0112626[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00602343[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0112921[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00636595 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00602343[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00282147[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0160249[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.005576[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00744229 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00282147[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(102400,1,5120,256) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00362783[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170811[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00365171[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00743212 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00362783[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(1,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00281134[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0161285[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00280641[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.0081846 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00280641[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(25600,1:4,1280,64) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00368586[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0170885[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00368715[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00851541 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00368586[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1:4,400,20,1) -> Float(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00484576[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0171141[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00484808[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/lateral_convs.0/act/Mul_output_0) (Reformat[0x80000006]) profiling completed in 0.00693454 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00484576[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00238278[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0053269[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00239627[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00830451 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00238278[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 [Float(102400,400,20,1) -> Int8(3200,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00428184 ms[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00375682[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005511[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0037651[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00791648 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00375682[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0053203[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558205[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0053186[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0072237 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0053186[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00240483[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00262157 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00240483[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00430359[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00255257 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00430359[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00384454[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549071[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00384392[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.0078561 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00384392[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00535467[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549456[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00536821[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00728744 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00535467[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00375527[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540181[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00378535[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00824715 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00375527[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0052595[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540198[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00524617[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00756121 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00524617[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00282911[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00337036 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00282911[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00422008[0m
[38;5;104m[X] /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0 (Reformat[0x80000006]) profiling completed in 0.00270665 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00422008[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,400,20,1) -> Int8(3200,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00469096[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.005504[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00467733[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.007908 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00467733[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(102400,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00343271[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055054[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00341671[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00855343 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00341671[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00276981[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545548[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00278756[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00955143 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00276981[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00446307[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0055872[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00292604[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.0082895 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00292604[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00741049[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547987[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0074157[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00689843 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00547987[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00957318[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00553373[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00954088[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00652832 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00553373[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0056687[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551939[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00566744[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00721252 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00551939[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00363525[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00563573[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270598[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00833563 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270598[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00451171[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560516[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00452843[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00793382 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00451171[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00974507[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556053[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00269419[0m
[38;5;104m[X] /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00773386 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00269419[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00631749[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00540146[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00338004[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00770408 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00338004[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0159431[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552411[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0160168[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00613839 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00552411[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.015472[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552481[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00389718[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00711216 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00389718[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0140556[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560729[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.014064[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00623736 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00560729[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0233934[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00565171[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0233358[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00612166 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00565171[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00467126[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00586281[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00283245[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00791729 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00283245[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552411 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00389718 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00565171 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00257337 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(204800,1600,40,1) -> Float(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00734493[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542211[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00734655[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00687993 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00542211[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(6400,1600:32,40,1) -> Float(204800,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0050283[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00534095[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00503944[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00743779 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0050283[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00542211 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0050283 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00630058[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555226[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00631749[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00697134 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00555226[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00567684[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548686[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00567955[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00713147 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00548686[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00450609[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00560942[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0045316[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Resize_1_output_0) (Reformat[0x80000006]) profiling completed in 0.00756159 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00450609[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571444 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(25600,1:16,640,16) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00777724[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00536364[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00772558[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00688594 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00536364[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00566689[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544258[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00566111[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/Resize_1_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00716374 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00544258[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00367156[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00551519[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00363629[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00808564 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00363629[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00987231[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00571679[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00439074[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00709513 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00439074[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0205214[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00631165[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0205289[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00609813 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00631165[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(1638400,6400,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0284764[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00576019[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0284596[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00597983 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00576019[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(3276800,6400,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129932[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00634123[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0130219[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00629244 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00634123[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(819200,6400:4,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00633137[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0063515[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00339523[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00770081 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00339523[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(204800,1:16,2560,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0104317[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00606875[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0104177[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.00639479 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00606875[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(51200,6400:32,80,1) -> Int8(102400,6400:32,80,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0275019[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00639819[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0038154[0m
[38;5;104m[X] /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy (Reformat[0x80000006]) profiling completed in 0.0070144 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0038154[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0169093[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00818524[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00626133[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00646395 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00626133[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0533105[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00669973[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0533653[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00622562 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00669973[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0483002[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0085184[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0111783[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00616041 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0085184[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0432187[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00764873[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0433133[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00606164 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00764873[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0801003[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.008504[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0799872[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00641468 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.008504[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,6400:32,80,1) -> Int8(819200,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00971428[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00861539[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00428239[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00755372 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00428239[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3276800,6400,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00669973 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400:4,80,1) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.0085184 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1:16,2560,32) -> Int8(102400,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.008504 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,6400:32,80,1) -> Int8(204800,6400:4,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00408039 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00541488 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00540336 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(819200,6400,80,1) -> Float(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00541488 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(25600,6400:32,80,1) -> Float(819200,6400,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00540336 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,6400,80,1) -> Int8(25600,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00625462 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0129846[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00725866[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0129657[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00635473 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00725866[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00634788[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00708222[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00335083[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00790043 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00335083[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(102400,1:16,1280,16) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0103657[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00650749[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0103538[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00657659 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00650749[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00982306[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00641682[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00436627[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00740746 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00436627[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0284889[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00625185[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0285013[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00598638 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00625185[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0265641[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00669035[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00448156[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00688403 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00448156[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0230002[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00677461[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0228914[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0060801 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00677461[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0418373[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00710014[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0419813[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00581148 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00710014[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,6400:32,80,1) -> Int8(409600,6400:4,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00633801[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00747662[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00339437[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00785125 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00339437[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(1638400,6400,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00625185 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,6400:4,80,1) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00448156 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1:16,1280,16) -> Int8(51200,6400:32,80,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00710014 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00565966 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0026972 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(25600,1600:32,40,1) long-strided -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x0000000000000000, 0.00451921 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00566111 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00268749 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00451229 ms[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_4_/model/encoder/downsample_convs.0/act/Mul_output_0_clone_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00269067 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00278853[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00605219[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00278222[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00857818 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00278222[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00443677[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00590484[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0029542[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00805825 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.0029542[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00738829[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0058633[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00739153[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00689288 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.0058633[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(409600,1600,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00937985[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00591457[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00938163[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00649823 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00591457[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00775615[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00583265[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00773818[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00679067 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00583265[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00801321[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00589717[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00799035[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00665313 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00589717[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00454011[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00586255[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00448427[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00756245 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00448427[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(25600,1:16,640,16) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0131368[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00584497[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0131102[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00629345 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00584497[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003ea[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(819200,1600,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00566183[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00583467[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00568841[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00712065 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00566183[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(204800,1600:4,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00357651[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00587359[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00267409[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00847171 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00267409[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(51200,1:16,1280,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0045156[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00587752[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00450897[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00752599 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00450897[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Int8(12800,1600:32,40,1) -> Int8(25600,1600:32,40,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0100132[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558702[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00270796[0m
[38;5;104m[X] /model/encoder/Resize_1_output_0 copy (Reformat[0x80000006]) profiling completed in 0.00762319 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00270796[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/Resize_1_output_0 copy[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00338004 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552411 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00389718 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00560729 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00565171 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1600:32,40,1) -> Int8(204800,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00283245 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(819200,1600,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00552411 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600:4,40,1) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00389718 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,1:16,1280,32) -> Int8(25600,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00565171 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,1600:32,40,1) -> Int8(51200,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00257337 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00542211 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0050283 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(204800,1600,40,1) -> Float(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00542211 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(6400,1600:32,40,1) -> Float(204800,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.0050283 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,1600,40,1) -> Int8(6400,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00555226 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00548686 ms[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00364151[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543484[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00268075[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) (Reformat[0x80000006]) profiling completed in 0.00836973 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00268075[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0) [Int8(12800,1600:32,40,1) -> Int8(25600,1:16,640,16)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00450609 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00295212 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571444 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299533 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00562862 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1600:32,40,1) -> Int8(102400,1600:4,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00267025 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(409600,1600,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00571444 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(102400,1600:4,40,1) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00299533 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(25600,1:16,640,16) -> Int8(12800,1600:32,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00573306 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00347956[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548179[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00342553[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.0081423 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00342553[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00288294[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547672[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00230503[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00816169 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00230503[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(6400,400:32,20,1) long-strided -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00320579[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558382[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00319492[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00832171 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00319492[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(204800,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00341606[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00552551[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00339847[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00821873 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00339847[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(51200,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00284592[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549298[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00230334[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00865407 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00230334[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(12800,1:16,640,32) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00318781[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00561867[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00323528[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00829089 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00318781[0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(3200,400:32,20,1) -> Int8(6400,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00460698[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555965[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00252638[0m
[38;5;104m[X] Optimizer Reformat(<in> -> /model/encoder/Concat_5_/model/encoder/downsample_convs.1/act/Mul_output_0_clone_0) (Reformat[0x80000006]) profiling completed in 0.00763757 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00252638[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0024023[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0053186[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00240123[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00803134 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00240123[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00274212[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00543656[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00227207[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00882224 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00227207[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00383804[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00548162[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00381237[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00783912 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00381237[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00434341[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00549229[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00434327[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.007608 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00434327[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00375898[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00547567[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00378679[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00786814 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00375898[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00386661[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00538615[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00384907[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00789676 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00384907[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00294653[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00541385[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00299276[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00875434 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00294653[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(102400,1,5120,256) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00542761[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00555366[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00541213[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00751215 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00541213[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00240414[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00273047 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00240414[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0027736[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0032208 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0027736[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00379188[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00267084 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00379188[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00434299[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0025189 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00434299[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00384258[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537703[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00385569[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00785427 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00384258[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00388999[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.0054037[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00391169[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00793706 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00388999[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00292547[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00537755[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00293006[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00804846 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00292547[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(25600,1:4,1280,64) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00546323[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00544482[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00543226[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00728475 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00543226[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00378679[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00536618[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00377924[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00792901 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00377924[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x0000000000000000[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00394001[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00542366[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00394403[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00816455 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00394001[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0029541[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00532673[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00297053[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0088429 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.0029541[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(3200,400:32,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00530015[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00558738[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00534282[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00723067 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00530015[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(204800,400,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00282425[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00310084 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00282425[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(51200,400:4,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00282612[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00317549 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00282612[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(12800,1:16,640,32) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00386759[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.00266675 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00386759[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] *************** Autotuning format combination: Float(1:4,400,20,1) -> Int8(6400,400:32,20,1) long-strided ***************[0m
[38;5;104m[X] --------------- Timing Runner: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00428567[0m
[38;5;104m[X] /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1 (Reformat[0x80000006]) profiling completed in 0.0025484 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00428567[0m
[38;5;104m[X] Skipping MyelinReformat: No valid tactics for /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1[0m
[38;5;104m[X] >>>>>>>>>>>>>>> Chose Runner Type: Reformat Tactic: 0x00000000000003e8[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.0026857 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563111 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300896 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00567286 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566545 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(6400,400:32,20,1) -> Int8(51200,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00241631 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(204800,400,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00563111 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(51200,400:4,20,1) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00300896 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(12800,1:16,640,32) -> Int8(6400,400:32,20,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00566545 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(1600,400:32,20,1) -> Int8(12800,400:4,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00277527[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00545738[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00218625[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00887254 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00218625[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,400,20,1) -> Float(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00354055[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00554562[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00353089[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00820564 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00353089[0m
[38;5;104m[X] *************** Autotuning Reformat: Float(1600,400:32,20,1) -> Float(51200,400,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00303593[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00539149[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00308345[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00865701 seconds. Fastest Tactic: 0x00000000000003e8 Time: 0.00303593[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(51200,400,20,1) -> Float(1600,400:32,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00353089 ms[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0 -> <out>) [Float(1600,400:32,20,1) -> Float(51200,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00303593 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Int8(51200,400,20,1) -> Int8(1600,400:32,20,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.00381782[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00556711[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.00377924[0m
[38;5;104m[X] Optimizer Reformat(/model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.00799413 seconds. Fastest Tactic: 0x0000000000000000 Time: 0.00377924[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0 -> <out>) [Int8(3200,400:32,20,1) -> Int8(25600,400:4,20,1)] got cached result: Reformat, tactic 0x0000000000000000, 0.00230348 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] *************** Autotuning Reformat: Float(51200,6400:32,80,1) -> Float(1638400,6400,80,1) ***************[0m
[38;5;104m[X] --------------- Timing Runner: Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006])[0m
[38;5;104m[X] Tactic: 0x00000000000003e8 Time: 0.0219853[0m
[38;5;104m[X] Tactic: 0x00000000000003ea Time: 0.00683603[0m
[38;5;104m[X] Tactic: 0x0000000000000000 Time: 0.0220227[0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.0/conv/Conv_output_0 -> <out>) (Reformat[0x80000006]) profiling completed in 0.0060006 seconds. Fastest Tactic: 0x00000000000003ea Time: 0.00683603[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.1/conv/Conv_output_0 -> <out>) [Float(12800,1600:32,40,1) -> Float(409600,1600,40,1)] got cached result: Reformat, tactic 0x00000000000003ea, 0.00550138 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs: [0m
[38;5;104m[X] Optimizer Reformat(/model/decoder/input_proj.2/conv/Conv_output_0 -> <out>) [Float(3200,400:32,20,1) -> Float(102400,400,20,1)] got cached result: Reformat, tactic 0x00000000000003e8, 0.00369055 ms[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] =============== Computing reformatting costs for available format set[0m
[38;5;104m[X] Formats and tactics selection completed in 31.2533 seconds.[0m
[38;5;104m[X] After reformat layers: 85 layers[0m
[38;5;104m[X] Total number of blocks in pre-optimized block assignment: 81[0m
[38;5;13m[V] Detected 2 inputs and 3 output network tensors.[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Host Persistent: 4880 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/MaxPool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Host Persistent: 4144 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Host Persistent: 4944 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Host Persistent: 4368 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: {ForeignNode[onnx::MatMul_3620 + ONNXTRT_Broadcast_101.../model/encoder/Transpose_1 + /model/encoder/Reshape_1]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 13107200 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Host Persistent: 308 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Host Persistent: 436 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Host Persistent: 5136 bytes Device Persistent: 0 bytes Scratch Memory: 0 bytes[0m
[38;5;104m[X] Layer: {ForeignNode[/postprocessor/Tile.../postprocessor/GatherElements]} Host Persistent: 80 bytes Device Persistent: 0 bytes Scratch Memory: 54067200 bytes[0m
[38;5;104m[X] Skipped printing memory information for 17 layers with 0 memory size i.e. Host Persistent + Device Persistent + Scratch Memory == 0.[0m
[38;5;13m[V] Total Host Persistent Memory: 307312 bytes[0m
[38;5;13m[V] Total Device Persistent Memory: 0 bytes[0m
[38;5;13m[V] Max Scratch Memory: 54067200 bytes[0m
[38;5;13m[V] [BlockAssignment] Started assigning block shifts. This will take 82 steps to complete.[0m
[38;5;104m[X] STILL ALIVE: Started step 76 of 82[0m
[38;5;13m[V] [BlockAssignment] Algorithm ShiftNTopDown took 1.25997ms to assign 6 blocks to 82 nodes requiring 63129600 bytes.[0m
[38;5;104m[X] Total number of blocks in optimized block assignment: 6[0m
[38;5;13m[V] Total Activation Memory: 63129600 bytes[0m
[38;5;13m[V] Total Weights Memory: 23931268 bytes[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv Set kernel index: 0[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv Set kernel index: 1[0m
[38;5;104m[X] Finalize: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: /model/backbone/MaxPool Set kernel index: 3[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv Set kernel index: 4[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv Set kernel index: 2[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu Set kernel index: 6[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool Set kernel index: 7[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv Set kernel index: 8[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv Set kernel index: 9[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv Set kernel index: 10[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu Set kernel index: 11[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool Set kernel index: 7[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv Set kernel index: 12[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv Set kernel index: 9[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv Set kernel index: 12[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu Set kernel index: 9[0m
[38;5;104m[X] Finalize: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool Set kernel index: 7[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv Set kernel index: 13[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv Set kernel index: 14[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu Set kernel index: 15[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv Set kernel index: 16[0m
[38;5;104m[X] Finalize: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu Set kernel index: 14[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv Set kernel index: 18[0m
[38;5;104m[X] Finalize: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv Set kernel index: 18[0m
[38;5;104m[X] Finalize: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Finalize: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul) Set kernel index: 19[0m
[38;5;104m[X] Finalize: /model/encoder/Resize Set kernel index: 20[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv Set kernel index: 21[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul) Set kernel index: 22[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 14[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul) Set kernel index: 25[0m
[38;5;104m[X] Finalize: /model/encoder/Resize_1 Set kernel index: 20[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv Set kernel index: 26[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul) Set kernel index: 27[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 28[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 28[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 9[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)) Set kernel index: 29[0m
[38;5;104m[X] Finalize: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul) Set kernel index: 30[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv Set kernel index: 31[0m
[38;5;104m[X] Finalize: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul) Set kernel index: 32[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv Set kernel index: 21[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul) Set kernel index: 22[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 23[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 14[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul) Set kernel index: 25[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv Set kernel index: 31[0m
[38;5;104m[X] Finalize: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv Set kernel index: 5[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul) Set kernel index: 34[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul) Set kernel index: 33[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv Set kernel index: 35[0m
[38;5;104m[X] Finalize: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)) Set kernel index: 24[0m
[38;5;104m[X] Finalize: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul) Set kernel index: 34[0m
[38;5;104m[X] Finalize: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv Set kernel index: 17[0m
[38;5;104m[X] Total number of generated kernels selected for the engine: 36[0m
[38;5;104m[X] Kernel: 0 CASK_STATIC[0m
[38;5;104m[X] Kernel: 1 CASK_STATIC[0m
[38;5;104m[X] Kernel: 2 CASK_STATIC[0m
[38;5;104m[X] Kernel: 3 CASK_STATIC[0m
[38;5;104m[X] Kernel: 4 CASK_STATIC[0m
[38;5;104m[X] Kernel: 5 CASK_STATIC[0m
[38;5;104m[X] Kernel: 6 CASK_STATIC[0m
[38;5;104m[X] Kernel: 7 CASK_STATIC[0m
[38;5;104m[X] Kernel: 8 CASK_STATIC[0m
[38;5;104m[X] Kernel: 9 CASK_STATIC[0m
[38;5;104m[X] Kernel: 10 CASK_STATIC[0m
[38;5;104m[X] Kernel: 11 CASK_STATIC[0m
[38;5;104m[X] Kernel: 12 CASK_STATIC[0m
[38;5;104m[X] Kernel: 13 CASK_STATIC[0m
[38;5;104m[X] Kernel: 14 CASK_STATIC[0m
[38;5;104m[X] Kernel: 15 CASK_STATIC[0m
[38;5;104m[X] Kernel: 16 CASK_STATIC[0m
[38;5;104m[X] Kernel: 17 CASK_STATIC[0m
[38;5;104m[X] Kernel: 18 CASK_STATIC[0m
[38;5;104m[X] Kernel: 19 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 20 TRT_SERIALIZABLE:ResizeVectorizedC4x4NearestKernel[0m
[38;5;104m[X] Kernel: 21 CASK_STATIC[0m
[38;5;104m[X] Kernel: 22 CASK_STATIC[0m
[38;5;104m[X] Kernel: 23 CASK_STATIC[0m
[38;5;104m[X] Kernel: 24 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 25 CASK_STATIC[0m
[38;5;104m[X] Kernel: 26 CASK_STATIC[0m
[38;5;104m[X] Kernel: 27 CASK_STATIC[0m
[38;5;104m[X] Kernel: 28 CASK_STATIC[0m
[38;5;104m[X] Kernel: 29 TRT_SERIALIZABLE:generatedNativePointwise[0m
[38;5;104m[X] Kernel: 30 CASK_STATIC[0m
[38;5;104m[X] Kernel: 31 CASK_STATIC[0m
[38;5;104m[X] Kernel: 32 CASK_STATIC[0m
[38;5;104m[X] Kernel: 33 CASK_STATIC[0m
[38;5;104m[X] Kernel: 34 CASK_STATIC[0m
[38;5;104m[X] Kernel: 35 CASK_STATIC[0m
[38;5;13m[V] Compiler backend is used during engine execution.[0m
[38;5;104m[X] Disabling unused tactic source: JIT_CONVOLUTIONS[0m
[38;5;13m[V] Engine generation completed in 32.2105 seconds.[0m
[38;5;104m[X] Layers:
    Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: images, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.conv1.conv1_1.conv.weight + /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,3,640,640], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 864}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_first_layer_i8i8_i8i32_f32_nchw_vect_c_4kcrs_vect_c_4_nchw_vect_c_32_tilesize8x16x32x32_stage1_warpsize4x1x1_tensor16x8x16_r3s3_u2v2_aligna4_alignc8, TacticValue: 0x5cc792a989a1d1a6, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_1/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_1/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_1/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_2.conv.weight + /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 32, Groups: 1, Weights: {"Type": "Int8", "Count": 9216}, Bias: {"Type": "Float", "Count": 32}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm75_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage1_warpsize2x1x1_g1_tensor8x8x16_t1r3s3, TacticValue: 0x13463e9bf9ae0d73, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_2/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_2/act/Relu]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_2/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.conv1.conv1_3.conv.weight + /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear + /model/backbone/conv1/conv1_3/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/conv1/conv1_3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,32,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 18432}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/Conv]
    [ONNX Layer: /model/backbone/conv1/conv1_3/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/conv1/conv1_3/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/conv1/conv1_3/conv/weight_quantizer/DequantizeLinear]
    Name: /model/backbone/MaxPool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/MaxPool_output_0, Location: Device, Dimensions: [1,64,320,320], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: MAX, WindowSize: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 1, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kMAX, TacticValue: 0x94215b398b8eb3ba, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/MaxPool]
    Name: model.backbone.res_layers.0.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xb936321f82fd390c, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.0.short.conv.weight + /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.0/short/conv/Conv + /model/backbone/res_layers.0/blocks.0/Add + /model/backbone/res_layers.0/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 4096}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/short/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x256x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x9dafb2758560cc1d, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.0.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.0/blocks.1/Add + /model/backbone/res_layers.0/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.0/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 64, Groups: 1, Weights: {"Type": "Int8", "Count": 36864}, Bias: {"Type": "Float", "Count": 64}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize256x64x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x0e07dc8353bf7e9f, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.0/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.0/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,160,160], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 73728}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x705baf38e41eee0b, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.1/blocks.0/Add + /model/backbone/res_layers.1/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,64,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 8192}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x128x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x214f03e23f252333, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.1.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.1/blocks.1/Add + /model/backbone/res_layers.1/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.1/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xad886d4d69834922, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.1/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.1/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 294912}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.2/blocks.0/Add + /model/backbone/res_layers.2/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xbb88763c3b0e94d4, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.2.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.2/blocks.1/Add + /model/backbone/res_layers.2/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.2/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.1/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool, LayerType: CaskPooling, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Pooling, PoolingType: AVERAGE, WindowSize: [2,2], PaddingMode: kEXPLICIT_ROUND_UP, PrePadding: [0,0], PostPadding: [0,0], Stride: [2,2], BlendFactor: 0, AverageCountExcludesPadding: 0, TacticName: sm72_xmma_pooling_IMMA_NCxHW32_generic_kAVERAGE, TacticValue: 0xd9375d43b61ffbcb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/pool/AveragePool]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 1179648}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_indexed_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0x322f337abc345152, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2b/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.0.short.conv.conv.weight + /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv + /model/backbone/res_layers.3/blocks.0/Add + /model/backbone/res_layers.3/blocks.0/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/branch2b/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x65fbe45b4cb1d8a5, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/short/conv/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/act/Relu]
    Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2a.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_avdt_dense_int8int8_tilesize64x64x32_tapsperload3_threadspercta128_r3s3_u1v1_scalebias_relu, TacticValue: 0x1d53511430a5d47e, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/act/Relu]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2a/conv/weight_quantizer/DequantizeLinear]
    Name: model.backbone.res_layers.3.blocks.1.branch2b.conv.weight + /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear + /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv + /model/backbone/res_layers.3/blocks.1/Add + /model/backbone/res_layers.3/blocks.1/act/Relu, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }, { Name: /model/backbone/res_layers.3/blocks.0/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 512, Groups: 1, Weights: {"Type": "Int8", "Count": 2359296}, Bias: {"Type": "Float", "Count": 512}, HasBias: 1, HasReLU: 1, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 1, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: RELU, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/Conv]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/norm/BatchNormalization]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/branch2b/conv/weight_quantizer/DequantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/Add]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.1/act/Relu]
    Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.1/act/Relu_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003ea, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.input_proj.2.conv.weight + /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 131072}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye9020_0_myl37_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddResTra_myl37_1, LayerType: kgen, Inputs: [ { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/encoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/encoder/input_proj_2/norm/BatchNormalization/model/encoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_MulAddResTra_0x862813689358e08ec79eab32f31fafdf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/encoder/Reshape][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_1]
    Name: __mye8937_myl37_2, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_TraAdd_myl37_3, LayerType: kgen, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Constant_output_0_constantFloat, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_3, Dimensions: [1,256,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_TraAdd_0x5a9388c92c5b2a167638420a28fa3cf0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Transpose][ONNX Layer: /model/encoder/encoder.0/layers.0/Add]
    Name: __mye8939_myl37_4, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_2_myl37_5, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_1_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8387_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8277/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8278/model/encoder/encoder_0/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8626_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_2][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_2]
    Name: __mye8941_myl37_6, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_1+/model/encoder/encoder_0/layers_0/self_attn/MatMul_myl37_7, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/Add_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8849dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye8319/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8320/model/encoder/encoder_0/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8774_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye8684, Dimensions: [2,400,256], Format/Datatype: Float }], TacticName: sm86_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Add]
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_3_myl37_8, LayerType: gemm, Inputs: [ { Name: __mye8684, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8684, Dimensions: [8,32,400], Format/Datatype: Float }, { Name: __mye8642, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye8333/model/encoder/encoder_0/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x128x16_stage4_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_3][ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl37_9, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_8, Dimensions: [8,400,400], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x486901888507314d28178a529899ff30, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Softmax]
    Name: __mye8943_myl37_10, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_myl37_11, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/MatMul_4_output_0'.1_9, Dimensions: [8,400,400], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,400,32], Format/Datatype: Float }, { Name: __mye8343/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8344/model/encoder/encoder_0/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl37_12, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [8,400,32], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0x053154cc4b930530fcf23b0caf04c63a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Reshape_3]
    Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_myl37_13, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Transpose_5 _ /model/encoder/encoder_0/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [400,256], Format/Datatype: Float }, { Name: __mye8407_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye8357/model/encoder/encoder_0/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye8358/model/encoder/encoder_0/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_116_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl37_14, LayerType: kgen, Inputs: [ { Name: __mye8585_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8575_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye9016_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: /model/encoder/encoder_0/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x81c6f38dc18b20647aef42cb9b16a94b, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/Add_1][ONNX Layer: /model/encoder/encoder.0/layers.0/norm1/LayerNormalization][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/encoder/encoder_0/layers_0/linear1/MatMul_myl37_15, LayerType: gemm, Inputs: [ { Name: /model/encoder/encoder_0/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,400,256], Format/Datatype: Int8 }, { Name: __mye8854dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye8646_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye8653zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear1_bias _ ONNXTRT_Broadcast_131_constantFloat, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_gelu_erf, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul_1][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Mul][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Div][ONNX Layer: /model/encoder/encoder.0/layers.0/activation/Erf][ONNX Layer: /model/encoder/encoder.0/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl37_16, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,400,1024], Format/Datatype: Int8 }, { Name: __mye8859dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye8657_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8664zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_encoder_encoder_0_layers_0_linear2_bias _ ONNXTRT_Broadcast_145_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/MatMul][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/encoder/encoder.0/layers.0/linear2/Add][ONNX Layer: /model/encoder/encoder.0/layers.0/Add_2]
    Name: __myl_ResMeaSubMulMea_myl37_17, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMea_0xade3a566ff3432c2f2753f66a7f593a6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization]
    Name: __myl_AddSqrDivMulMulAddResTra_myl37_18, LayerType: kgen, Inputs: [ { Name: __mye8539_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye8549_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_17, Dimensions: [400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_18, Dimensions: [400,1], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Reshape_1_output_0, Dimensions: [1,256,400], Format/Datatype: Float }], TacticName: __myl_AddSqrDivMulMulAddResTra_0x0caebe133d43683f5670c896620d9227, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/encoder.0/layers.0/norm2/LayerNormalization][ONNX Layer: /model/encoder/Transpose_1]
    [ONNX Layer: /model/encoder/Reshape_1]
    Name: model.encoder.input_proj.0.conv.weight + /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x483ad1560c6e5e27, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.2/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.input_proj.1.conv.weight + /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize96x64x64_stage3_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x483ad1560c6e5e27, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/input_proj.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/backbone/res_layers.3/blocks.0/branch2a/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Reshape_1_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.lateral_convs.0.conv.weight + /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(/model/encoder/lateral_convs.0/act/Sigmoid, /model/encoder/lateral_convs.0/act/Mul), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/lateral_convs.0/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 1, InputArgs: ["arg0"], NbOutputVars: 1, OutputVars: ["var4"], NbParams: 0, Params: [], NbLiterals: 5, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 5, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);"], TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.0/act/Mul]
    Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_clone_0, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x00000000000003e8, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    Name: /model/encoder/Resize, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize]
    Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_2_/model/encoder/Resize_output_0_clone_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_2]
    Name: model.encoder.fpn_blocks.0.conv2.conv.weight + /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv1.conv.weight + /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv1/act/Sigmoid, /model/encoder/fpn_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x709ddd0e503c7fd7, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.0.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.0/conv2/act/Sigmoid, /model/encoder/fpn_blocks.0/conv2/act/Mul), /model/encoder/fpn_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.0/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.0.conv3.conv.weight + /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.0/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.0/conv3/act/Sigmoid, /model/encoder/fpn_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.lateral_convs.1.conv.weight + /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/lateral_convs.1/conv/Conv + PWN(/model/encoder/lateral_convs.1/act/Sigmoid, /model/encoder/lateral_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/lateral_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/lateral_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/lateral_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/lateral_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1, LayerType: Resize, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Resize, InterpolationMode: NEAREST, ResizeScales: [1, 1, 2, 2, 0, 0, 0, 0], ExcludeOutside: 0, CubicCoeff: -0.75, CoordTransform: kASYMMETRIC, ResizeSelector: kFORMULA, NNRounding: kFLOOR, TacticValue: 0x0000000000000005, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Resize_1]
    Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Concat_3_/model/encoder/Resize_1_output_0_clone_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_3]
    Name: model.encoder.fpn_blocks.1.conv2.conv.weight + /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32, TacticValue: 0xe742f4598442d2f1, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv1.conv.weight + /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv1/act/Sigmoid, /model/encoder/fpn_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x458f02d2b10db57c, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.0.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.1.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xfdf7509af98902e0, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.fpn_blocks.1.bottlenecks.2.conv.weight + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x2d8ab2aa0639fda9, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/fpn_blocks.1/conv2/act/Sigmoid, /model/encoder/fpn_blocks.1/conv2/act/Mul), /model/encoder/fpn_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }, { Name: /model/encoder/fpn_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x000000000000001a, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/fpn_blocks.1/Add]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.fpn_blocks.1.conv3.conv.weight + /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/fpn_blocks.1/conv3/conv/Conv + PWN(/model/encoder/fpn_blocks.1/conv3/act/Sigmoid, /model/encoder/fpn_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize128x128x64_stage4_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x65a38dbc9e991257, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/fpn_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.0.conv.weight + /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.0/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.0.conv.weight + /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.0/conv/Conv + PWN(/model/encoder/downsample_convs.0/act/Sigmoid, /model/encoder/downsample_convs.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,80,80], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc722efd60bc6ea84, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.0/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.0/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/Resize_1_output_0 copy, LayerType: Reformat, Inputs: [ { Name: /model/encoder/Resize_1_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: CONCAT, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/Concat_4]
    Name: model.encoder.pan_blocks.0.conv2.conv.weight + /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x9ec201b34455146e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.conv1.conv.weight + /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv1/act/Sigmoid, /model/encoder/pan_blocks.0/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x709ddd0e503c7fd7, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0x6fd15a9d85252b17, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.0.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0x45f7566cdb2b10fb, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.0/conv2/act/Sigmoid, /model/encoder/pan_blocks.0/conv2/act/Mul), /model/encoder/pan_blocks.0/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.0/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.0/Add]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.0.conv3.conv.weight + /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.0/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.0/conv3/act/Sigmoid, /model/encoder/pan_blocks.0/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0x2eba0b6a8ec55fa3, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/act/Mul]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.0/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.1.conv.weight + /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.1/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize128x32x64_stage4_warpsize4x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x733ba2a91a48d431, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.downsample_convs.1.conv.weight + /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/downsample_convs.1/conv/Conv + PWN(/model/encoder/downsample_convs.1/act/Sigmoid, /model/encoder/downsample_convs.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/downsample_convs.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,40,40], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [2,2], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 589824}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/Conv]
    [ONNX Layer: /model/encoder/downsample_convs.1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/downsample_convs.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/downsample_convs.1/conv/weight_quantizer/DequantizeLinear]
    Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_clone_1, LayerType: Reformat, Inputs: [ { Name: /model/encoder/lateral_convs.0/act/Mul_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Reformat, Origin: QDQ, TacticValue: 0x0000000000000000, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv2.conv.weight + /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, TacticValue: 0x6d377e4222886190, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.conv1.conv.weight + /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv1/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv1/act/Sigmoid, /model/encoder/pan_blocks.1/conv1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,512,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.0.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.0/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.1.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv + PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3_swish, TacticValue: 0xc985777c89c6b3a4, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.1/conv/weight_quantizer/DequantizeLinear]
    Name: model.encoder.pan_blocks.1.bottlenecks.2.conv.weight + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [3,3], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [1,1], PostPadding: [1,1], Stride: [1,1], Dilation: [1,1], OutMaps: 128, Groups: 1, Weights: {"Type": "Int8", "Count": 147456}, Bias: {"Type": "Float", "Count": 128}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r3s3, TacticValue: 0xd14bd6d95fefd45e, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/weight_quantizer/DequantizeLinear]
    Name: PWN(PWN(/model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid, /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul), PWN(PWN(/model/encoder/pan_blocks.1/conv2/act/Sigmoid, /model/encoder/pan_blocks.1/conv2/act/Mul), /model/encoder/pan_blocks.1/Add)), LayerType: PointWiseV2, Inputs: [ { Name: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/conv/Conv_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }, { Name: /model/encoder/pan_blocks.1/conv2/norm/BatchNormalization_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Float }], Outputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], ParameterType: PointWise, ParameterSubType: PointWiseExpression, NbInputArgs: 2, InputArgs: ["arg0", "arg1"], NbOutputVars: 1, OutputVars: ["var10"], NbParams: 0, Params: [], NbLiterals: 10, Literals: ["0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f", "0.000000e+00f", "1.000000e+00f", "0.000000e+00f", "0.000000e+00f", "5.000000e-01f"], NbOperations: 11, Operations: ["auto const var0 = pwgen::iMul(literal4, arg0);", "auto const var1 = pwgen::iTanh(var0);", "auto const var2 = pwgen::iMul(var1, literal4);", "auto const var3 = pwgen::iPlus(var2, literal4);", "auto const var4 = pwgen::iMul(arg0, var3);", "auto const var5 = pwgen::iMul(literal9, arg1);", "auto const var6 = pwgen::iTanh(var5);", "auto const var7 = pwgen::iMul(var6, literal9);", "auto const var8 = pwgen::iPlus(var7, literal9);", "auto const var9 = pwgen::iMul(arg1, var8);", "auto const var10 = pwgen::iPlus(var4, var9);"], TacticValue: 0x0000000000000018, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/bottlenecks/bottlenecks.2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv2/act/Mul]
    [ONNX Layer: /model/encoder/pan_blocks.1/Add]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear]
    Name: model.encoder.pan_blocks.1.conv3.conv.weight + /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear + /model/encoder/pan_blocks.1/conv3/conv/Conv + PWN(/model/encoder/pan_blocks.1/conv3/act/Sigmoid, /model/encoder/pan_blocks.1/conv3/act/Mul), LayerType: CaskConvolution, Inputs: [ { Name: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,128,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 32768}, Bias: {"Type": "Float", "Count": 256}, HasBias: 1, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: SWISH, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1_swish, TacticValue: 0xc6cdb1e47323bb01, StreamId: 0, Metadata: [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/Conv]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/norm/BatchNormalization]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Sigmoid]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/act/Mul]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/encoder/pan_blocks.1/conv3/conv/weight_quantizer/DequantizeLinear]
    Name: model.decoder.input_proj.2.conv.weight + /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear + /model/decoder/input_proj.2/conv/Conv, LayerType: CaskConvolution, Inputs: [ { Name: /model/decoder/input_proj.2/conv/input_quantizer/QuantizeLinear_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Location: Device, Dimensions: [1,256,20,20], Format/Datatype: Float }], ParameterType: Convolution, Kernel: [1,1], PaddingMode: kEXPLICIT_ROUND_DOWN, PrePadding: [0,0], PostPadding: [0,0], Stride: [1,1], Dilation: [1,1], OutMaps: 256, Groups: 1, Weights: {"Type": "Int8", "Count": 65536}, Bias: {"Type": "Float", "Count": 0}, HasBias: 0, HasReLU: 0, HasSparseWeights: 0, HasDynamicFilter: 0, HasDynamicBias: 0, HasResidual: 0, ConvXAsActInputIdx: -1, BiasAsActInputIdx: -1, ResAsActInputIdx: -1, Activation: NONE, TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8f32_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_tilesize64x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_t1r1s1_alignc4, TacticValue: 0x5e4f6d7c83746fd6, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/QuantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/Conv]
    [ONNX Layer: /model/decoder/input_proj.2/conv/input_quantizer/DequantizeLinear]
    [ONNX Layer: /model/decoder/input_proj.2/conv/weight_quantizer/DequantizeLinear]
    Name: dummy_shape_call__mye158104_0_myl84_0, LayerType: shape_call, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^signal^1_myl84_1, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: entry^bb^wait^1_myl84_2, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl84_3, LayerType: kgen, Inputs: [ { Name: __mye155645_dconst, Dimensions: [1,1,6400], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.0/conv/Conv_output_0, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: /model/decoder/input_proj_0/norm/BatchNormalization/model/decoder/input_proj_0/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xb7911a963641d99b9b7644b75b6b02a0, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.0/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulMinMaxRouCasResTra_myl84_4, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_5, Dimensions: [1,256,80,80], Format/Datatype: Float }, { Name: __mye158004_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x53ec280dcdcbc7be42089db5a99e26ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape]
    [ONNX Layer: /model/decoder/Transpose]
    Name: __myl_MulAddResMulMinMaxRouCasTra_myl84_5, LayerType: kgen, Inputs: [ { Name: __mye155668_dconst, Dimensions: [1,1,1600], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.1/conv/Conv_output_0, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: /model/decoder/input_proj_1/norm/BatchNormalization/model/decoder/input_proj_1/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }], TacticName: __myl_MulAddResMulMinMaxRouCasTra_0xc7826108fa2ff5e34bf8bfa07dbc52f7, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/input_proj.1/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __myl_MulMinMaxRouCasResTra_myl84_6, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_8, Dimensions: [1,256,40,40], Format/Datatype: Float }, { Name: __mye158004_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }], TacticName: __myl_MulMinMaxRouCasResTra_0x8592f20b4eb6c9ee9a9e56f44ec5871e, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Reshape_1]
    [ONNX Layer: /model/decoder/Transpose_1]
    Name: __mye157470_myl84_7, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __mye157472_myl84_8, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_myl84_9, LayerType: kgen, Inputs: [ { Name: __mye158004_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_7, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_10, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: __mye155691_dconst, Dimensions: [1,1,400], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_6, Dimensions: [1,6400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_9, Dimensions: [1,1600,256], Format/Datatype: Int8 }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_shift_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }, { Name: /model/decoder/input_proj.2/conv/Conv_output_0, Dimensions: [1,256,20,20], Format/Datatype: Float }, { Name: /model/decoder/input_proj_2/norm/BatchNormalization/model/decoder/input_proj_2/norm/BatchNormalization_scale_wFloat, Dimensions: [1,256,1,1], Format/Datatype: Float }], Outputs: [ { Name: __mye154083_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MulAddMulMinMaxRouCasResResTraMulMinMaxRouCasTraConCon_0x14d97ab92d57b85a1bd3815e99f6e152, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/input_proj.2/norm/BatchNormalization][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/Concat_3][ONNX Layer: /model/decoder/Reshape_2]
    [ONNX Layer: /model/decoder/Transpose_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/QuantizeLinear]
    Name: __mye157474_myl84_10, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157476_myl84_11, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_1/cross_attn/value_proj/MatMul+/model/decoder/decoder/layers_0/cross_attn/value_proj/MatMul_myl84_12, LayerType: gemm, Inputs: [ { Name: __mye154083_12, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156340dconst, Dimensions: [3,256,256], Format/Datatype: Int8 }, { Name: __mye154107_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye154128_dconst, Dimensions: [3,1,256], Format/Datatype: Float }, { Name: __mye155126_dconst, Dimensions: [3,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye154083, Dimensions: [3,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/value_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/value_proj/Add]
    Name: /model/decoder/enc_output/proj/MatMul_myl84_13, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_11, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156345dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153194_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153201zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_proj_bias _ ONNXTRT_Broadcast_275_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/proj/MatMul][ONNX Layer: /model/decoder/enc_output/proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_output/proj/Add]
    Name: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_14, LayerType: kgen, Inputs: [ { Name: model_decoder_enc_output_norm_weight _ ONNXTRT_Broadcast_279_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: model_decoder_enc_output_norm_bias _ ONNXTRT_Broadcast_281_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }, { Name: __mye158014_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_14, Dimensions: [1,8400,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: __myl_MeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0xf1c80ff651c1b506b1815818d6281ad3, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_output/norm/LayerNormalization][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/QuantizeLinear]
    Name: __mye157478_myl84_15, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157480_myl84_16, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_score_head/MatMul_myl84_17, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156355dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153232_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153239zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_enc_score_head_bias _ ONNXTRT_Broadcast_289_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize128x128x64_stage3_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/enc_score_head/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_score_head/Add]
    Name: __myl_Max_myl84_18, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_17, Dimensions: [1,8400,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400,1], Format/Datatype: Float }], TacticName: __myl_Max_0x4330a02939b906fc5f8c1bd769456467, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/ReduceMax]
    Name: __myl_Top_myl84_19, LayerType: kgen, Inputs: [ { Name: /model/decoder/ReduceMax_output_0'_unsqueezed0.1, Dimensions: [1,8400], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/TopK_output_0'.1, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x7e62297dffa2e596ee60049838a70f81, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/TopK]
    Name: __mye157482_myl84_20, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/enc_bbox_head/layers_0/MatMul_myl84_21, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_15, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157544_xformed___mye156350dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157552_xformed___mye153216_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153212zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157548_xformed___mye153225_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.0/MatMul][ONNX Layer: /model/decoder/enc_score_head/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.0/Add]
    Name: /model/decoder/enc_bbox_head/layers_1/MatMul_myl84_22, LayerType: gemm, Inputs: [ { Name: /model/decoder/enc_bbox_head/layers_2/input_quantizer/QuantizeLinear_output_0'.1_21, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye157556_xformed___mye156360dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye157564_xformed___mye153254_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153250zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye157560_xformed___mye153263_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_conv_fprop_smallk_i8i8_i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_warptilesize32x32_stage4_warpsize2x4x1_r1s1_u1v1_hw1_c256_scalebias_relu, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.1/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/act_1/Relu][ONNX Layer: /model/decoder/enc_bbox_head/layers.1/Add]
    Name: __myl_FcAdd_myl84_23, LayerType: fusion, Inputs: [ { Name: model_decoder_anchors_constantFloat, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_22, Dimensions: [1,8400,256], Format/Datatype: Int8 }, { Name: __mye156370dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153270_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153277zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_enc_bbox_head_layers_2_bias _ ONNXTRT_Broadcast_311_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize64x64x64_stage4_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/enc_bbox_head/layers.2/MatMul][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/enc_bbox_head/layers.2/Add][ONNX Layer: /model/decoder/Add]
    Name: __mye157484_myl84_24, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_myl84_25, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_23, Dimensions: [1,8400,4], Format/Datatype: Float }, { Name: __mye158018_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_20, Dimensions: [1,300], Format/Datatype: Int32 }], Outputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,4], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], TacticName: __myl_CasResCasRepGatResNegExpAddDivMulMinMaxRouCas_0xea994e8a02766a6b87cc77a0ab1bb663, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/Unsqueeze][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Sigmoid][ONNX Layer: /model/decoder/GatherElements]
    Name: __myl_MovCon_myl84_26, LayerType: kgen, Inputs: [ { Name: __mye156638, Dimensions: [1,300,12], Format/Datatype: Int8 }, { Name: /model/decoder/decoder/query_pos_head/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,4], Format/Datatype: Int8 }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MovCon_0x9482c2d60923b5d68d1030431d0b6d2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_0/MatMul_myl84_27, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/input_quantizer/QuantizeLinear_output_0'.1_27, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156652_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153292_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153288zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153301_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1/MatMul_myl84_28, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1_28, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156380dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153308_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153315zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye149975_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/Add]
    Name: __myl_RepGatResAdd_myl84_29, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_16, Dimensions: [1,8400,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_25, Dimensions: [1,300,1], Format/Datatype: Int32 }], Outputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_RepGatResAdd_0x3585782c9d9cf8f0d2b18744e46affde, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/GatherElements_1][ONNX Layer: /model/decoder/decoder/layers.0/Add]
    Name: __mye157486_myl84_30, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157488_myl84_31, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_2_myl84_32, LayerType: gemm, Inputs: [ { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156365dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149287/model/decoder/decoder/layers_0/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149288/model/decoder/decoder/layers_0/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye153089_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_2]
    Name: __mye157490_myl84_33, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_1+/model/decoder/decoder/layers_0/self_attn/MatMul_myl84_34, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156385dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149341/model/decoder/decoder/layers_0/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149342/model/decoder/decoder/layers_0/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155376_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye154068, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Add]
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_3_myl84_35, LayerType: gemm, Inputs: [ { Name: __mye154068, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye154068, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153149, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149376/model/decoder/decoder/layers_0/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_36, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_34, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Softmax]
    Name: __mye157492_myl84_37, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_myl84_38, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/MatMul_4_output_0'.1_35, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149386/model/decoder/decoder/layers_0/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149387/model/decoder/decoder/layers_0/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_39, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_36, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_0/self_attn/Gemm_myl84_40, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Transpose_5 _ /model/decoder/decoder/layers_0/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye149991_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149400/model/decoder/decoder/layers_0/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149401/model/decoder/decoder/layers_0/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_self_attn_out_proj_bias _ ONNXTRT_Broadcast_351_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/self_attn/Gemm]
    Name: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_41, LayerType: kgen, Inputs: [ { Name: __mye152985_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152975_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158022_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/GatherElements_1_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_AddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x91b2c7046943674462a660380f1917c4, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/Add_1][ONNX Layer: /model/decoder/decoder/layers.0/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/Add_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __myl_FcMulAdd_myl84_42, LayerType: fusion, Inputs: [ { Name: __mye154035_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: __mye154017_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156390dconst, Dimensions: [1,256,288], Format/Datatype: Int8 }, { Name: __mye153994_dconst, Dimensions: [1,1,288], Format/Datatype: Float }], Outputs: [ { Name: __mye154044mul_beta, Dimensions: [1,300,288], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/attention_weights/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/sampling_offsets/Add]
    Name: __mye157494_myl84_43, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157496_myl84_44, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_ResMaxSubExpSum_myl84_45, LayerType: kgen, Inputs: [ { Name: __mye154044mul_beta, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_42, Dimensions: [1,300,8,1], Format/Datatype: Float }], TacticName: __myl_ResMaxSubExpSum_0x7c7453772a39d1d1294358f10a1e770b, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax]
    Name: __mye157498_myl84_46, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_myl84_47, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye154083, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18222, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18237, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18252, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18267, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150470_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18446, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18461, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18476, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18491, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150480_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18670, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18685, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18700, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18715, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150490_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150465_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150475_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150485_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150579, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150575, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye149996_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }, { Name: __mye154044mul_beta, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_44, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_0x3718754d33237a7346a050fa256f2cc0, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Unsqueeze_8]
    Name: __mye157500_myl84_48, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_49, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_42, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye158036_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_45, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_44, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_46, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_43, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150954_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/Mul_8][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_50, LayerType: kgen, Inputs: [ { Name: __mye150954_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_48, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_51, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_40, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_48, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155546_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153341_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153348zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_575_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_49, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_52, LayerType: kgen, Inputs: [ { Name: __mye152940_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152930_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158040_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_49, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_51, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_0/linear1/MatMul_myl84_53, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157568_xformed___mye156395dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153363_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153359zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153372_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.0/linear1/Add]
    Name: __myl_FcAdd_myl84_54, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_51, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_52, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156400dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153379_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153386zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_0_linear2_bias _ ONNXTRT_Broadcast_597_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.0/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.0/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.0/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_55, LayerType: kgen, Inputs: [ { Name: __mye152904_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152890_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158044_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_53, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157502_myl84_56, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157504_myl84_57, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_2_myl84_58, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156410dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149485/model/decoder/decoder/layers_1/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149486/model/decoder/decoder/layers_1/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152875_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_2]
    Name: __mye157506_myl84_59, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/MatMul_myl84_60, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157572_xformed___mye156405dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153401_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153397zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153410_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_57, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_1/MatMul_myl84_61, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/input_quantizer/QuantizeLinear_output_0'.1_57, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157576_xformed___mye156415dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153428_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153424zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153437_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_58, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/MatMul_myl84_62, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_0/layers_2/Add_output_0'.1_58, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156420dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153444_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153451zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_0_layers_2_bias _ ONNXTRT_Broadcast_627_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_59, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.0/layers.2/Add]
    Name: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_myl84_63, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_26, Dimensions: [300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_59, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156659, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye158018_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_60, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinMaxSubMinMaxMinDivLogResAddNegExpAddDivMulMinMaxRouConCas_0xefac8e563c6580f9cd110df4750663ce, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log][ONNX Layer: /model/decoder/decoder/Sigmoid_1][ONNX Layer: /model/decoder/decoder/Add][ONNX Layer: /model/decoder/decoder/Div][ONNX Layer: /model/decoder/decoder/Sub][ONNX Layer: /model/decoder/decoder/Clip]
    Name: /model/decoder/decoder/query_pos_head/layers_0_1/MatMul_myl84_64, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_60, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156675_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153466_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153462zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153475_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_62, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_1/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_1/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_1/MatMul_myl84_65, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1_62, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156430dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153482_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153489zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150074_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_1/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_1/Add]
    Name: __myl_Add_myl84_66, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_1+/model/decoder/decoder/layers_1/self_attn/MatMul_myl84_67, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156435dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149550/model/decoder/decoder/layers_1/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149551/model/decoder/decoder/layers_1/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155386_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153981, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Add]
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_3_myl84_68, LayerType: gemm, Inputs: [ { Name: __mye153981, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153981, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153153, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149585/model/decoder/decoder/layers_1/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_66, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_69, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_66, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_66, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_67, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Softmax]
    Name: __mye157508_myl84_70, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_myl84_71, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/MatMul_4_output_0'.1_67, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149595/model/decoder/decoder/layers_1/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149596/model/decoder/decoder/layers_1/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_68, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_72, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_68, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_1/self_attn/Gemm_myl84_73, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Transpose_5 _ /model/decoder/decoder/layers_1/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150090_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149609/model/decoder/decoder/layers_1/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149610/model/decoder/decoder/layers_1/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_self_attn_out_proj_bias _ ONNXTRT_Broadcast_672_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_74, LayerType: kgen, Inputs: [ { Name: __mye152825_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152815_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158051_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_1/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_0/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_72, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.0/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_1][ONNX Layer: /model/decoder/decoder/layers.1/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/Add_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __myl_FcMulAdd_myl84_75, LayerType: fusion, Inputs: [ { Name: __mye153948_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: __mye153930_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156440dconst, Dimensions: [1,256,288], Format/Datatype: Int8 }, { Name: __mye153907_dconst, Dimensions: [1,1,288], Format/Datatype: Float }], Outputs: [ { Name: __mye153957mul_beta, Dimensions: [1,300,288], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/attention_weights/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/sampling_offsets/Add]
    Name: __mye157510_myl84_76, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157512_myl84_77, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_ResMaxSubExpSum_myl84_78, LayerType: kgen, Inputs: [ { Name: __mye153957mul_beta, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_75, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_74, Dimensions: [1,300,8,1], Format/Datatype: Float }], TacticName: __myl_ResMaxSubExpSum_0x7c7453772a39d1d1294358f10a1e770b, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax]
    Name: __mye157514_myl84_79, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_myl84_80, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye154083, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18921, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18936, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18951, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye18966, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150500_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19145, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19160, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19175, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19190, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150510_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19369, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19384, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19399, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19414, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150520_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150495_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150505_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150515_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150607, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150603, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150095_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }, { Name: __mye153957mul_beta, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_78, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_77, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_76, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_0xc0b46290445cedee2db7e5baf77a0f2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Unsqueeze_8]
    Name: __mye157516_myl84_81, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_82, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_74, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye158064_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_77, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_76, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_78, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_75, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150960_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_83, LayerType: kgen, Inputs: [ { Name: __mye150960_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_80, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_84, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_72, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_80, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155492_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153515_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153522zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_896_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_81, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_85, LayerType: kgen, Inputs: [ { Name: __mye152780_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152770_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158068_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_81, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_83, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_1/linear1/MatMul_myl84_86, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157580_xformed___mye156445dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153537_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153533zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153546_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_84, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.1/linear1/Add]
    Name: __myl_FcAdd_myl84_87, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_83, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_84, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156450dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153553_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153560zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_1_linear2_bias _ ONNXTRT_Broadcast_918_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.1/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.1/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.1/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_88, LayerType: kgen, Inputs: [ { Name: __mye152744_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152730_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158072_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_85, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x4e14cc44ca088d44748af6a96514ac7a, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157518_myl84_89, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157520_myl84_90, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_2_myl84_91, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156460dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149694/model/decoder/decoder/layers_2/self_attn/MatMul_2_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149695/model/decoder/decoder/layers_2/self_attn/MatMul_2_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152715_reshape, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_2][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_2]
    Name: __mye157522_myl84_92, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/MatMul_myl84_93, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157584_xformed___mye156455dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153575_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153571zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153584_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_89, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_1/MatMul_myl84_94, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/input_quantizer/QuantizeLinear_output_0'.1_89, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157588_xformed___mye156465dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153602_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153598zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153611_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_90, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/MatMul_myl84_95, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_1/layers_2/Add_output_0'.1_90, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156470dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153618_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153625zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_1_layers_2_bias _ ONNXTRT_Broadcast_948_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_91, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.1/layers.2/Add]
    Name: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_myl84_96, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_61, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_91, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye156682, Dimensions: [1,300,12], Format/Datatype: Float }, { Name: __mye158018_const-lit-in, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_92, Dimensions: [1,300,16], Format/Datatype: Int8 }], TacticName: __myl_MaxMinSubMaxMinMaxMinDivLogAddNegExpAddDivMulMinMaxRouConCas_0xa06819df43d11e9f71ec4d6314dfc9b2, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/Log_1][ONNX Layer: /model/decoder/decoder/Add_1][ONNX Layer: /model/decoder/decoder/Sigmoid_2][ONNX Layer: /model/decoder/decoder/Div_1][ONNX Layer: /model/decoder/decoder/Sub_1][ONNX Layer: /model/decoder/decoder/Clip_3]
    Name: /model/decoder/decoder/query_pos_head/layers_0_2/MatMul_myl84_97, LayerType: gemm, Inputs: [ { Name: __myln_k_arg__bb1_92, Dimensions: [1,300,16], Format/Datatype: Int8 }, { Name: __mye156698_dconst, Dimensions: [1,16,512], Format/Datatype: Int8 }, { Name: __mye153640_dconst, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153636zero_beta, Dimensions: [1,512], Format/Datatype: Float }, { Name: __mye153649_dconst, Dimensions: [1,1,512], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_94, Dimensions: [1,300,512], Format/Datatype: Int8 }], TacticName: sm80_xmma_gemm_i8i8_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/QuantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/act_2/Relu][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.0_2/Add]
    Name: /model/decoder/decoder/query_pos_head/layers_1_2/MatMul_myl84_98, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1_94, Dimensions: [1,300,512], Format/Datatype: Int8 }, { Name: __mye156480dconst, Dimensions: [1,512,256], Format/Datatype: Int8 }, { Name: __mye153656_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153663zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye150169_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/MatMul][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/input_quantizer_2/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/query_pos_head/layers.1_2/Add]
    Name: __myl_Add_myl84_99, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: __myl_Add_0xfcef7142c0478fafffb74a07ab8ea30f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_1+/model/decoder/decoder/layers_2/self_attn/MatMul_myl84_100, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye156485dconst, Dimensions: [2,256,256], Format/Datatype: Float }, { Name: __mye149759/model/decoder/decoder/layers_2/self_attn/MatMul_1_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149760/model/decoder/decoder/layers_2/self_attn/MatMul_1_beta, Dimensions: [1], Format/Datatype: Float }, { Name: __mye155396_dconst, Dimensions: [2,1,256], Format/Datatype: Float }], Outputs: [ { Name: __mye153894, Dimensions: [2,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize64x32x64_stage4_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Add]
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_3_myl84_101, LayerType: gemm, Inputs: [ { Name: __mye153894, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye153894, Dimensions: [8,32,300], Format/Datatype: Float }, { Name: __mye153157, Dimensions: [1,1,1], Format/Datatype: Float }, { Name: __mye149794/model/decoder/decoder/layers_2/self_attn/MatMul_3_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_98, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize128x64x16_stage6_warpsize2x2x1_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_3][ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Mul_1]
    Name: __myl_MaxSubExpSumDivMul_myl84_102, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_98, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_98, Dimensions: [8,300,300], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_99, Dimensions: [8,300,300], Format/Datatype: Float }], TacticName: __myl_MaxSubExpSumDivMul_0x4bb1dc97991e61c47e3d11f2b659751f, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Softmax]
    Name: __mye157524_myl84_103, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_myl84_104, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/MatMul_4_output_0'.1_99, Dimensions: [8,300,300], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Add_2_output_0'.1, Dimensions: [8,300,32], Format/Datatype: Float }, { Name: __mye149804/model/decoder/decoder/layers_2/self_attn/MatMul_4_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149805/model/decoder/decoder/layers_2/self_attn/MatMul_4_beta, Dimensions: [1], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,32], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_nn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/MatMul_4]
    Name: __myl_Tra_myl84_105, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_100, Dimensions: [8,300,32], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,8,32], Format/Datatype: Float }], TacticName: __myl_Tra_0xbff89681337b526d248c0838f5d94e94, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Transpose_5]
    [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Reshape_3]
    Name: /model/decoder/decoder/layers_2/self_attn/Gemm_myl84_106, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Transpose_5 _ /model/decoder/decoder/layers_2/self_attn/Reshape_3_first_transpose_output.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: __mye150185_dconst, Dimensions: [256,256], Format/Datatype: Float }, { Name: __mye149818/model/decoder/decoder/layers_2/self_attn/Gemm_alpha, Dimensions: [1], Format/Datatype: Float }, { Name: __mye149819/model/decoder/decoder/layers_2/self_attn/Gemm_beta, Dimensions: [1], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_self_attn_out_proj_bias _ ONNXTRT_Broadcast_993_constantFloat, Dimensions: [1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_f32f32_tf32f32_f32_tn_n_tilesize32x32x64_stage3_warpsize2x1x2_tensor16x8x8, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/self_attn/Gemm]
    Name: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_myl84_107, LayerType: kgen, Inputs: [ { Name: __mye152665_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152655_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158079_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: /model/decoder/decoder/query_pos_head/layers_1_2/Add_output_0'.1, Dimensions: [300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/self_attn/Gemm_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_1/norm3/LayerNormalization_normalizationBiased.1, Dimensions: [300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_104, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResAddResMeaSubMulMeaAddSqrDivMulMulAddResAddMulMinMaxRouCas_0x2ee8fbc8ddb7baf5b46cceba6a86227b, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.1/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_1][ONNX Layer: /model/decoder/decoder/layers.2/norm1/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/Add_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear]
    Name: __myl_FcMulAdd_myl84_108, LayerType: fusion, Inputs: [ { Name: __mye153861_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: __mye153843_dconst, Dimensions: [1,288], Format/Datatype: Float }, { Name: /model/decoder/decoder/layers_2/cross_attn/sampling_offsets/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156490dconst, Dimensions: [1,256,288], Format/Datatype: Int8 }, { Name: __mye153820_dconst, Dimensions: [1,1,288], Format/Datatype: Float }], Outputs: [ { Name: __mye153870mul_beta, Dimensions: [1,300,288], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/attention_weights/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/sampling_offsets/Add]
    Name: __mye157526_myl84_109, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157528_myl84_110, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_ResMaxSubExpSum_myl84_111, LayerType: kgen, Inputs: [ { Name: __mye153870mul_beta, Dimensions: [1,300,96], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_107, Dimensions: [1,300,8,12], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_106, Dimensions: [1,300,8,1], Format/Datatype: Float }], TacticName: __myl_ResMaxSubExpSum_0x7c7453772a39d1d1294358f10a1e770b, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax]
    Name: __mye157530_myl84_112, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_myl84_113, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye154083, Dimensions: [1,8400,8,32], Format/Datatype: Float }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19620, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19635, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19650, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19665, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150530_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19844, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19859, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19874, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye19889, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150540_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20068, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20083, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20098, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye20113, Dimensions: [1,1,1,2], Format/Datatype: Int32 }, { Name: __mye150550_dconst, Dimensions: [2], Format/Datatype: Int32 }, { Name: __mye150525_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150535_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150545_dconst, Dimensions: [2], Format/Datatype: Float }, { Name: __mye158026_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye150635, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150631, Dimensions: [1,1,1,1,1], Format/Datatype: Float }, { Name: __mye150190_dconst, Dimensions: [1,1,1,12,1], Format/Datatype: Float }, { Name: __mye153870mul_beta, Dimensions: [1,300,192], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_110, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_109, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_108, Dimensions: [8,32,300,4], Format/Datatype: Float }], TacticName: __myl_TraResSliResSliResSliResResSliSliResMulMulMulAddMulAddTraResSliRevTraAddMulAddMulFloCasSubEtc_0xc0b46290445cedee2db7e5baf77a0f2e, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Sub][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_1]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_3][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Add][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Split][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/GridSample_2][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_4][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_6][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_7][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_8][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Slice_1][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Unsqueeze_8]
    Name: __mye157532_myl84_114, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_DivMulTraResConMulSumMulMinMaxRouCas_myl84_115, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_106, Dimensions: [1,300,8,1], Format/Datatype: Float }, { Name: __mye158092_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_109, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_108, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_110, Dimensions: [8,32,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_107, Dimensions: [1,300,8,12], Format/Datatype: Float }], Outputs: [ { Name: __mye150966_q8, Dimensions: [8,32,300,1], Format/Datatype: Int8 }], TacticName: __myl_DivMulTraResConMulSumMulMinMaxRouCas_0xfdc36321684ce402a67e9cc028ee3fea, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Softmax][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Transpose_2]
    [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Reshape_9][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Concat_10][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/ReduceSum][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/Mul_5][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/QuantizeLinear]
    Name: __myl_Mov_myl84_116, LayerType: kgen, Inputs: [ { Name: __mye150966_q8, Dimensions: [1,300,256], Format/Datatype: Int8 }], Outputs: [ { Name: __myln_k_arg__bb1_112, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: __myl_Mov_0xccd11d8190e5ec819f0de6935e8e6ebe, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add]
    Name: __myl_FcAdd_myl84_117, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_104, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_112, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye155438_dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153689_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153696zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_cross_attn_output_proj_bias _ ONNXTRT_Broadcast_1217_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_113, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/cross_attn/output_proj/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_3]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_myl84_118, LayerType: kgen, Inputs: [ { Name: __mye152620_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye152610_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158096_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_113, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }, { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddResMulMinMaxRouCas_0x7374e3706e69002aff2a29c077287875, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm2/LayerNormalization][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/QuantizeLinear]
    Name: /model/decoder/decoder/layers_2/linear1/MatMul_myl84_119, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/layers_2/linear1/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157592_xformed___mye156495dconst, Dimensions: [1,256,1024], Format/Datatype: Int8 }, { Name: __mye153711_dconst, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153707zero_beta, Dimensions: [1,1024], Format/Datatype: Float }, { Name: __mye153720_dconst, Dimensions: [1,1,1024], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,1024], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize64x64x64_stage6_warpsize2x2x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear1/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/activation/Relu][ONNX Layer: /model/decoder/decoder/layers.2/linear1/Add]
    Name: __myl_FcAdd_myl84_120, LayerType: fusion, Inputs: [ { Name: __myln_k_arg__bb1_115, Dimensions: [1,300,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_116, Dimensions: [1,300,1024], Format/Datatype: Int8 }, { Name: __mye156500dconst, Dimensions: [1,1024,256], Format/Datatype: Int8 }, { Name: __mye153727_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153734zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: model_decoder_decoder_layers_2_linear2_bias _ ONNXTRT_Broadcast_1239_constantFloat, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_117, Dimensions: [1,300,256], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32_by_fusion_tactic, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/linear2/MatMul][ONNX Layer: /model/decoder/decoder/layers.2/linear2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/layers.2/linear2/Add][ONNX Layer: /model/decoder/decoder/layers.2/Add_4]
    Name: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_myl84_121, LayerType: kgen, Inputs: [ { Name: __mye152584_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye158100_const-lit-in, Dimensions: [1], Format/Datatype: Float }, { Name: __mye152578_reshape, Dimensions: [1,256], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_117, Dimensions: [1,300,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [300,256], Format/Datatype: Int8 }], TacticName: __myl_ResMeaSubMulMeaAddSqrDivMulMulAddMulMinMaxRouCas_0x3f53c92c8e85fb99f9934c06da28da1c, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/layers.2/norm3/LayerNormalization][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/QuantizeLinear]
    Name: __mye157534_myl84_122, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __mye157536_myl84_123, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_score_head_2/MatMul_myl84_124, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156505dconst, Dimensions: [1,256,80], Format/Datatype: Int8 }, { Name: __mye153738_dconst, Dimensions: [1,80], Format/Datatype: Float }, { Name: __mye153745zero_beta, Dimensions: [1,80], Format/Datatype: Float }, { Name: model_decoder_dec_score_head_2_bias _ ONNXTRT_Broadcast_1287_constantFloat, Dimensions: [1,1,80], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,300,80], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 1, Metadata: [ONNX Layer: /model/decoder/decoder/dec_score_head.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_score_head.2/Add]
    Name: __myl_GatResNegExpAddDivRes_myl84_125, LayerType: kgen, Inputs: [ { Name: /model/decoder/decoder/dec_score_head_2/Add_output_0'.1, Dimensions: [1,1,300,80], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_120, Dimensions: [1,24000], Format/Datatype: Float }], TacticName: __myl_GatResNegExpAddDivRes_0x1d563258c32f843400fb4233ccab3fa6, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/Sigmoid][ONNX Layer: /postprocessor/Flatten][ONNX Layer: /model/decoder/Gather_8]
    Name: __myl_Top_myl84_126, LayerType: kgen, Inputs: [ { Name: __myln_k_arg__bb1_120, Dimensions: [1,24000], Format/Datatype: Float }], Outputs: [ { Name: scores, Dimensions: [1,300], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_122, Dimensions: [1,300], Format/Datatype: Int32 }], TacticName: __myl_Top_0x1c85ccd1fad109f046189f0d3e8dff44, StreamId: 1, Metadata: [ONNX Layer: /postprocessor/TopK]
    Name: __mye157538_myl84_127, LayerType: signal, Inputs: [], Outputs: [], TacticName: , StreamId: 1, Metadata: 
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/MatMul_myl84_128, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_0/input_quantizer/QuantizeLinear_output_0'.1, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157596_xformed___mye156510dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153760_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153756zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153769_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_123, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.0/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_1/MatMul_myl84_129, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/input_quantizer/QuantizeLinear_output_0'.1_123, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye157600_xformed___mye156515dconst, Dimensions: [1,256,256], Format/Datatype: Int8 }, { Name: __mye153787_dconst, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153783zero_beta, Dimensions: [1,256], Format/Datatype: Float }, { Name: __mye153796_dconst, Dimensions: [1,1,256], Format/Datatype: Float }], Outputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_124, Dimensions: [1,300,256], Format/Datatype: Int8 }], TacticName: sm80_xmma_fprop_implicit_gemm_interleaved_i8i8_i8i32_f32_nchw_vect_c_32kcrs_vect_c_32_nchw_vect_c_32_tilesize32x32x64_stage6_warpsize2x1x1_g1_tensor16x8x32_simple_t1r1s1, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/QuantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/act_1/Relu][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.1/Add]
    Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/MatMul_myl84_130, LayerType: gemm, Inputs: [ { Name: /model/decoder/decoder/dec_bbox_head_2/layers_2/Add_output_0'.1_124, Dimensions: [1,300,256], Format/Datatype: Int8 }, { Name: __mye156520dconst, Dimensions: [1,256,4], Format/Datatype: Int8 }, { Name: __mye153803_dconst, Dimensions: [1,4], Format/Datatype: Float }, { Name: __mye153810zero_beta, Dimensions: [1,4], Format/Datatype: Float }, { Name: model_decoder_dec_bbox_head_2_layers_2_bias _ ONNXTRT_Broadcast_1269_constantFloat, Dimensions: [1,1,4], Format/Datatype: Float }], Outputs: [ { Name: __myln_k_arg__bb1_125, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: sm80_xmma_gemm_i8f32_i8i32_f32_tn_n_tilesize32x64x64_stage6_warpsize2x2x1_tensor16x8x32, StreamId: 0, Metadata: [ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/MatMul][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/input_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/weight_quantizer/DequantizeLinear][ONNX Layer: /model/decoder/decoder/dec_bbox_head.2/layers.2/Add]
    Name: __mye157540_myl84_131, LayerType: wait, Inputs: [], Outputs: [], TacticName: , StreamId: 0, Metadata: 
    Name: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_myl84_132, LayerType: kgen, Inputs: [ { Name: orig_target_sizes, Dimensions: [1,2], Format/Datatype: Int64 }, { Name: __myln_k_arg__bb1_122, Dimensions: [1,300], Format/Datatype: Int32 }, { Name: __mye150647, Dimensions: [1,1], Format/Datatype: Int64 }, { Name: __mye150651, Dimensions: [1,1], Format/Datatype: Float }, { Name: __mye150655, Dimensions: [1,1], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_125, Dimensions: [1,300,4], Format/Datatype: Float }, { Name: __myln_k_arg__bb1_93, Dimensions: [1,300,4], Format/Datatype: Float }], Outputs: [ { Name: labels, Dimensions: [1,300], Format/Datatype: Int64 }, { Name: boxes, Dimensions: [1,300,4], Format/Datatype: Float }], TacticName: __myl_RepResCasMaxMinSubMaxMinMaxMinDivLogCasDivResCasRepMulSubAddNegExpAddDivResGatSliResSliResEtc_0x046287ea34a14bdbbd780dcf069cdb4a, StreamId: 0, Metadata: [ONNX Layer: Cast_3039][ONNX Layer: /model/decoder/decoder/Clip_6][ONNX Layer: /model/decoder/decoder/Sub_2][ONNX Layer: /model/decoder/decoder/Div_2][ONNX Layer: /model/decoder/decoder/Sigmoid_3][ONNX Layer: /model/decoder/Gather_9][ONNX Layer: /model/decoder/decoder/Unsqueeze_3][ONNX Layer: /model/decoder/decoder/Add_2][ONNX Layer: /model/decoder/decoder/Log_2][ONNX Layer: /postprocessor/Split][ONNX Layer: /postprocessor/Squeeze_1][ONNX Layer: /postprocessor/Squeeze_2][ONNX Layer: /postprocessor/Mul][ONNX Layer: /postprocessor/Add][ONNX Layer: /postprocessor/Unsqueeze_2][ONNX Layer: /postprocessor/Sub][ONNX Layer: /postprocessor/Unsqueeze][ONNX Layer: /postprocessor/Concat][ONNX Layer: /postprocessor/Mul_2][ONNX Layer: /postprocessor/GatherElements][ONNX Layer: /postprocessor/Unsqueeze_5][ONNX Layer: /postprocessor/Unsqueeze_3][ONNX Layer: /postprocessor/Add_1][ONNX Layer: /postprocessor/Squeeze][ONNX Layer: /postprocessor/Unsqueeze_1][ONNX Layer: /postprocessor/Sub_1][ONNX Layer: /postprocessor/Mul_1][ONNX Layer: /postprocessor/Squeeze_3][ONNX Layer: /postprocessor/Mul_3][ONNX Layer: /postprocessor/Sub_2][ONNX Layer: /postprocessor/Div][ONNX Layer: /postprocessor/Unsqueeze_4][ONNX Layer: /postprocessor/Tile]
    
    Bindings:
    images
    orig_target_sizes
    labels
    boxes
    scores[0m
[38;5;13m[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 2 MiB, GPU 66 MiB[0m
[38;5;104m[X] Adding 1 engine(s) to plan file.[0m
[38;5;104m[X] Adding 1 engine weights(s) to plan file.[0m
[38;5;10m[I] Finished engine building in 32.402 seconds[0m
[38;5;13m[V] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchedNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::BatchTilePlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Clip_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CoordConvAC version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResizeDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::CropAndResize version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DecodeBbox3DPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::DetectionLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Explicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_Implicit_TF_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_ONNX_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::EfficientNMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::FlattenConcat_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GenerateDetection_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchor_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::GridAnchorRect_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::InstanceNormalization_TRT version 3[0m
[38;5;104m[X] Plugin creator already registered - ::LReLU_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ModulatedDeformConv2d version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelCropAndResize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultilevelProposeROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::MultiscaleDeformableAttnPlugin_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMSDynamic_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::NMS_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Normalize_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PillarScatterPlugin version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PriorBox_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalDynamic version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ProposalLayer_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Proposal version 1[0m
[38;5;104m[X] Plugin creator already registered - ::PyramidROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Region_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 2[0m
[38;5;104m[X] Plugin creator already registered - ::Reorg_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ResizeNearest_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ROIAlign_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::RPROI_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 1[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterElements version 2[0m
[38;5;104m[X] Plugin creator already registered - ::ScatterND version 1[0m
[38;5;104m[X] Plugin creator already registered - ::SpecialSlice_TRT version 1[0m
[38;5;104m[X] Plugin creator already registered - ::Split version 1[0m
[38;5;104m[X] Plugin creator already registered - ::VoxelGeneratorPlugin version 1[0m
[38;5;13m[V] Loaded engine size: 29 MiB[0m
[38;5;104m[X] Deserialization required 6934 microseconds.[0m
[38;5;104m[X] Adding 1 engine(s) to plan file.[0m
[38;5;104m[X] Adding 1 engine weights(s) to plan file.[0m
[I] Saving engine to default_mtq_int8_q_qint8baseline-output_modified.engine
[38;5;13m[V] [MS] Running engine with multi stream info[0m
[38;5;13m[V] [MS] Number of aux streams is 1[0m
[38;5;13m[V] [MS] Number of total worker streams is 2[0m
[38;5;13m[V] [MS] The main stream provided by execute/enqueue calls is the first worker stream[0m
[38;5;104m[X] Total per-runner device persistent memory is 0[0m
[38;5;104m[X] Total per-runner host persistent memory is 307312[0m
[38;5;104m[X] Allocated device scratch memory of size 63129600[0m
[38;5;104m[X] - Runner scratch: 63129600 bytes[0m
[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +61, now: CPU 0, GPU 83 (MiB)[0m
[38;5;104m[X] CUDA lazy loading is enabled.[0m
[38;5;13m[V] Found candidate CUDA libraries: ['/usr/local/cuda/lib64/libcudart.so.12.1.55', '/usr/local/cuda/lib64/libcudart.so.12', '/usr/local/cuda/lib64/libcudart.so'][0m
[38;5;13m[V] Loading inputs from data loader[0m
[38;5;13m[V] Loaded Module: numpy | Version: 1.25.2 | Path: ['/root/miniconda3/envs/rtdetr/lib/python3.10/site-packages/numpy'][0m
[38;5;13m[V] Loaded Module: torch | Version: 2.5.0+cu121 | Path: ['/root/miniconda3/envs/rtdetr/lib/python3.10/site-packages/torch'][0m
[38;5;11m[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. [0m
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] trt-runner-N0-05/19/25-15:03:57    
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[38;5;104m[X] trt-runner-N0-05/19/25-15:03:57     | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}[0m
[38;5;13m[V] trt-runner-N0-05/19/25-15:03:57     | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}[0m
[38;5;104m[X] Reallocated output tensor: labels to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: boxes to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)[0m
[38;5;104m[X] Reallocated output tensor: scores to: tensor([0, 0, 0,  ..., 0, 0, 0], device='cuda:0', dtype=torch.uint8)[0m
[I] trt-runner-N0-05/19/25-15:03:57    
    ---- Inference Output(s) ----
    {labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[38;5;104m[X] trt-runner-N0-05/19/25-15:03:57     | Inference Time: 44.976 ms | Received outputs:
        {'labels': tensor([[67, 67, 67,  0, 67, 67, 67,  0, 67, 67, 67, 67, 67,  0, 67,  8,  8,  8,
                  8,  0,  8,  8, 24, 67,  0, 67,  8,  8,  8,  8, 67,  8,  8, 24,  8, 24,
                  0,  8,  8, 67,  0,  0,  8,  8,  8,  0,  8,  8,  8,  0,  8,  8,  8,  0,
                 67, 24,  8,  8,  0, 13,  8,  8,  0, 67,  8,  8,  8,  0,  8,  0,  8,  8,
                 67,  8,  8,  8,  8,  8,  8,  8,  8,  0,  8,  0,  0, 67,  8, 26, 24, 24,
                  0,  8,  8,  8,  8,  8,  0,  8,  0,  0,  2,  8,  0,  8, 13, 67,  0,  8,
                  0,  0,  0,  0, 67, 67,  0,  0,  8, 24,  0,  0,  8, 26,  8,  8,  0,  8,
                 24,  8,  8,  8, 24,  8,  8,  8,  8,  8, 13,  0,  8,  8, 58,  0,  8,  8,
                  0,  0,  8,  8,  0,  0, 79, 74,  8, 24, 67, 13,  8,  8,  0,  0, 26,  8,
                  0, 27, 24,  8,  8,  0, 39,  0, 26, 13,  2,  2,  8,  8,  0,  0,  8, 24,
                  0,  8,  8, 74,  0,  2, 26,  0,  0,  0, 24,  0,  8,  8, 26, 58, 26,  0,
                  1,  0, 28,  0,  8, 26,  0, 13, 13, 13,  2,  0,  0,  8,  2,  2,  2, 24,
                  0, 39,  0, 13,  0, 24, 10,  9, 13,  0, 13, 13, 13,  0, 13,  0, 79,  0,
                 13,  8, 24,  8,  0,  8, 13,  0, 13, 73, 28, 13,  8, 26,  0,  8, 13,  0,
                  2,  2, 13,  8, 67,  0,  0, 24, 26, 52,  0, 26, 26, 13, 24, 13, 24,  0,
                 24, 13, 43, 13,  0, 24, 13,  9, 27, 79,  2,  0, 13, 39, 79,  0, 24,  0,
                 24, 58,  1,  1, 58,  8, 13, 43,  0,  2, 26,  2]]), 'boxes': tensor([[[356.1694, 142.4833, 379.7729, 186.1868],
                 [357.5754, 157.5763, 377.5587, 175.0622],
                 [359.9948, 149.4757, 376.7187, 177.3886],
                 ...,
                 [128.7645, 213.3606, 152.0828, 227.7635],
                 [423.1789, 368.7846, 605.4547, 484.8478],
                 [206.7028, 182.7572, 218.6928, 194.1239]]]), 'scores': tensor([[0.3239, 0.2784, 0.2533, 0.2328, 0.2200, 0.1941, 0.1862, 0.1808, 0.1791,
                 0.1696, 0.1534, 0.1416, 0.1390, 0.1388, 0.1308, 0.1239, 0.1161, 0.1100,
                 0.1086, 0.1073, 0.1064, 0.1041, 0.1036, 0.1015, 0.1005, 0.1005, 0.0996,
                 0.0994, 0.0983, 0.0978, 0.0962, 0.0959, 0.0957, 0.0954, 0.0949, 0.0937,
                 0.0912, 0.0912, 0.0902, 0.0883, 0.0879, 0.0871, 0.0856, 0.0836, 0.0821,
                 0.0814, 0.0814, 0.0794, 0.0774, 0.0766, 0.0756, 0.0751, 0.0734, 0.0727,
                 0.0726, 0.0723, 0.0720, 0.0709, 0.0703, 0.0691, 0.0690, 0.0673, 0.0670,
                 0.0666, 0.0665, 0.0664, 0.0656, 0.0656, 0.0650, 0.0650, 0.0647, 0.0643,
                 0.0641, 0.0638, 0.0632, 0.0628, 0.0628, 0.0628, 0.0618, 0.0615, 0.0614,
                 0.0613, 0.0609, 0.0607, 0.0601, 0.0597, 0.0595, 0.0589, 0.0576, 0.0575,
                 0.0575, 0.0568, 0.0565, 0.0563, 0.0557, 0.0554, 0.0553, 0.0551, 0.0549,
                 0.0548, 0.0547, 0.0546, 0.0546, 0.0545, 0.0542, 0.0536, 0.0531, 0.0530,
                 0.0529, 0.0529, 0.0526, 0.0524, 0.0521, 0.0511, 0.0511, 0.0510, 0.0507,
                 0.0505, 0.0504, 0.0494, 0.0493, 0.0493, 0.0489, 0.0486, 0.0484, 0.0478,
                 0.0473, 0.0472, 0.0472, 0.0464, 0.0462, 0.0458, 0.0456, 0.0455, 0.0455,
                 0.0450, 0.0448, 0.0445, 0.0442, 0.0439, 0.0436, 0.0430, 0.0430, 0.0427,
                 0.0420, 0.0419, 0.0418, 0.0417, 0.0417, 0.0416, 0.0415, 0.0415, 0.0412,
                 0.0409, 0.0407, 0.0406, 0.0405, 0.0405, 0.0404, 0.0401, 0.0400, 0.0399,
                 0.0398, 0.0396, 0.0392, 0.0390, 0.0387, 0.0386, 0.0383, 0.0383, 0.0382,
                 0.0381, 0.0381, 0.0380, 0.0377, 0.0377, 0.0376, 0.0376, 0.0375, 0.0371,
                 0.0370, 0.0370, 0.0369, 0.0367, 0.0367, 0.0365, 0.0362, 0.0361, 0.0360,
                 0.0358, 0.0356, 0.0356, 0.0356, 0.0354, 0.0352, 0.0350, 0.0350, 0.0348,
                 0.0347, 0.0347, 0.0346, 0.0346, 0.0344, 0.0342, 0.0341, 0.0341, 0.0340,
                 0.0338, 0.0338, 0.0338, 0.0336, 0.0336, 0.0334, 0.0334, 0.0332, 0.0330,
                 0.0330, 0.0330, 0.0329, 0.0329, 0.0327, 0.0326, 0.0326, 0.0325, 0.0322,
                 0.0322, 0.0321, 0.0320, 0.0318, 0.0318, 0.0317, 0.0317, 0.0316, 0.0316,
                 0.0316, 0.0314, 0.0314, 0.0314, 0.0314, 0.0312, 0.0311, 0.0311, 0.0310,
                 0.0309, 0.0309, 0.0309, 0.0309, 0.0309, 0.0307, 0.0307, 0.0307, 0.0306,
                 0.0305, 0.0305, 0.0305, 0.0304, 0.0302, 0.0302, 0.0302, 0.0301, 0.0300,
                 0.0300, 0.0300, 0.0300, 0.0300, 0.0298, 0.0298, 0.0298, 0.0296, 0.0293,
                 0.0293, 0.0292, 0.0291, 0.0290, 0.0290, 0.0290, 0.0289, 0.0288, 0.0288,
                 0.0288, 0.0288, 0.0286, 0.0285, 0.0285, 0.0284, 0.0284, 0.0284, 0.0283,
                 0.0283, 0.0281, 0.0281, 0.0278, 0.0277, 0.0277, 0.0276, 0.0276, 0.0276,
                 0.0275, 0.0275, 0.0275]])}[0m
[38;5;10m[I] trt-runner-N0-05/19/25-15:03:57     | Completed 1 iteration(s) in 44.98 ms | Average inference time: 44.98 ms.[0m
[38;5;14m[I] onnxrt-runner-N0-05/19/25-15:03:57  | Activating and starting inference[0m
[38;5;13m[V] Loaded Module: onnxruntime | Version: 1.20.2 | Path: ['/root/miniconda3/envs/rtdetr/lib/python3.10/site-packages/onnxruntime'][0m
[38;5;14m[I] Creating ONNX-Runtime Inference Session with providers: ['CPUExecutionProvider'][0m
[38;5;11m[W] Input tensor: orig_target_sizes | Buffer shape (torch.Size([1, 1, 2])) does not match expected input shape (BoundedShape([1, 2], min=None, max=None)). Attempting to transpose/reshape. [0m
[I] Reshaped array from shape: torch.Size([1, 1, 2]) to: torch.Size([1, 2])
[I] onnxrt-runner-N0-05/19/25-15:03:57 
    ---- Inference Input(s) ----
    {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}
[38;5;104m[X] onnxrt-runner-N0-05/19/25-15:03:57  | Feeding inputs:
        {'images': array([[[[0.98039216, 0.98039216, 0.9764706 , ..., 0.16862746,
                  0.25490198, 0.22352941],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.23137255,
                  0.2784314 , 0.28627452],
                 [0.98039216, 0.98039216, 0.9764706 , ..., 0.3019608 ,
                  0.29411766, 0.31764707],
                 ...,
                 [0.49803922, 0.5686275 , 0.5529412 , ..., 0.4509804 ,
                  0.4       , 0.44313726],
                 [0.49019608, 0.60784316, 0.5647059 , ..., 0.54509807,
                  0.4392157 , 0.45882353],
                 [0.5921569 , 0.7058824 , 0.54509807, ..., 0.5882353 ,
                  0.48235294, 0.4392157 ]],
        
                [[0.99607843, 0.99607843, 0.99215686, ..., 0.22745098,
                  0.32156864, 0.29411766],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.29803923,
                  0.34509805, 0.35686275],
                 [0.99607843, 0.99607843, 0.99215686, ..., 0.36862746,
                  0.36078432, 0.38039216],
                 ...,
                 [0.4862745 , 0.57254905, 0.5686275 , ..., 0.4862745 ,
                  0.4509804 , 0.5058824 ],
                 [0.47843137, 0.6117647 , 0.5803922 , ..., 0.53333336,
                  0.44705883, 0.4745098 ],
                 [0.5803922 , 0.70980394, 0.56078434, ..., 0.5254902 ,
                  0.43529412, 0.4       ]],
        
                [[0.99215686, 0.99215686, 0.9882353 , ..., 0.24705882,
                  0.3529412 , 0.34117648],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.32156864,
                  0.3764706 , 0.39607844],
                 [0.99215686, 0.99215686, 0.9882353 , ..., 0.4       ,
                  0.39215687, 0.41568628],
                 ...,
                 [0.4627451 , 0.5568628 , 0.5686275 , ..., 0.46666667,
                  0.42745098, 0.4745098 ],
                 [0.4509804 , 0.5921569 , 0.5764706 , ..., 0.49411765,
                  0.40784314, 0.43529412],
                 [0.5529412 , 0.6901961 , 0.5568628 , ..., 0.4627451 ,
                  0.3882353 , 0.3647059 ]]]], dtype=float32), 'orig_target_sizes': tensor([[640, 480]])}[0m
[38;5;13m[V] onnxrt-runner-N0-05/19/25-15:03:57  | Input metadata is: {images [dtype=float32, shape=(1, 3, 640, 640)],
     orig_target_sizes [dtype=int64, shape=(1, 2)]}[0m
[I] onnxrt-runner-N0-05/19/25-15:03:57 
    ---- Inference Output(s) ----
    {labels [dtype=int64, shape=(1, 300)],
     boxes [dtype=float32, shape=(1, 300, 4)],
     scores [dtype=float32, shape=(1, 300)]}
[38;5;104m[X] onnxrt-runner-N0-05/19/25-15:03:57  | Inference Time: 137.351 ms | Received outputs:
        {'labels': tensor([[ 0, 67,  8,  8, 26, 67,  8,  0,  0,  0, 28,  8,  8,  0,  8,  8,  0,  0,
                 67,  8,  8,  8,  8,  8,  8,  8,  8,  0, 24,  8, 67, 24, 13,  8,  0,  8,
                  8, 67,  0,  8,  8,  8, 67, 26,  8,  0,  0, 26,  8,  0, 67,  8,  8, 26,
                  0,  0,  8,  0,  8, 13,  8,  8,  8, 28, 13,  8,  0, 28, 24,  8,  0, 26,
                 28, 26,  8, 24,  0, 26,  0,  8, 24,  0, 24,  0,  0, 67,  0, 67, 67,  8,
                 24,  0, 26, 67, 24,  0, 67, 79, 58,  8,  8, 24,  0,  0,  0,  8, 28,  8,
                  8, 56,  0, 58, 28, 67, 67,  0, 76,  0, 26, 24, 73,  8, 60, 24, 28,  8,
                 24,  8, 13,  0,  0, 24,  0,  8, 28, 24, 11,  8, 43,  2, 26,  0, 26,  8,
                  8, 28,  8,  8, 24,  0, 24, 28, 24,  2, 24, 26, 26,  0, 26, 28, 39, 26,
                 26, 13, 56, 24,  8,  0, 26, 13, 67, 24,  8, 24, 24, 26,  8, 28,  0, 28,
                  2, 24, 24, 13,  8, 56, 67, 26, 24,  0, 26,  0, 76, 24,  9,  8,  8, 34,
                  0, 58, 67, 28, 13, 24, 13,  8,  8, 24, 56, 79, 56, 24,  0, 12, 25, 28,
                 43, 26,  3,  0, 27, 58, 74,  3,  8,  8,  2, 26, 58,  0,  8, 74, 60,  9,
                  0, 24,  0, 26, 24, 24,  0, 11,  8,  8,  0, 13,  0,  0,  0,  1, 79,  8,
                 28, 24,  0,  1,  2, 59,  0, 58, 24, 28,  0,  9, 26, 24,  8, 26, 79,  8,
                 26, 26, 24, 24,  8, 24, 13, 67,  2,  8,  0, 26,  0, 13, 26, 24,  0, 58,
                  0,  8, 67,  8, 11, 25,  2,  0, 56,  2, 28, 24]]), 'boxes': tensor([[[262.9576,  71.5327, 547.7870, 476.1678],
                 [361.2971, 137.0405, 373.3970, 169.7942],
                 [ -1.0678, 216.7084,  27.7456, 234.6881],
                 ...,
                 [100.0009, 178.9919, 123.2679, 196.2153],
                 [175.3711, 198.3185, 231.4542, 280.3434],
                 [368.4505, 398.3175, 518.1219, 478.9788]]]), 'scores': tensor([[0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                 0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                 0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                 0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                 0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                 0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                 0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                 0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                 0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                 0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                 0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                 0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                 0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                 0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                 0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                 0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                 0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                 0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                 0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                 0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                 0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                 0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                 0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                 0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                 0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                 0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                 0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                 0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                 0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                 0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                 0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                 0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                 0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                 0.0419, 0.0417, 0.0417]])}[0m
[38;5;10m[I] onnxrt-runner-N0-05/19/25-15:03:57  | Completed 1 iteration(s) in 137.4 ms | Average inference time: 137.4 ms.[0m
[38;5;13m[V] Successfully ran: ['trt-runner-N0-05/19/25-15:03:57', 'onnxrt-runner-N0-05/19/25-15:03:57'][0m
[38;5;14m[I] Accuracy Comparison | trt-runner-N0-05/19/25-15:03:57 vs. onnxrt-runner-N0-05/19/25-15:03:57[0m
[38;5;14m[I]     Comparing Output: 'labels' (dtype=int64, shape=torch.Size([1, 300])) with 'labels' (dtype=int64, shape=torch.Size([1, 300]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N0-05/19/25-15:03:57 vs. onnxrt-runner-N0-05/19/25-15:03:57[0m
[38;5;104m[X]             trt-runner-N0-05/19/25-15:03:57     | Mismatched values:
                tensor([67, 67,  0, 67, 67, 67, 67, 67, 67, 67, 67,  8,  8,  8,  0, 24, 67,  0,
                        67,  8,  8,  8,  8, 24,  8, 24,  0,  8,  8, 67,  0,  0,  8,  8,  8,  8,
                         8,  0, 67, 24,  8,  0,  0, 67,  8,  8,  0,  8,  0,  8,  8, 67,  8,  8,
                         8,  8,  8,  8,  8,  8, 26, 24, 24,  0,  8,  8,  8,  8,  8,  0,  8,  0,
                         0,  2,  8,  8, 13, 67,  0,  0,  0,  0, 67,  0,  8, 24,  0,  0,  8, 26,
                         8,  8,  0,  8,  8, 24,  8,  8,  8,  8, 13,  0,  8,  8, 58,  8,  0,  0,
                         0, 79, 74,  8, 24, 67, 13,  8,  8,  0,  0, 26,  8,  0, 27, 24,  8, 39,
                         0, 26, 13,  2,  2,  8,  8,  0,  0,  8, 24,  0,  8,  8, 74,  0,  2, 26,
                         0,  0, 24,  8,  8, 26, 58, 26,  0,  1,  0, 28,  0,  8, 26,  0, 13, 13,
                        13,  2,  0,  0,  8,  2,  2,  2, 24,  0, 39,  0, 13,  0, 24, 10,  9, 13,
                         0, 13, 13, 13, 13,  0, 79,  0, 13,  8, 24,  8,  0,  8, 13,  0, 13, 73,
                        28,  8, 26,  8, 13,  0,  2,  2, 13,  8, 67,  0, 24, 26, 52, 26, 13, 24,
                        13, 24,  0, 24, 13, 43, 13,  0,  9, 27, 79,  2,  0, 13, 39, 79,  0, 24,
                         0, 24, 58,  1,  1, 58,  8, 13, 43,  0, 26,  2])[0m
[38;5;104m[X]             onnxrt-runner-N0-05/19/25-15:03:57  | Mismatched values:
                tensor([ 0,  8,  8, 26,  8,  0,  0, 28,  8,  8,  8,  0,  0, 67,  8,  8,  8,  8,
                         8,  0, 24, 24, 13,  8,  0,  8,  8, 67,  0,  8,  8,  8, 67, 26,  0, 26,
                        67, 26,  0,  0,  0,  8,  8, 28, 13,  0, 28, 24,  8,  0, 26, 28, 26, 24,
                         0, 26,  0, 24, 24,  0, 67, 67,  8, 24,  0, 26, 67, 24,  0, 67, 79, 58,
                         8,  8, 24,  0,  0,  8, 28,  8, 56, 58, 28, 67, 76,  0, 26, 24, 73,  8,
                        60, 24, 28, 13,  0,  0, 24,  0, 28, 24, 11,  8, 43,  2, 26, 26,  8, 28,
                        24, 24, 28, 24,  2, 24, 26, 26,  0, 26, 28, 39, 26, 26, 13, 56, 24, 26,
                        13, 67, 24,  8, 24, 24, 26,  8, 28,  0, 28,  2, 24, 24, 13,  8, 56, 67,
                        26, 24, 26, 76, 24,  9,  8,  8, 34,  0, 58, 67, 28, 13, 24, 13,  8,  8,
                        24, 56, 79, 56, 24,  0, 12, 25, 28, 43, 26,  3,  0, 27, 58, 74,  3,  8,
                         8,  2, 26, 58,  8, 74, 60,  9,  0, 24,  0, 26, 24, 24,  0, 11,  8,  8,
                         0,  0,  0,  1, 79,  8, 28, 24,  0,  1,  2, 59, 58, 24, 28,  9, 24,  8,
                        26, 79,  8, 26, 26, 24, 24,  8, 67,  2,  8,  0, 26,  0, 13, 26, 24,  0,
                        58,  0,  8, 67,  8, 11, 25,  2,  0, 56, 28, 24])[0m
[I]         trt-runner-N0-05/19/25-15:03:57: labels | Stats: mean=16.197, std-dev=21.225, var=450.5, median=8, min=0 at (0, 3), max=79 at (0, 150), avg-magnitude=16.197, p90=67, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         94 | ##############################
                (7 , 15) |        124 | ########################################
                (15, 23) |          0 | 
                (23, 31) |         40 | ############
                (31, 39) |          3 | 
                (39, 47) |          2 | 
                (47, 55) |          1 | 
                (55, 63) |          4 | #
                (63, 71) |         25 | ########
                (71, 79) |          7 | ##
[I]         onnxrt-runner-N0-05/19/25-15:03:57: labels | Stats: mean=20.497, std-dev=21.41, var=458.4, median=12, min=0 at (0, 0), max=79 at (0, 97), avg-magnitude=20.497, p90=59, p95=67, p99=79
[I]             ---- Histogram ----
                Bin Range|  Num Elems | Visualization
                (0 , 7 ) |         73 | ################################
                (7 , 15) |         89 | #######################################
                (15, 23) |          0 | 
                (23, 31) |         90 | ########################################
                (31, 39) |          2 | 
                (39, 47) |          2 | 
                (47, 55) |          0 | 
                (55, 63) |         16 | #######
                (63, 71) |         19 | ########
                (71, 79) |          9 | ####
[I]         Error Metrics: labels
[I]             Minimum Required Tolerance: elemwise error | [abs=79] OR [rel=nan] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=21.113, std-dev=20.994, var=440.73, median=16, min=0 at (0, 1), max=79 at (0, 209), avg-magnitude=21.113, p90=59, p95=65, p99=71
[I]                 ---- Histogram ----
                    Bin Range|  Num Elems | Visualization
                    (0 , 7 ) |         83 | ########################################
                    (7 , 15) |         64 | ##############################
                    (15, 23) |         49 | #######################
                    (23, 31) |         32 | ###############
                    (31, 39) |         11 | #####
                    (39, 47) |         11 | #####
                    (47, 55) |          8 | ###
                    (55, 63) |         24 | ###########
                    (63, 71) |         16 | #######
                    (71, 79) |          2 | 
[I]             Relative Difference | Stats: mean=nan, std-dev=nan, var=nan, median=nan, min=nan at (0, 7), max=nan at (0, 7), avg-magnitude=nan, p90=nan, p95=nan, p99=nan
[38;5;13m[V]                 Could not generate histogram. Note: Error was: torch.histogramdd: dimension 0's range [-nan, -nan] is not finite[0m
[I]                 
[38;5;104m[X]         Finished comparing: 'labels' (dtype=int64, shape=torch.Size([1, 300])) [trt-runner-N0-05/19/25-15:03:57] and 'labels' (dtype=int64, shape=torch.Size([1, 300])) [onnxrt-runner-N0-05/19/25-15:03:57][0m
[38;5;9m[E]         FAILED | Output: 'labels' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) with 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N0-05/19/25-15:03:57 vs. onnxrt-runner-N0-05/19/25-15:03:57[0m
[38;5;104m[X]             trt-runner-N0-05/19/25-15:03:57     | Mismatched values:
                tensor([356.1694, 142.4833, 379.7729,  ..., 182.7572, 218.6928, 194.1239])[0m
[38;5;104m[X]             onnxrt-runner-N0-05/19/25-15:03:57  | Mismatched values:
                tensor([262.9576,  71.5327, 547.7870,  ..., 398.3175, 518.1219, 478.9788])[0m
[I]         trt-runner-N0-05/19/25-15:03:57: boxes | Stats: mean=232.75, std-dev=123.69, var=15298, median=213.49, min=-27.047 at (0, 193, 0), max=665.68 at (0, 155, 2), avg-magnitude=232.94, p90=377.85, p95=501.79, p99=616.82
[I]             ---- Histogram ----
                Bin Range    |  Num Elems | Visualization
                (-27 , 42.2) |       65.0 | #####
                (42.2, 111 ) |       67.0 | #####
                (111 , 181 ) |      227.0 | ##################
                (181 , 250 ) |      493.0 | ########################################
                (250 , 319 ) |      137.0 | ###########
                (319 , 389 ) |      104.0 | ########
                (389 , 458 ) |       32.0 | ##
                (458 , 527 ) |       21.0 | #
                (527 , 596 ) |       17.0 | #
                (596 , 666 ) |       37.0 | ###
[I]         onnxrt-runner-N0-05/19/25-15:03:57: boxes | Stats: mean=260.78, std-dev=140.6, var=19769, median=224.33, min=-1.3738 at (0, 105, 0), max=648.52 at (0, 276, 2), avg-magnitude=260.79, p90=479.46, p95=594.98, p99=622.91
[I]             ---- Histogram ----
                Bin Range    |  Num Elems | Visualization
                (-27 , 42.2) |       45.0 | ###
                (42.2, 111 ) |       79.0 | ######
                (111 , 181 ) |      168.0 | ##############
                (181 , 250 ) |      456.0 | ########################################
                (250 , 319 ) |      142.0 | ############
                (319 , 389 ) |      123.0 | ##########
                (389 , 458 ) |       40.0 | ###
                (458 , 527 ) |       52.0 | ####
                (527 , 596 ) |       40.0 | ###
                (596 , 666 ) |       55.0 | ####
[I]         Error Metrics: boxes
[I]             Minimum Required Tolerance: elemwise error | [abs=616.28] OR [rel=668.16] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=127.61, std-dev=129.16, var=16681, median=76.615, min=0.02124 at (0, 49, 0), max=616.28 at (0, 175, 2), avg-magnitude=127.61, p90=310.94, p95=387.7, p99=553.56
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0.0212, 61.6) |      522.0 | ########################################
                    (61.6  , 123 ) |      220.0 | ################
                    (123   , 185 ) |      128.0 | #########
                    (185   , 247 ) |      115.0 | ########
                    (247   , 308 ) |       91.0 | ######
                    (308   , 370 ) |       49.0 | ###
                    (370   , 431 ) |       32.0 | ##
                    (431   , 493 ) |       18.0 | #
                    (493   , 555 ) |       14.0 | #
                    (555   , 616 ) |       11.0 | 
[I]             Relative Difference | Stats: mean=3.6665, std-dev=35.425, var=1254.9, median=0.34604, min=3.6471e-05 at (0, 49, 0), max=668.16 at (0, 3, 0), avg-magnitude=3.6665, p90=1.4157, p95=2.9162, p99=27.504
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (3.65e-05, 66.8) |     1191.0 | ########################################
                    (66.8    , 134 ) |        0.0 | 
                    (134     , 200 ) |        1.0 | 
                    (200     , 267 ) |        2.0 | 
                    (267     , 334 ) |        1.0 | 
                    (334     , 401 ) |        2.0 | 
                    (401     , 468 ) |        1.0 | 
                    (468     , 535 ) |        0.0 | 
                    (535     , 601 ) |        0.0 | 
                    (601     , 668 ) |        2.0 | 
[38;5;104m[X]         Finished comparing: 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [trt-runner-N0-05/19/25-15:03:57] and 'boxes' (dtype=float32, shape=torch.Size([1, 300, 4])) [onnxrt-runner-N0-05/19/25-15:03:57][0m
[38;5;9m[E]         FAILED | Output: 'boxes' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;14m[I]     Comparing Output: 'scores' (dtype=float32, shape=torch.Size([1, 300])) with 'scores' (dtype=float32, shape=torch.Size([1, 300]))[0m
[I]         Tolerance: [abs=1e-08, rel=1e-05] | Checking elemwise error
[38;5;104m[X]         Note: Comparing trt-runner-N0-05/19/25-15:03:57 vs. onnxrt-runner-N0-05/19/25-15:03:57[0m
[38;5;104m[X]             trt-runner-N0-05/19/25-15:03:57     | Mismatched values:
                tensor([0.3239, 0.2784, 0.2533, 0.2328, 0.2200, 0.1941, 0.1862, 0.1808, 0.1791,
                        0.1696, 0.1534, 0.1416, 0.1390, 0.1388, 0.1308, 0.1239, 0.1161, 0.1100,
                        0.1086, 0.1073, 0.1064, 0.1041, 0.1036, 0.1015, 0.1005, 0.1005, 0.0996,
                        0.0994, 0.0983, 0.0978, 0.0962, 0.0959, 0.0957, 0.0954, 0.0949, 0.0937,
                        0.0912, 0.0912, 0.0902, 0.0883, 0.0879, 0.0871, 0.0856, 0.0836, 0.0821,
                        0.0814, 0.0814, 0.0794, 0.0774, 0.0766, 0.0756, 0.0751, 0.0734, 0.0727,
                        0.0726, 0.0723, 0.0720, 0.0709, 0.0703, 0.0691, 0.0690, 0.0673, 0.0670,
                        0.0666, 0.0665, 0.0664, 0.0656, 0.0656, 0.0650, 0.0650, 0.0647, 0.0643,
                        0.0641, 0.0638, 0.0632, 0.0628, 0.0628, 0.0628, 0.0618, 0.0615, 0.0614,
                        0.0613, 0.0609, 0.0607, 0.0601, 0.0597, 0.0595, 0.0589, 0.0576, 0.0575,
                        0.0575, 0.0568, 0.0565, 0.0563, 0.0557, 0.0554, 0.0553, 0.0551, 0.0549,
                        0.0548, 0.0547, 0.0546, 0.0546, 0.0545, 0.0542, 0.0536, 0.0531, 0.0530,
                        0.0529, 0.0529, 0.0526, 0.0524, 0.0521, 0.0511, 0.0511, 0.0510, 0.0507,
                        0.0505, 0.0504, 0.0494, 0.0493, 0.0493, 0.0489, 0.0486, 0.0484, 0.0478,
                        0.0473, 0.0472, 0.0472, 0.0464, 0.0462, 0.0458, 0.0456, 0.0455, 0.0455,
                        0.0450, 0.0448, 0.0445, 0.0442, 0.0439, 0.0436, 0.0430, 0.0430, 0.0427,
                        0.0420, 0.0419, 0.0418, 0.0417, 0.0417, 0.0416, 0.0415, 0.0415, 0.0412,
                        0.0409, 0.0407, 0.0406, 0.0405, 0.0405, 0.0404, 0.0401, 0.0400, 0.0399,
                        0.0398, 0.0396, 0.0392, 0.0390, 0.0387, 0.0386, 0.0383, 0.0383, 0.0382,
                        0.0381, 0.0381, 0.0380, 0.0377, 0.0377, 0.0376, 0.0376, 0.0375, 0.0371,
                        0.0370, 0.0370, 0.0369, 0.0367, 0.0367, 0.0365, 0.0362, 0.0361, 0.0360,
                        0.0358, 0.0356, 0.0356, 0.0356, 0.0354, 0.0352, 0.0350, 0.0350, 0.0348,
                        0.0347, 0.0347, 0.0346, 0.0346, 0.0344, 0.0342, 0.0341, 0.0341, 0.0340,
                        0.0338, 0.0338, 0.0338, 0.0336, 0.0336, 0.0334, 0.0334, 0.0332, 0.0330,
                        0.0330, 0.0330, 0.0329, 0.0329, 0.0327, 0.0326, 0.0326, 0.0325, 0.0322,
                        0.0322, 0.0321, 0.0320, 0.0318, 0.0318, 0.0317, 0.0317, 0.0316, 0.0316,
                        0.0316, 0.0314, 0.0314, 0.0314, 0.0314, 0.0312, 0.0311, 0.0311, 0.0310,
                        0.0309, 0.0309, 0.0309, 0.0309, 0.0309, 0.0307, 0.0307, 0.0307, 0.0306,
                        0.0305, 0.0305, 0.0305, 0.0304, 0.0302, 0.0302, 0.0302, 0.0301, 0.0300,
                        0.0300, 0.0300, 0.0300, 0.0300, 0.0298, 0.0298, 0.0298, 0.0296, 0.0293,
                        0.0293, 0.0292, 0.0291, 0.0290, 0.0290, 0.0290, 0.0289, 0.0288, 0.0288,
                        0.0288, 0.0288, 0.0286, 0.0285, 0.0285, 0.0284, 0.0284, 0.0284, 0.0283,
                        0.0283, 0.0281, 0.0281, 0.0278, 0.0277, 0.0277, 0.0276, 0.0276, 0.0276,
                        0.0275, 0.0275, 0.0275])[0m
[38;5;104m[X]             onnxrt-runner-N0-05/19/25-15:03:57  | Mismatched values:
                tensor([0.6030, 0.5362, 0.5060, 0.4716, 0.4550, 0.4172, 0.4056, 0.3967, 0.3598,
                        0.3229, 0.2707, 0.2680, 0.2555, 0.2454, 0.2324, 0.2316, 0.2067, 0.2027,
                        0.2022, 0.2010, 0.1952, 0.1934, 0.1921, 0.1915, 0.1900, 0.1865, 0.1848,
                        0.1837, 0.1807, 0.1792, 0.1760, 0.1732, 0.1698, 0.1648, 0.1636, 0.1600,
                        0.1595, 0.1585, 0.1555, 0.1547, 0.1529, 0.1523, 0.1518, 0.1502, 0.1478,
                        0.1432, 0.1393, 0.1369, 0.1360, 0.1356, 0.1341, 0.1318, 0.1289, 0.1285,
                        0.1260, 0.1236, 0.1187, 0.1185, 0.1182, 0.1180, 0.1179, 0.1175, 0.1174,
                        0.1169, 0.1167, 0.1138, 0.1123, 0.1119, 0.1114, 0.1112, 0.1103, 0.1103,
                        0.1102, 0.1101, 0.1092, 0.1092, 0.1083, 0.1081, 0.1059, 0.1050, 0.1040,
                        0.1031, 0.1025, 0.1021, 0.1009, 0.0988, 0.0972, 0.0961, 0.0952, 0.0948,
                        0.0943, 0.0942, 0.0942, 0.0942, 0.0940, 0.0934, 0.0934, 0.0932, 0.0930,
                        0.0925, 0.0924, 0.0922, 0.0921, 0.0915, 0.0913, 0.0910, 0.0907, 0.0899,
                        0.0898, 0.0883, 0.0872, 0.0868, 0.0864, 0.0864, 0.0859, 0.0849, 0.0835,
                        0.0833, 0.0830, 0.0819, 0.0814, 0.0810, 0.0804, 0.0800, 0.0790, 0.0788,
                        0.0786, 0.0780, 0.0779, 0.0776, 0.0776, 0.0773, 0.0771, 0.0770, 0.0766,
                        0.0765, 0.0760, 0.0759, 0.0752, 0.0751, 0.0749, 0.0748, 0.0744, 0.0742,
                        0.0740, 0.0736, 0.0732, 0.0732, 0.0731, 0.0730, 0.0722, 0.0718, 0.0717,
                        0.0717, 0.0710, 0.0706, 0.0705, 0.0693, 0.0692, 0.0691, 0.0686, 0.0684,
                        0.0680, 0.0678, 0.0677, 0.0677, 0.0672, 0.0670, 0.0656, 0.0656, 0.0655,
                        0.0655, 0.0654, 0.0654, 0.0652, 0.0645, 0.0640, 0.0639, 0.0638, 0.0631,
                        0.0628, 0.0626, 0.0619, 0.0617, 0.0612, 0.0612, 0.0611, 0.0610, 0.0609,
                        0.0606, 0.0606, 0.0604, 0.0604, 0.0604, 0.0604, 0.0601, 0.0597, 0.0597,
                        0.0597, 0.0596, 0.0595, 0.0593, 0.0589, 0.0586, 0.0586, 0.0586, 0.0585,
                        0.0578, 0.0574, 0.0573, 0.0573, 0.0571, 0.0570, 0.0568, 0.0567, 0.0562,
                        0.0557, 0.0556, 0.0555, 0.0555, 0.0555, 0.0544, 0.0542, 0.0535, 0.0533,
                        0.0533, 0.0531, 0.0530, 0.0527, 0.0525, 0.0522, 0.0521, 0.0519, 0.0518,
                        0.0517, 0.0515, 0.0514, 0.0512, 0.0507, 0.0507, 0.0507, 0.0505, 0.0504,
                        0.0503, 0.0502, 0.0502, 0.0499, 0.0496, 0.0495, 0.0487, 0.0487, 0.0485,
                        0.0484, 0.0482, 0.0480, 0.0480, 0.0480, 0.0476, 0.0472, 0.0467, 0.0466,
                        0.0462, 0.0460, 0.0460, 0.0460, 0.0459, 0.0456, 0.0455, 0.0455, 0.0455,
                        0.0454, 0.0451, 0.0451, 0.0445, 0.0444, 0.0444, 0.0443, 0.0441, 0.0441,
                        0.0440, 0.0440, 0.0439, 0.0439, 0.0438, 0.0438, 0.0437, 0.0436, 0.0435,
                        0.0432, 0.0430, 0.0429, 0.0426, 0.0426, 0.0425, 0.0423, 0.0419, 0.0419,
                        0.0419, 0.0417, 0.0417])[0m
[I]         trt-runner-N0-05/19/25-15:03:57: scores | Stats: mean=0.056002, std-dev=0.040066, var=0.0016053, median=0.041576, min=0.027456 at (0, 299), max=0.3239 at (0, 0), avg-magnitude=0.056002, p90=0.096346, p95=0.12429, p99=0.23303
[I]             ---- Histogram ----
                Bin Range       |  Num Elems | Visualization
                (0.0275, 0.085) |      257.0 | ########################################
                (0.085 , 0.143) |       32.0 | ####
                (0.143 , 0.2  ) |        6.0 | 
                (0.2   , 0.258) |        3.0 | 
                (0.258 , 0.315) |        1.0 | 
                (0.315 , 0.373) |        1.0 | 
                (0.373 , 0.43 ) |        0.0 | 
                (0.43  , 0.488) |        0.0 | 
                (0.488 , 0.545) |        0.0 | 
                (0.545 , 0.603) |        0.0 | 
[I]         onnxrt-runner-N0-05/19/25-15:03:57: scores | Stats: mean=0.097993, std-dev=0.080413, var=0.0064662, median=0.072622, min=0.041714 at (0, 299), max=0.60297 at (0, 0), avg-magnitude=0.097993, p90=0.17633, p95=0.23168, p99=0.47193
[I]             ---- Histogram ----
                Bin Range       |  Num Elems | Visualization
                (0.0275, 0.085) |      185.0 | ########################################
                (0.085 , 0.143) |       69.0 | ##############
                (0.143 , 0.2  ) |       26.0 | #####
                (0.2   , 0.258) |        8.0 | #
                (0.258 , 0.315) |        2.0 | 
                (0.315 , 0.373) |        2.0 | 
                (0.373 , 0.43 ) |        3.0 | 
                (0.43  , 0.488) |        2.0 | 
                (0.488 , 0.545) |        2.0 | 
                (0.545 , 0.603) |        1.0 | 
[I]         Error Metrics: scores
[I]             Minimum Required Tolerance: elemwise error | [abs=0.27907] OR [rel=0.54438] (requirements may be lower if both abs/rel tolerances are set)
[I]             Absolute Difference | Stats: mean=0.041991, std-dev=0.040647, var=0.0016522, median=0.030713, min=0.014257 at (0, 299), max=0.27907 at (0, 0), avg-magnitude=0.041991, p90=0.079988, p95=0.10181, p99=0.2389
[I]                 ---- Histogram ----
                    Bin Range        |  Num Elems | Visualization
                    (0.0143, 0.0407) |      216.0 | ########################################
                    (0.0407, 0.0672) |       47.0 | ########
                    (0.0672, 0.0937) |       20.0 | ###
                    (0.0937, 0.12  ) |        6.0 | #
                    (0.12  , 0.147 ) |        1.0 | 
                    (0.147 , 0.173 ) |        1.0 | 
                    (0.173 , 0.2   ) |        1.0 | 
                    (0.2   , 0.226 ) |        3.0 | 
                    (0.226 , 0.253 ) |        2.0 | 
                    (0.253 , 0.279 ) |        3.0 | 
[I]             Relative Difference | Stats: mean=0.40701, std-dev=0.034743, var=0.0012071, median=0.41082, min=0.34145 at (0, 295), max=0.54438 at (0, 7), avg-magnitude=0.40701, p90=0.43906, p95=0.46284, p99=0.51671
[I]                 ---- Histogram ----
                    Bin Range      |  Num Elems | Visualization
                    (0.341, 0.362) |       42.0 | #############
                    (0.362, 0.382) |       13.0 | ####
                    (0.382, 0.402) |       53.0 | #################
                    (0.402, 0.423) |      121.0 | ########################################
                    (0.423, 0.443) |       41.0 | #############
                    (0.443, 0.463) |       16.0 | #####
                    (0.463, 0.484) |        7.0 | ##
                    (0.484, 0.504) |        2.0 | 
                    (0.504, 0.524) |        2.0 | 
                    (0.524, 0.544) |        3.0 | 
[38;5;104m[X]         Finished comparing: 'scores' (dtype=float32, shape=torch.Size([1, 300])) [trt-runner-N0-05/19/25-15:03:57] and 'scores' (dtype=float32, shape=torch.Size([1, 300])) [onnxrt-runner-N0-05/19/25-15:03:57][0m
[38;5;9m[E]         FAILED | Output: 'scores' | Difference exceeds tolerance (rel=1e-05, abs=1e-08)[0m
[38;5;9m[E]     FAILED | Mismatched outputs: ['labels', 'boxes', 'scores'][0m
[38;5;104m[X]     Finished comparing trt-runner-N0-05/19/25-15:03:57 with onnxrt-runner-N0-05/19/25-15:03:57[0m
[38;5;9m[E] Accuracy Summary | trt-runner-N0-05/19/25-15:03:57 vs. onnxrt-runner-N0-05/19/25-15:03:57 | Passed: 0/1 iterations | Pass Rate: 0.0%[0m
